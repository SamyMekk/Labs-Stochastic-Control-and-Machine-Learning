{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe8af4b",
   "metadata": {},
   "source": [
    "<h1> <center> An introductory notebook to PyTorch module. </center ></h1>\n",
    "\n",
    "This notebook will cover the basics of PyTorch, a popular deep learning framework. We will go through the following topics:\n",
    "\n",
    "-  Installation of PyTorch\n",
    "- Tensors in PyTorch\n",
    "- Basic operations on Tensors\n",
    "- Autograd: Automatic differentiation\n",
    "- Building a simple neural network\n",
    "- Training the neural network\n",
    "- Evaluating the model\n",
    "- Saving and loading models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf05a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "-- start training PINN --\n",
      "Epoch      0: Loss=3.4906e+00, Time=0.33s\n",
      "Epoch   2000: Loss=1.0593e-04, Time=945.85s\n",
      "Epoch   4000: Loss=4.7318e-05, Time=1945.25s\n",
      "Epoch   6000: Loss=3.3304e-05, Time=2884.06s\n",
      "Epoch   8000: Loss=2.1530e-05, Time=3861.93s\n",
      "\n",
      "-- start training M-PINN --\n",
      "Epoch      0: Loss=2.3577e+00, Time=14.47s\n",
      "Epoch   2000: Loss=1.1931e-04, Time=171136.06s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "def setup_logging():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f'1D_M_PIRL_PINN_training_{timestamp}.log'\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename, encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return log_filename\n",
    "\n",
    "log_filename = setup_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def menet_pricer(t0, S0, K, T, r, sigma, num_paths, num_timesteps):\n",
    "    with torch.no_grad():\n",
    "        if not isinstance(r, torch.Tensor):\n",
    "            r = torch.tensor(r, device=DEVICE, dtype=torch.float32)\n",
    "        if not isinstance(sigma, torch.Tensor):\n",
    "            sigma = torch.tensor(sigma, device=DEVICE, dtype=torch.float32)\n",
    "        if not isinstance(K, torch.Tensor):\n",
    "            K = torch.tensor(K, device=DEVICE, dtype=torch.float32)\n",
    "        \n",
    "        dt = (T - t0) / num_timesteps\n",
    "        S = S0.expand(-1, num_paths)\n",
    "        \n",
    "        drift = (r - 0.5 * sigma**2) * dt\n",
    "        vol = sigma * torch.sqrt(dt)\n",
    "        \n",
    "        for _ in range(num_timesteps):\n",
    "            Z = torch.randn_like(S)\n",
    "            S = S * torch.exp(drift + vol * Z)\n",
    "        \n",
    "        payoff = torch.relu(S - K)\n",
    "        expected_payoff = torch.mean(payoff, dim=1, keepdim=True)\n",
    "        price = expected_payoff * torch.exp(-r * (T - t0))\n",
    "        return price\n",
    "\n",
    "class ResidualPINN(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=48, output_dim=1, layers=8):\n",
    "        super(ResidualPINN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        for _ in range(layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim + input_dim, hidden_dim))\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.activation = nn.Tanh()\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            fan_in = m.in_features\n",
    "            fan_out = m.out_features\n",
    "            std = math.sqrt(2 / (fan_in + fan_out))\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=std)\n",
    "            m.weight.data = torch.clamp(m.weight.data, min=-2*std, max=2*std)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_input = x\n",
    "        x = self.layers[0](x)\n",
    "        x = self.activation(x)\n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            x_concat = torch.cat([x, original_input], dim=1)\n",
    "            update = self.layers[i](x_concat)\n",
    "            update = self.activation(update)\n",
    "            x = x + update  \n",
    "        output = self.layers[-1](x)\n",
    "        return output\n",
    "\n",
    "class StandardPINN(nn.Module):\n",
    "    def __init__(self, layers=4, hidden_dim=64):\n",
    "        super(StandardPINN, self).__init__()\n",
    "        layer_list = [nn.Linear(2, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(layers - 2):\n",
    "            layer_list.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layer_list.append(nn.Linear(hidden_dim, 1))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_pde_residual(model, t, S, K, r, sigma):\n",
    "    V = model(torch.cat([t, S], dim=1))\n",
    "    V_t = torch.autograd.grad(V, t, grad_outputs=torch.ones_like(V), create_graph=True)[0]\n",
    "    V_s = torch.autograd.grad(V, S, grad_outputs=torch.ones_like(V), create_graph=True)[0]\n",
    "    V_ss = torch.autograd.grad(V_s, S, grad_outputs=torch.ones_like(V_s), create_graph=True)[0]\n",
    "    residual = V_t + r * S * V_s + 0.5 * sigma**2 * S**2 * V_ss - r * V\n",
    "    return residual\n",
    "\n",
    "def train_model(model_type, params):\n",
    "    if model_type in ['PIRL', 'M-PIRL']:\n",
    "        model = ResidualPINN(input_dim=params['input_dim'], hidden_dim=params['residual_hidden_dim'], \n",
    "                             layers=params['residual_layers']).to(DEVICE)\n",
    "    elif model_type in ['PINN', 'M-PINN']:\n",
    "        model = StandardPINN(layers=params['standard_layers'], hidden_dim=params['standard_hidden_dim']).to(DEVICE)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    if params['use_scheduler']:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.7)\n",
    "\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    logger.info(f\"\\n-- start training {model_type} --\")\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        t_pde = torch.rand(params['N_interior'], 1, device=DEVICE) * params['T']\n",
    "        S_pde = torch.rand(params['N_interior'], 1, device=DEVICE) * \\\n",
    "                (params['S_max'] - params['S_min']) + params['S_min']\n",
    "        t_bc = torch.ones(params['N_bc'], 1, device=DEVICE) * params['T']\n",
    "        S_bc = torch.rand(params['N_bc'], 1, device=DEVICE) * \\\n",
    "               (params['S_max'] - params['S_min']) + params['S_min']\n",
    "        \n",
    "        num_batches = max(1, params['N_interior'] // params['batch_size'])\n",
    "        perm_interior = torch.randperm(params['N_interior'], device=DEVICE)\n",
    "        perm_bc = torch.randperm(params['N_bc'], device=DEVICE)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for b in range(num_batches):\n",
    "            start_idx = b * params['batch_size']\n",
    "            end_idx = min(start_idx + params['batch_size'], params['N_interior'])\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            idx_interior = perm_interior[start_idx:end_idx]\n",
    "            \n",
    "            start_bc = (b * params['batch_size']) % params['N_bc']\n",
    "            idx_bc = perm_bc[start_bc : start_bc + batch_size_actual]\n",
    "            if len(idx_bc) < batch_size_actual:\n",
    "                extra = batch_size_actual - len(idx_bc)\n",
    "                idx_bc = torch.cat([idx_bc, perm_bc[:extra]])\n",
    "            \n",
    "            t_pde_batch, S_pde_batch = t_pde[idx_interior], S_pde[idx_interior]\n",
    "            t_bc_batch, S_bc_batch = t_bc[idx_bc], S_bc[idx_bc]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            t_pde_batch.requires_grad = True\n",
    "            S_pde_batch.requires_grad = True\n",
    "            pde_residual = compute_pde_residual(model, t_pde_batch, S_pde_batch, \n",
    "                                               params['K'], params['r'], params['sigma'])\n",
    "            loss_pde = torch.mean(pde_residual**2)\n",
    "\n",
    "            V_pred_bc = model(torch.cat([t_bc_batch, S_bc_batch], dim=1))\n",
    "            V_true_bc = torch.relu(S_bc_batch - params['K'])\n",
    "            loss_bc = torch.mean((V_pred_bc - V_true_bc)**2)\n",
    "\n",
    "            total_loss = loss_pde + params['lambda_bc'] * loss_bc\n",
    "\n",
    "            if 'M-' in model_type:\n",
    "                batch_mar = min(batch_size_actual, params['N_martingale'] // num_batches)\n",
    "                t_mar = torch.rand(batch_mar, 1, device=DEVICE) * (params['T'] * 0.99)\n",
    "                S_mar = torch.rand(batch_mar, 1, device=DEVICE) * \\\n",
    "                        (params['S_max'] - params['S_min']) + params['S_min']\n",
    "                \n",
    "                V_pinn_mar = model(torch.cat([t_mar, S_mar], dim=1))\n",
    "                V_menet_mar = menet_pricer(t_mar, S_mar, params['K'], params['T'], params['r'], params['sigma'],\n",
    "                                          params['menet_paths'], params['menet_steps'])\n",
    "                loss_mar = torch.mean((V_pinn_mar - V_menet_mar)**2)\n",
    "                total_loss += params['lambda_martingale'] * loss_mar\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        loss_history.append(epoch_loss)\n",
    "        \n",
    "        if params['use_scheduler']:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % 2000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            logger.info(f\"Epoch {epoch:6d}: Loss={epoch_loss:.4e}, Time={elapsed:.2f}s\")\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    return model, loss_history, training_time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = 1\n",
    "    shared_params = {\n",
    "        'K': 1.0, 'T': 1.0, 'r': 0.05, 'sigma': 0.2,\n",
    "        'S_min': 0.5, 'S_max': 1.5,\n",
    "        'input_dim': 2,\n",
    "        'residual_hidden_dim': 48, 'residual_layers': 8,\n",
    "        'standard_hidden_dim': 64, 'standard_layers': 4,\n",
    "        'lr': 1e-3, 'epochs': 10000, 'batch_size': 512,\n",
    "        'use_scheduler': True,\n",
    "        'lambda_bc': 100.0, 'lambda_martingale': 1.0,\n",
    "        'N_interior': 10000 * n,\n",
    "        'N_bc': 2000 * n,\n",
    "        'N_martingale': 2000 * n,\n",
    "        'menet_paths': 2000, 'menet_steps': 252\n",
    "    }\n",
    "\n",
    "    pinn_model, pinn_loss_hist, pinn_time = train_model('PINN', shared_params)\n",
    "    m_pinn_model, m_pinn_loss_hist, m_pinn_time = train_model('M-PINN', shared_params)\n",
    "    pirl_model, pirl_loss_hist, pirl_time = train_model('PIRL', shared_params)\n",
    "    m_pirl_model, m_pirl_loss_hist, m_pirl_time = train_model('M-PIRL', shared_params)\n",
    "\n",
    "    def black_scholes_call_numpy(t, S, K, T, r, sigma):\n",
    "        epsilon = 1e-8\n",
    "        T_t = T - t + epsilon\n",
    "        d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T_t) / (sigma * np.sqrt(T_t))\n",
    "        d2 = d1 - sigma * np.sqrt(T_t)\n",
    "        return (S * norm.cdf(d1) - K * np.exp(-r * T_t) * norm.cdf(d2))\n",
    "\n",
    "    t_grid_np = np.linspace(0, shared_params['T'], 100)\n",
    "    S_grid_np = np.linspace(shared_params['S_min'], shared_params['S_max'], 100)\n",
    "    T_mesh, S_mesh = np.meshgrid(t_grid_np, S_grid_np)\n",
    "    grid_points = torch.tensor(np.stack([T_mesh.flatten(), S_mesh.flatten()], axis=-1), \n",
    "                              dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    V_true = black_scholes_call_numpy(T_mesh, S_mesh, **{k: shared_params[k] for k in ['K', 'T', 'r', 'sigma']})\n",
    "\n",
    "    pinn_model.eval()\n",
    "    m_pinn_model.eval()\n",
    "    pirl_model.eval()\n",
    "    m_pirl_model.eval()\n",
    "    with torch.no_grad():\n",
    "        V_pred_pinn = pinn_model(grid_points).cpu().numpy().reshape(T_mesh.shape)\n",
    "        V_pred_m_pinn = m_pinn_model(grid_points).cpu().numpy().reshape(T_mesh.shape)\n",
    "        V_pred_pirl = pirl_model(grid_points).cpu().numpy().reshape(T_mesh.shape)\n",
    "        V_pred_m_pirl = m_pirl_model(grid_points).cpu().numpy().reshape(T_mesh.shape)\n",
    "\n",
    "    error_pinn = np.linalg.norm(V_pred_pinn - V_true) / np.linalg.norm(V_true)\n",
    "    error_m_pinn = np.linalg.norm(V_pred_m_pinn - V_true) / np.linalg.norm(V_true)\n",
    "    error_pirl = np.linalg.norm(V_pred_pirl - V_true) / np.linalg.norm(V_true)\n",
    "    error_m_pirl = np.linalg.norm(V_pred_m_pirl - V_true) / np.linalg.norm(V_true)\n",
    "\n",
    "    detailed_data = {\n",
    "        'Time_t': T_mesh.flatten(), 'Stock_S': S_mesh.flatten(), 'V_true': V_true.flatten(),\n",
    "        'V_pred_PINN': V_pred_pinn.flatten(), 'V_pred_M_PINN': V_pred_m_pinn.flatten(),\n",
    "        'V_pred_PIRL': V_pred_pirl.flatten(), 'V_pred_M_PIRL': V_pred_m_pirl.flatten()\n",
    "    }\n",
    "    df_results = pd.DataFrame(detailed_data)\n",
    "    csv_path = f\"1D_M_PIRL_PINN_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    logger.info(f\"Data saved in: {csv_path}\")\n",
    "\n",
    "    # 1. Loss Comparison\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(pinn_loss_hist, label='PINN Loss', color='green')\n",
    "    plt.plot(m_pinn_loss_hist, label='M-PINN Loss', color='blue')\n",
    "    plt.plot(pirl_loss_hist, label='PIRL Loss', color='red')\n",
    "    plt.plot(m_pirl_loss_hist, label='M-PIRL Loss', color='purple')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Loss Convergence Comparison')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Total Loss (log scale)')\n",
    "    plt.legend(); plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.savefig('1_loss_convergence.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Price Curve at t=0\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(S_grid_np, V_true[:, 0], 'k-', label='Analytical Solution', linewidth=2)\n",
    "    plt.plot(S_grid_np, V_pred_pinn[:, 0], 'g--', label=f'PINN (Err: {error_pinn:.2%})')\n",
    "    plt.plot(S_grid_np, V_pred_m_pinn[:, 0], 'b-.', label=f'M-PINN (Err: {error_m_pinn:.2%})')\n",
    "    plt.plot(S_grid_np, V_pred_pirl[:, 0], 'r--', label=f'PIRL (Err: {error_pirl:.2%})')\n",
    "    plt.plot(S_grid_np, V_pred_m_pirl[:, 0], 'p-.', label=f'M-PIRL (Err: {error_m_pirl:.2%})')\n",
    "    plt.title('Price Curve at t=0')\n",
    "    plt.xlabel('Stock Price S'); plt.ylabel('Option Price V')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    plt.savefig('2_price_curve_t0.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    logger.info(\"Two figures saved: 1_loss_convergence.png, 2_price_curve_t0.png\")\n",
    "    logger.info(\"Training and evaluation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c12a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cec970e5",
   "metadata": {},
   "source": [
    "<h3> Setting PyTorch </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c70bbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823ba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cpu\n",
      "Device : cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samym\\AppData\\Local\\Temp\\ipykernel_10668\\1737830671.py:63: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | step   50 | loss 2.3549 | 1117 samples/s\n",
      "\n",
      "Epoch 1 done | avg loss 2.3294 | throughput 1065 samples/s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = pick_device()\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device :\", device)\n",
    "if device.type == \"cuda\":\n",
    "    prop = torch.cuda.get_device_properties(0)\n",
    "    print(\"GPU    :\", prop.name, f\"({prop.total_memory/(1024**3):.2f} GB)\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# -------------------------\n",
    "# Synthetic data (no torchvision)\n",
    "# -------------------------\n",
    "# Tu peux augmenter N pour plus long, ou batch_size pour plus de charge GPU\n",
    "N = 200_000          # nombre d'exemples\n",
    "in_dim = 1024        # taille input\n",
    "num_classes = 10\n",
    "batch_size = 2048    # si OOM: 2048 -> 1024 -> 512\n",
    "epochs = 5\n",
    "\n",
    "# Données aléatoires\n",
    "X = torch.randn(N, in_dim)\n",
    "y = torch.randint(0, num_classes, (N,))\n",
    "ds = TensorDataset(X, y)\n",
    "loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=(device.type==\"cuda\"))\n",
    "\n",
    "# -------------------------\n",
    "# Simple FFN\n",
    "# -------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    seen = 0\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    for step, (xb, yb) in enumerate(loader, 1):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        seen += bs\n",
    "        loss_sum += loss.item() * bs\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            dt = time.time() - t0\n",
    "            imgs_s = seen / max(dt, 1e-9)\n",
    "            msg = f\"Epoch {epoch} | step {step:4d} | loss {loss_sum/seen:.4f} | {imgs_s:.0f} samples/s\"\n",
    "            if device.type == \"cuda\":\n",
    "                peak = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "                msg += f\" | peak VRAM {peak:.0f} MB\"\n",
    "            print(msg)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    samples_s = seen / max(dt, 1e-9)\n",
    "    msg = f\"\\nEpoch {epoch} done | avg loss {loss_sum/seen:.4f} | throughput {samples_s:.0f} samples/s\"\n",
    "    if device.type == \"cuda\":\n",
    "        peak = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        msg += f\" | peak VRAM {peak:.0f} MB\"\n",
    "    print(msg, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab3b23",
   "metadata": {},
   "source": [
    "<h3> Basic Operations on PyTorch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f56ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version torch: 2.9.1+cpu\n",
      "a, b, c:\n",
      "tensor([1, 2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[-0.8073,  0.5935,  0.9673],\n",
      "        [-0.1903,  0.2044, -0.1103]])\n",
      "dtype, device, shape: torch.float32 cpu torch.Size([2, 3])\n",
      "b[0,1]: tensor(1)\n",
      "b slice: tensor([[1, 2],\n",
      "        [4, 5],\n",
      "        [7, 8]])\n",
      "view reshape: torch.Size([3, 4]) torch.Size([2, 6])\n",
      "unsqueeze/squeeze: torch.Size([1, 12]) torch.Size([12])\n",
      "flatten: torch.Size([12])\n",
      "permute: torch.Size([4, 2, 3])\n",
      "cat dim0: torch.Size([4, 3])\n",
      "stack dim0: torch.Size([2, 2, 3])\n",
      "u + 1: tensor([2., 3., 4.])\n",
      "broadcast add: tensor([[2., 3., 4.],\n",
      "        [3., 4., 5.]])\n",
      "matmul: tensor([[ 0.4739,  0.4555],\n",
      "        [-0.9184, -2.1857],\n",
      "        [-0.5198,  3.8290]]) shape: torch.Size([3, 2])\n",
      "sum, mean, min, max: -3.723116636276245 -0.18615582585334778 -2.5148234367370605 1.3412643671035767\n",
      "argmax dim0: tensor([2, 1, 2, 0])\n",
      "mask sum positive: 9\n",
      "masked select: tensor([0.4465, 0.5806, 1.2030, 0.7499, 0.1512, 1.3413, 0.2629, 0.0161, 0.9788])\n",
      "in-place add_: tensor([2., 3., 4.])\n",
      "grad: tensor([2., 4., 6.])\n",
      "numpy: [1. 2. 3.] <class 'numpy.ndarray'>\n",
      "astype (float->int): tensor([1, 2], dtype=torch.int32)\n",
      "clamp: tensor([0.0000, 0.2000, 3.0000, 3.0000])\n",
      "round: tensor([-2.,  0.,  4.,  4.])\n",
      "unique: tensor([-1.5000,  0.2000,  3.7000])\n",
      "sort: tensor([-1.5000,  0.2000,  3.7000,  3.7000])\n",
      "einsum (trace): tensor(-0.3977)\n",
      "Exemple résumé OK\n"
     ]
    }
   ],
   "source": [
    "print(\"Version torch:\", torch.__version__)\n",
    "\n",
    "# Création\n",
    "a = torch.tensor([1, 2, 3])                    # à partir d'une liste (dtype inféré)\n",
    "b = torch.arange(0, 9).reshape(3, 3)           # arange + reshape\n",
    "c = torch.randn(2, 3)                          # aléatoire normale\n",
    "zeros = torch.zeros(2, 2)\n",
    "ones = torch.ones(2, 2)\n",
    "eye = torch.eye(3)\n",
    "from_numpy = torch.from_numpy(np.array([10, 20, 30]))  # depuis numpy (même mémoire si CPU)\n",
    "\n",
    "print(\"a, b, c:\", a, b, c, sep=\"\\n\")\n",
    "\n",
    "# Propriétés\n",
    "print(\"dtype, device, shape:\", c.dtype, c.device, c.shape)\n",
    "\n",
    "# Indexing & slicing\n",
    "print(\"b[0,1]:\", b[0, 1])\n",
    "print(\"b slice:\", b[:, 1:3])\n",
    "\n",
    "# Reshape / view / flatten / squeeze / unsqueeze\n",
    "x = torch.arange(12)\n",
    "print(\"view reshape:\", x.view(3, 4).shape, x.reshape(2, 6).shape)\n",
    "print(\"unsqueeze/squeeze:\", x.unsqueeze(0).shape, x.unsqueeze(0).squeeze().shape)\n",
    "print(\"flatten:\", x.flatten().shape)\n",
    "\n",
    "# Transpose / permute\n",
    "m = torch.randn(2, 3, 4)\n",
    "print(\"permute:\", m.permute(2, 0, 1).shape)\n",
    "\n",
    "# Concat / stack\n",
    "p1 = torch.randn(2, 3)\n",
    "p2 = torch.randn(2, 3)\n",
    "print(\"cat dim0:\", torch.cat([p1, p2], dim=0).shape)\n",
    "print(\"stack dim0:\", torch.stack([p1, p2], dim=0).shape)\n",
    "\n",
    "# Arithmétique (élément-wise) et broadcasting\n",
    "u = torch.tensor([1.0, 2.0, 3.0])\n",
    "v = torch.tensor([[1.0], [2.0]])\n",
    "print(\"u + 1:\", u + 1)\n",
    "print(\"broadcast add:\", u + v)   # broadcasting\n",
    "\n",
    "# Matmul / @\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 2)\n",
    "print(\"matmul:\", A @ B, \"shape:\", (A @ B).shape)\n",
    "\n",
    "# Reductions\n",
    "t = torch.randn(5, 4)\n",
    "print(\"sum, mean, min, max:\", t.sum().item(), t.mean().item(), t.min().item(), t.max().item())\n",
    "print(\"argmax dim0:\", t.argmax(dim=0))\n",
    "\n",
    "# Comparaisons & masque logique\n",
    "mask = t > 0\n",
    "print(\"mask sum positive:\", mask.sum().item())\n",
    "print(\"masked select:\", t[t > 0])\n",
    "\n",
    "# In-place ops (attention aux gradients)\n",
    "z = torch.tensor([1.0, 2.0, 3.0])\n",
    "z.add_(1.0)   # modifie z\n",
    "print(\"in-place add_:\", z)\n",
    "\n",
    "# Clone / detach / requires_grad / backward\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = (x ** 2).sum()\n",
    "y.backward()            # dy/dx = 2*x\n",
    "print(\"grad:\", x.grad)\n",
    "\n",
    "xd = x.detach().clone() # détaché du graphe\n",
    "\n",
    "# Conversion vers numpy (doit être sur CPU)\n",
    "arr = x.cpu().detach().numpy()\n",
    "print(\"numpy:\", arr, type(arr))\n",
    "\n",
    "# Déplacement device / dtype conversion\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA disponible, envoi sur GPU\")\n",
    "    A_cuda = A.to('cuda')\n",
    "    A_cpu = A_cuda.to('cpu')\n",
    "\n",
    "print(\"astype (float->int):\", torch.tensor([1.2, 2.8]).to(torch.int))\n",
    "\n",
    "# Utilitaires: clamp, round, sort, unique\n",
    "vals = torch.tensor([-1.5, 0.2, 3.7, 3.7])\n",
    "print(\"clamp:\", vals.clamp(0, 3))\n",
    "print(\"round:\", vals.round())\n",
    "print(\"unique:\", vals.unique())\n",
    "print(\"sort:\", vals.sort().values)\n",
    "\n",
    "# Autres utiles: einsum, torch.nn.functional ops\n",
    "print(\"einsum (trace):\", torch.einsum('ii->', torch.randn(3, 3)))\n",
    "\n",
    "# Remise à zéro des gradients\n",
    "x.grad.zero_()\n",
    "\n",
    "# Exemple rapide résumé\n",
    "print(\"Exemple résumé OK\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d437f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce82adef",
   "metadata": {},
   "source": [
    "<h1> <center> A comparison with Numpy :  </center> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdcd747",
   "metadata": {},
   "source": [
    "Some exercises to train :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7167e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
