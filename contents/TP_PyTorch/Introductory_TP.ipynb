{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe8af4b",
   "metadata": {},
   "source": [
    "<h1> <center> An introductory notebook to PyTorch module. </center ></h1>\n",
    "\n",
    "This notebook will cover the basics of PyTorch, a popular deep learning framework. We will go through the following topics:\n",
    "\n",
    "-  Installation of PyTorch\n",
    "- Tensors in PyTorch\n",
    "- Basic operations on Tensors\n",
    "- Autograd: Automatic differentiation\n",
    "- Building a simple neural network\n",
    "- Training the neural network\n",
    "- Evaluating the model\n",
    "- Saving and loading models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c12a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cec970e5",
   "metadata": {},
   "source": [
    "<h3> Setting PyTorch </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c70bbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823ba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab3b23",
   "metadata": {},
   "source": [
    "<h3> Basic Operations on PyTorch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f56ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version torch: 2.9.1+cpu\n",
      "a, b, c:\n",
      "tensor([1, 2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[-0.8073,  0.5935,  0.9673],\n",
      "        [-0.1903,  0.2044, -0.1103]])\n",
      "dtype, device, shape: torch.float32 cpu torch.Size([2, 3])\n",
      "b[0,1]: tensor(1)\n",
      "b slice: tensor([[1, 2],\n",
      "        [4, 5],\n",
      "        [7, 8]])\n",
      "view reshape: torch.Size([3, 4]) torch.Size([2, 6])\n",
      "unsqueeze/squeeze: torch.Size([1, 12]) torch.Size([12])\n",
      "flatten: torch.Size([12])\n",
      "permute: torch.Size([4, 2, 3])\n",
      "cat dim0: torch.Size([4, 3])\n",
      "stack dim0: torch.Size([2, 2, 3])\n",
      "u + 1: tensor([2., 3., 4.])\n",
      "broadcast add: tensor([[2., 3., 4.],\n",
      "        [3., 4., 5.]])\n",
      "matmul: tensor([[ 0.4739,  0.4555],\n",
      "        [-0.9184, -2.1857],\n",
      "        [-0.5198,  3.8290]]) shape: torch.Size([3, 2])\n",
      "sum, mean, min, max: -3.723116636276245 -0.18615582585334778 -2.5148234367370605 1.3412643671035767\n",
      "argmax dim0: tensor([2, 1, 2, 0])\n",
      "mask sum positive: 9\n",
      "masked select: tensor([0.4465, 0.5806, 1.2030, 0.7499, 0.1512, 1.3413, 0.2629, 0.0161, 0.9788])\n",
      "in-place add_: tensor([2., 3., 4.])\n",
      "grad: tensor([2., 4., 6.])\n",
      "numpy: [1. 2. 3.] <class 'numpy.ndarray'>\n",
      "astype (float->int): tensor([1, 2], dtype=torch.int32)\n",
      "clamp: tensor([0.0000, 0.2000, 3.0000, 3.0000])\n",
      "round: tensor([-2.,  0.,  4.,  4.])\n",
      "unique: tensor([-1.5000,  0.2000,  3.7000])\n",
      "sort: tensor([-1.5000,  0.2000,  3.7000,  3.7000])\n",
      "einsum (trace): tensor(-0.3977)\n",
      "Exemple résumé OK\n"
     ]
    }
   ],
   "source": [
    "print(\"Version torch:\", torch.__version__)\n",
    "\n",
    "# Création\n",
    "a = torch.tensor([1, 2, 3])                    # à partir d'une liste (dtype inféré)\n",
    "b = torch.arange(0, 9).reshape(3, 3)           # arange + reshape\n",
    "c = torch.randn(2, 3)                          # aléatoire normale\n",
    "zeros = torch.zeros(2, 2)\n",
    "ones = torch.ones(2, 2)\n",
    "eye = torch.eye(3)\n",
    "from_numpy = torch.from_numpy(np.array([10, 20, 30]))  # depuis numpy (même mémoire si CPU)\n",
    "\n",
    "print(\"a, b, c:\", a, b, c, sep=\"\\n\")\n",
    "\n",
    "# Propriétés\n",
    "print(\"dtype, device, shape:\", c.dtype, c.device, c.shape)\n",
    "\n",
    "# Indexing & slicing\n",
    "print(\"b[0,1]:\", b[0, 1])\n",
    "print(\"b slice:\", b[:, 1:3])\n",
    "\n",
    "# Reshape / view / flatten / squeeze / unsqueeze\n",
    "x = torch.arange(12)\n",
    "print(\"view reshape:\", x.view(3, 4).shape, x.reshape(2, 6).shape)\n",
    "print(\"unsqueeze/squeeze:\", x.unsqueeze(0).shape, x.unsqueeze(0).squeeze().shape)\n",
    "print(\"flatten:\", x.flatten().shape)\n",
    "\n",
    "# Transpose / permute\n",
    "m = torch.randn(2, 3, 4)\n",
    "print(\"permute:\", m.permute(2, 0, 1).shape)\n",
    "\n",
    "# Concat / stack\n",
    "p1 = torch.randn(2, 3)\n",
    "p2 = torch.randn(2, 3)\n",
    "print(\"cat dim0:\", torch.cat([p1, p2], dim=0).shape)\n",
    "print(\"stack dim0:\", torch.stack([p1, p2], dim=0).shape)\n",
    "\n",
    "# Arithmétique (élément-wise) et broadcasting\n",
    "u = torch.tensor([1.0, 2.0, 3.0])\n",
    "v = torch.tensor([[1.0], [2.0]])\n",
    "print(\"u + 1:\", u + 1)\n",
    "print(\"broadcast add:\", u + v)   # broadcasting\n",
    "\n",
    "# Matmul / @\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 2)\n",
    "print(\"matmul:\", A @ B, \"shape:\", (A @ B).shape)\n",
    "\n",
    "# Reductions\n",
    "t = torch.randn(5, 4)\n",
    "print(\"sum, mean, min, max:\", t.sum().item(), t.mean().item(), t.min().item(), t.max().item())\n",
    "print(\"argmax dim0:\", t.argmax(dim=0))\n",
    "\n",
    "# Comparaisons & masque logique\n",
    "mask = t > 0\n",
    "print(\"mask sum positive:\", mask.sum().item())\n",
    "print(\"masked select:\", t[t > 0])\n",
    "\n",
    "# In-place ops (attention aux gradients)\n",
    "z = torch.tensor([1.0, 2.0, 3.0])\n",
    "z.add_(1.0)   # modifie z\n",
    "print(\"in-place add_:\", z)\n",
    "\n",
    "# Clone / detach / requires_grad / backward\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = (x ** 2).sum()\n",
    "y.backward()            # dy/dx = 2*x\n",
    "print(\"grad:\", x.grad)\n",
    "\n",
    "xd = x.detach().clone() # détaché du graphe\n",
    "\n",
    "# Conversion vers numpy (doit être sur CPU)\n",
    "arr = x.cpu().detach().numpy()\n",
    "print(\"numpy:\", arr, type(arr))\n",
    "\n",
    "# Déplacement device / dtype conversion\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA disponible, envoi sur GPU\")\n",
    "    A_cuda = A.to('cuda')\n",
    "    A_cpu = A_cuda.to('cpu')\n",
    "\n",
    "print(\"astype (float->int):\", torch.tensor([1.2, 2.8]).to(torch.int))\n",
    "\n",
    "# Utilitaires: clamp, round, sort, unique\n",
    "vals = torch.tensor([-1.5, 0.2, 3.7, 3.7])\n",
    "print(\"clamp:\", vals.clamp(0, 3))\n",
    "print(\"round:\", vals.round())\n",
    "print(\"unique:\", vals.unique())\n",
    "print(\"sort:\", vals.sort().values)\n",
    "\n",
    "# Autres utiles: einsum, torch.nn.functional ops\n",
    "print(\"einsum (trace):\", torch.einsum('ii->', torch.randn(3, 3)))\n",
    "\n",
    "# Remise à zéro des gradients\n",
    "x.grad.zero_()\n",
    "\n",
    "# Exemple rapide résumé\n",
    "print(\"Exemple résumé OK\")\n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
