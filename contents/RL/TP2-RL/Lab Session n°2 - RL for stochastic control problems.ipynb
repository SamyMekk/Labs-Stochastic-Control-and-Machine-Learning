{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0436e0c9",
   "metadata": {},
   "source": [
    "<h1><center> Lab Session n¬∞2 : RL for stochastic control problems  </center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h2> üìå Objectives: </h2>\n",
    "\n",
    "In the **first part** of the **lab session**, you will solve two stochastic control problems in finance,  the Markowitz portfolio optimization problem and the market impact problem.  You will explore and implement the algorithms introduced in the lecture using Python and its scientific libraries.\n",
    "\n",
    "The **second part** of the **lab session** will be devoted to mathematical questions on solving linear quadratic control problems.\n",
    "\n",
    "\n",
    "<h2>üìö Goal of the Lab: </h2>\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Undertand and implement some **reinforcement algorithms** to solve stochastic control problems arising in finance.\n",
    "\n",
    "- Derive explicitly the **optimal policy** and **value function** arising in the **Linear quadratic** control problems in continuous time.\n",
    "\n",
    "\n",
    "<h2> üóÇÔ∏è Lab Structure and assignments: </h2>\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "\n",
    "**1. [On the Markowitz problem](#Markowitz-Applications)**  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.1 [Problem formulation](#Markowitz-problemformulation)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.2 [Optimal Q-function](#Markowitz-Q-function)\n",
    "\n",
    "\n",
    "\n",
    "**2. [On the Market impact problem](#MarketImpact-Applications)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.1 [Problem formulation](#MarketImpact-problemformulation)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.2 [Optimal Q-function](#MarketImpact-Q-function)\n",
    "\n",
    "\n",
    "**3. [Mathematical questions](#Mathematical-Questions)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; [Linear quadratic case in continuous time](#Mathematical-Questions-LQ-case)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**4. [References](#references)**  \n",
    "\n",
    "\n",
    "This lab will include **mathematics** and/or **coding** questions indicated by ‚ùì. **Your answers** indicated by ‚úèÔ∏è will count for your final grade of the course, with a weight to be determined later with respect to the project. Note that the project will have a significant higher weight in the final grade.\n",
    "\n",
    "\n",
    "**Mathematics Questions**\n",
    "\n",
    "- You can answer directly in the **Jupyter notebook** using LaTeX (compatible with Markdown).\n",
    "\n",
    "\n",
    "**Coding Questions**\n",
    "\n",
    "-  Complete the corresponding code sections **directly in the notebook**.\n",
    "-  **Code readability**, **quality**, and **clarity of comments** will be taken into account in the **grading**.\n",
    "\n",
    "\n",
    "If you choose this lab, you will have to send your work by e-mail at [samy.mekkaoui@polytechnique.edu](mailto:samy.mekkaoui@polytechnique.edu). The submission deadline will be announced later during the course.\n",
    "\n",
    "\n",
    "\n",
    "<h2>‚ÑπÔ∏è Other informations: </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Key References**: If you want to go deeper on the use of RL methods for solving stochastic control problems in finance, you can look at the section [References](#references). <br> <br>\n",
    "\n",
    "\n",
    "\n",
    "- **Contact**: If you find any mistakes in this notebook, or have any other feedback or questions, please feel free to e-mail me at [samy.mekkaoui@polytechnique.edu](mailto:samy.mekkaoui@polytechnique.edu).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc8288",
   "metadata": {},
   "source": [
    "<a id=Markowitz-Applications></a>\n",
    "\n",
    "<h1> <center> 1: The Markowitz Problem  </center> </h1>\n",
    "\n",
    "\n",
    "\n",
    "<a  id=Markowitz-problemformulation></a>\n",
    "\n",
    "<h2> 1.1 Problem formulation : </h2>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be12f6",
   "metadata": {},
   "source": [
    "\n",
    "<a id=Markowitz-Q-function></a>\n",
    "\n",
    "<h2> 1.2 Optimal Q-function : </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681308ef",
   "metadata": {},
   "source": [
    "<a id=MarketImpact-Applications></a>\n",
    "\n",
    "<h1> <center> 2: On the Market Impact Problem  </center> </h1>\n",
    "\n",
    "\n",
    "\n",
    "<a  id=MarketImpact-problemformulation></a>\n",
    "\n",
    "<h2> 2.1 Problem formulation : </h2>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895091c",
   "metadata": {},
   "source": [
    "<a id=MarketImpact-Q-function></a>\n",
    "\n",
    "<h2> 2.2 Optimal Q-function : </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e7fb2",
   "metadata": {},
   "source": [
    "<a id=MarketImpact-Applications></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a01fb3b",
   "metadata": {},
   "source": [
    "<a id=Mathematical-Questions></a>\n",
    "<h1> <center> 3. Mathematical Questions </center> </h1>\n",
    "\n",
    "We recall that given a policy $\\pi = (\\pi_s)_{t \\leq s \\leq T}$, i.e., a $\\mathcal{P}(A)$-valued map,  the value function $V^{\\pi}$ of the stochastic control problem is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(t,x) &= \\mathbb{E}_{\\pi} \\Big[ \\int_t^T f(s,X_s,\\pi_s) ds + \\lambda \\mathcal{E}(\\pi(s,X_s))  g(X_T) \\big | X_t = x \\Big], \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\Big[ \\int_t^T f(s,X_s,\\pi_s) ds - \\lambda \\text{log} (\\pi(s,X_s,\\alpha_s))  g(X_T) \\big | X_t = x \\Big]\n",
    "\\end{align}\n",
    "$$\n",
    "where the controlled state process is given for $\\alpha=(\\alpha_s)_{t \\leq s \\leq T} \\sim \\pi$ by \n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "dX_s &= b(X_s, \\alpha_s) ds + \\sigma(X_s, \\alpha_s) dW_s, \\quad s \\in [t,T], \\notag \\\\\n",
    "X_t &= x, \\notag\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "and the optimal value function is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(t,x) &= \\sup_{\\pi} V^{\\pi}(t,x). \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "  ‚ùì **Question 3.1**:  Given a policy $\\pi$, recall the Bellman relation for $V^{\\pi}$ and the Bellman optimality principle for $V$.\n",
    "\n",
    "  ‚ùì **Question 3.2**:  Apply It√¥'s formula to the process $V^{\\pi}(s,X_s)$ between $t$ and $t+h$ for $h > 0$ and show that $V^{\\pi}$ satisfies the   following linear PDE:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial V^{\\pi}}{\\partial t} (t,x) + \\int_{A} \\big[ H(x,a, \\nabla_x V^{\\pi}(t,x), D_x^2 V^{\\pi}(t,x)) - \\lambda \\text{log}(\\pi(t,x,a)) \\big] \\pi(t,x,a)\\nu(da) &= 0,\\notag \\\\\n",
    "V^{\\pi}(T,x) &= g(x),\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "where the map $H$ is defined as\n",
    "$$\n",
    "\\begin{align}\n",
    "H(x,a,p,M) = b(x,a) \\cdot p + \\frac{1}{2} \\text{tr}(\\sigma \\sigma^{\\top}(x,a) M) + f(x,a), \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "for $x \\in \\mathbb{R}^d$, $a \\in A$, $p \\in \\mathbb{R}^d$, and $M \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "\n",
    "‚ùì **Question 3.3**:  Deduce the Bellman equation satisfied by the optimal value function $V$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial V}{\\partial t} (t,x) + \\underset{\\pi \\in \\mathcal{P}(A)}{\\text{ sup }} \\int_{A} \\big[ H(x,a, \\nabla_x V^{\\pi}(t,x), D_x^2 V^{\\pi}(t,x)) - \\lambda \\text{log}(\\pi(t,x,a)) \\big] \\pi(t,x,a)\\nu(da) &= 0,\\notag \\\\\n",
    "V(T,x) &= g(x),\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "‚ùì **Question 3.4**:  Recall from the course the form of the optimal randomized policy $\\pi^{\\star}$ in terms of the value function $V$ and show that it leads to the following form of the Bellman equation for $V$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial V}{\\partial t} (t,x) + \\lambda \\text{ log } \\bigg[ \\int_{A} \\text{ exp } (   \\frac{1}{\\lambda} H(x,a, D_x V(t,x)),D^2_x V(t,x)) \\nu(d a ) \\bigg] &= 0,\\notag \\\\\n",
    "V(T,x) &= g(x),\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<a id=Mathematical-Questions-LQ-case></a>\n",
    "\n",
    "<h2> Linear quadratic case in continuous time : </h2>\n",
    "\n",
    "\n",
    "\n",
    "Suppose that the coefficients of the state dynamics and the reward functions are given by\n",
    "$$\\begin{align}\n",
    "\\begin{cases}\n",
    "b(x,a) &= Bx + Ca, \\notag \\\\\n",
    "\\sigma(x,a) &= Dx + Fa, \\notag \\\\\n",
    "f(x,a) &=  x^{\\top} Q x + a^{\\top} N a , \\notag \\\\\n",
    "g(x) &= x^{\\top} P x, \\notag\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "with $x \\in \\mathbb{R}^d$, $a \\in \\mathbb{R}^m$, and where $B$, $C$, $D$, $F$, $Q$, $N$, and $P$ are matrices of appropriate dimensions, with $Q$, $N$, and $P$ symmetric positive definite.\n",
    "\n",
    "\n",
    "‚ùì **Question 3.5**:  Make the ansatz that the optimal value function is quadratic in the state variable, ie., of the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(t,x) = x^{\\top} K(t) x + R(t), \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "for some deterministic functions $K : [0,T] \\rightarrow \\mathbb{S}_{+}^d$ and $R : [0,T] \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "- Show that the map $H$ is given by the form in the course (Slide 59 of Lecture 1).\n",
    "- Show using Question 3.1.4 that the Bellman equation for $V$ is satisfies if $K$ and $R$ satisfy the system of ODEs given in the course (Slide 60 of Lecture 1).\n",
    "- Show that the optimal feedback control policy $\\pi^{\\star}$ is given by a Gaussian distribution with mean and covariance matrix given in the course (Slide 60 of Lecture 1). Discuss the impact of $\\lambda$ on the optimal policy and compare it to the case $\\lambda = 0$ (without entropy regularization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c166f7",
   "metadata": {},
   "source": [
    "<a id=Mathematical-Questions></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fef6e4",
   "metadata": {},
   "source": [
    "<a id=references></a>\n",
    "<h2> <center> 4. References   </center> </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- R. Sutton and A. Barto: Introduction to reinforcement learning, second edition 2016, available [here](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).\n",
    "\n",
    "- Y. Jia and X.Y. Zhou: Policy gradient and Actor-Critic learning in continuous time and space: theory and algorithms, 2022, Journal of Machine Learning and Research. available [here](https://arxiv.org/abs/2111.11232).\n",
    "\n",
    "-  Y. Jia and X.Y. Zhou: q-Learning in continuous time, 2023, Journal of Machine Learning and Research. available [here](https://arxiv.org/abs/2207.00713).\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppStatApp_TP2",
   "language": "python",
   "name": "appstatapp_tp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
