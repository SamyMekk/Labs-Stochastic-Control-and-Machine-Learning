{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0436e0c9",
   "metadata": {},
   "source": [
    "<h1><center> Lab Session n¬∞2 : RL for stochastic control problems  </center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<h2> üìå Objectives: </h2>\n",
    "\n",
    "In the **first part** of the **lab session**, you will solve one stochastic contol problem in finance : A simple market impact model.  You will explore and implement the algorithms introduced in the lecture using Python and its scientific libraries.\n",
    "\n",
    "The **second part** of the **lab session** will be devoted to mathematical questions on solving linear quadratic control problems in continuous time.\n",
    "\n",
    "\n",
    "<h2>üìö Goal of the Lab: </h2>\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Undertand and implement some **reinforcement algorithms** to solve stochastic control problems arising in finance.\n",
    "\n",
    "- Derive explicitly the **optimal policy** and **value function** arising in the **Linear quadratic** control problems in continuous time.\n",
    "\n",
    "\n",
    "<h2> üóÇÔ∏è Lab Structure and assignments: </h2>\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**1. [On the Market impact problem](#MarketImpact-Applications)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.1 [Problem formulation](#MarketImpact-problemformulation)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.2 [Q-learning and Sarsa algorithms](#MarketImpact-Q-function)\n",
    "\n",
    "\n",
    "**2. [Mathematical questions](#Mathematical-Questions)**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; [Linear quadratic case in continuous time](#Mathematical-Questions-LQ-case)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**3. [References](#references)**  \n",
    "\n",
    "\n",
    "This lab will include **mathematics** and/or **coding** questions indicated by ‚ùì. **Your answers** indicated by ‚úèÔ∏è will count for your final grade of the course, with a weight to be determined later with respect to the project. Note that the project will have a significant higher weight in the final grade.\n",
    "\n",
    "\n",
    "**Mathematics Questions**\n",
    "\n",
    "- You can answer directly in the **Jupyter notebook** using LaTeX (compatible with Markdown).\n",
    "\n",
    "\n",
    "**Coding Questions**\n",
    "\n",
    "-  Complete the corresponding code sections **directly in the notebook**.\n",
    "-  **Code readability**, **quality**, and **clarity of comments** will be taken into account in the **grading**.\n",
    "\n",
    "\n",
    "If you choose this lab, you will have to send your work by e-mail at [samy.mekkaoui@polytechnique.edu](mailto:samy.mekkaoui@polytechnique.edu). The submission deadline will be announced later during the course.\n",
    "\n",
    "\n",
    "\n",
    "<h2>‚ÑπÔ∏è Other informations: </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Key References**: If you want to go deeper on the use of RL methods for solving stochastic control problems in finance, you can look at the section [References](#references). <br> <br>\n",
    "\n",
    "\n",
    "\n",
    "- **Contact**: If you find any mistakes in this notebook, or have any other feedback or questions, please feel free to e-mail me at [samy.mekkaoui@polytechnique.edu](mailto:samy.mekkaoui@polytechnique.edu).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c9a161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681308ef",
   "metadata": {},
   "source": [
    "<a id=MarketImpact-Applications></a>\n",
    "\n",
    "<h1> <center> 1: On the Market Impact Problem  </center> </h1>\n",
    "\n",
    "\n",
    "\n",
    "<a  id=MarketImpact-problemformulation></a>\n",
    "\n",
    "<h2> 1.1 Problem formulation : </h2>\n",
    "\n",
    "Assume that the broker has to sell $N$ blocks of shares with $n$ shares in each block.  \n",
    "\n",
    "\n",
    "In each step, the agent has four possible actions $a_t= a^{(i)}$ where $ i \\in \\lbrace 0, 1, 2 ,3 \\rbrace$ and $a^{(i)}=i$ measure the number of blocks of shares sold at time $t$. The updated equation for the stock number of blocks is then given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_{t+1} = (X_t-a_t)_{+}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The trades influence the stock price dynamics through a linear market impact as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_{t+1} = S_t e^{(1- \\nu)a_t)} + \\sigma S_t \\epsilon_t,\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\nu$ represents a market impact parameter.\n",
    "\n",
    "The reward function is given by\n",
    "\n",
    "$r_t= n a_t S_t - \\lambda n \\text{ Var }[ S_{t+1} X_{t+1} ]$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 1000\n",
    "NUM_BLOCKS = 10\n",
    "NUM_S      = 12  #number of discrete values of S\n",
    "NUM_TIME_STEPS = 10\n",
    "dt         = 1 # time step\n",
    "sigma      = 0.1 # volatility of the stock\n",
    "nu         = 1 # market impact parameter\n",
    "S0         = 1 # initial stock price\n",
    "lmbda      = 0.01 # risk aversion parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b467f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1# Probability for exploration\n",
    "eta = 0.5# Step size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef355f",
   "metadata": {},
   "source": [
    "In the list below, we define \n",
    "\n",
    "- The potential actions values, i.e., the number of stocks to sell in the current time and state.\n",
    "- The state vector for the initial state : [number of blocks of shares, stock price, time]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [0, 1, 2, 3]\n",
    "START = [NUM_BLOCKS - 1, S0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776edf5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ‚ùì **Question 1.1**: Fill in the function **transition** below the next state and reward obtained from taking action $a$ in state $s$ at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(\n",
    "    state,   # list: [inventory, price_state, time]\n",
    "    action,  # int: number of blocks sold\n",
    "):\n",
    "    \"\"\"\n",
    "    One-step transition for an optimal execution MDP.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : list of length 3\n",
    "        state[0] : inventory (number of blocks)\n",
    "        state[1] : price state (discretized)\n",
    "        state[2] : time\n",
    "\n",
    "    action : int\n",
    "        Number of blocks sold at the current time step\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (next_state, reward) :  State at the next time step and immediate reward\n",
    "    \"\"\"\n",
    "\n",
    "    X = state[0]   # inventory\n",
    "    S = state[1]   # discretized price state\n",
    "    t = state[2]   # time\n",
    "\n",
    "     # You can't sell more stock than you have\n",
    "    if action > X: \n",
    "        action = X\n",
    "\n",
    "    X_next = ### \n",
    "\n",
    "    t_next = t + dt\n",
    " \n",
    "    S_next = (\n",
    "        S * np.exp(1 - nu * action)\n",
    "        + sigma * S * np.sqrt(dt) * np.random.randn()\n",
    "    )\n",
    "    S_next = int(np.clip(np.ceil(S_next), 0, NUM_S - 1))\n",
    "\n",
    "    next_state = [X_next, S_next,t + dt]\n",
    "   \n",
    "    reward = ###\n",
    "\n",
    "    return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c73695",
   "metadata": {},
   "source": [
    "\n",
    " ‚ùì **Question 1.2**: Implement in the function **epsilon_greedy** below the $\\epsilon$-greedy strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e712d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy for action selection\n",
    "def epsilon_greedy(\n",
    "    state,                      # Current state [inventory, price_state, time]\n",
    "    q_value,                    # Q-value table Q(s,a)\n",
    "    eps=epsilon                 # Exploration rate Œµ\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        action: integer action selected according to Œµ-greedy policy\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Exploration vs exploitation decision ---\n",
    "    # Draw a Bernoulli(Œµ) random variable\n",
    "    if ______________________________:\n",
    "        action = ______________________\n",
    "\n",
    "    # --- Step 2: Greedy action selection ---\n",
    "    # Extract Q-values at current state and select a maximizer\n",
    "    else:\n",
    "        values_ = ______________________\n",
    "        action = ______________________\n",
    "\n",
    "    # --- Step 3: Enforce inventory constraint ---\n",
    "    # Ensure action does not exceed available inventory\n",
    "    if ______________________________:\n",
    "        action = ______________________\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895091c",
   "metadata": {},
   "source": [
    "<a id=MarketImpact-Q-function></a>\n",
    "\n",
    "<h2> 1.2 Q-learning and Sarsa algorithms : </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6874a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sarsa_qlearning(\n",
    "    num_episodes=1000,\n",
    "    num_runs=100,\n",
    "    epoch_size=25\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare SARSA and Q-learning by averaging episode returns\n",
    "    over multiple independent runs.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Initialize arrays to store average episode rewards\n",
    "    # --------------------------------------------------\n",
    "    avg_rewards_sarsa = np.zeros(num_episodes)\n",
    "    avg_rewards_qlearning = np.zeros(num_episodes)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Loop over independent runs\n",
    "    # --------------------------------------------------\n",
    "    for run_id in tqdm(range(num_runs)):\n",
    "\n",
    "        # Initialize Q-tables\n",
    "        q_table_sarsa = np.zeros(\n",
    "            (NUM_BLOCKS, NUM_S, NUM_TIME_STEPS, len(ACTIONS))\n",
    "        )\n",
    "        q_table_qlearning = np.copy(q_table_sarsa)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Loop over episodes\n",
    "        # --------------------------------------------------\n",
    "        for episode in range(num_episodes):\n",
    "\n",
    "            # Epsilon decay schedule (piecewise exponential)\n",
    "            eps = epsilon * ((1 - epsilon) ** (episode // epoch_size))\n",
    "\n",
    "            avg_rewards_sarsa[episode] += sarsa(\n",
    "                q_table_sarsa, eps=eps\n",
    "            )\n",
    "            avg_rewards_qlearning[episode] += q_learning(\n",
    "                q_table_qlearning, eps=eps\n",
    "            )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Average over runs\n",
    "    # --------------------------------------------------\n",
    "    avg_rewards_sarsa /= num_runs\n",
    "    avg_rewards_qlearning /= num_runs\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Plot learning curves\n",
    "    # --------------------------------------------------\n",
    "    plt.plot(avg_rewards_sarsa, label=\"SARSA\")\n",
    "    plt.plot(avg_rewards_qlearning, label=\"Q-learning\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Average return per episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    return q_table_sarsa, q_table_qlearning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b15e4",
   "metadata": {},
   "source": [
    "\n",
    " ‚ùì **Question 1.5**: Comment the plots obtained for the Q-learning and Sarsa algorithms. Compare the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e7fb2",
   "metadata": {},
   "source": [
    "<a id=MarketImpact-Applications></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fcca23",
   "metadata": {},
   "source": [
    "\n",
    " ‚ùì **Question 1.6**: Fill the function **optimal_policy** to compute the optimal policy from the learned Q-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1361b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_optimal_policy(q_value):\n",
    "    \"\"\"\n",
    "    Display the greedy policy associated with a learned Q-table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_value : array-like\n",
    "        Q-value table of shape (NUM_BLOCKS, NUM_S, NUM_TIME_STEPS, |A|)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 1: Initialize the policy container\n",
    "    # --------------------------------------------------\n",
    "    optimal_policy = ______________________________\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 2: Extract greedy action at each state\n",
    "    # --------------------------------------------------\n",
    "    for i in range(NUM_BLOCKS):\n",
    "        for j in range(NUM_S):\n",
    "            for k in range(NUM_TIME_STEPS):\n",
    "                ______________________________\n",
    "                optimal_policy[i, j, k] = ______________________________\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Step 3: Display policy by time slice\n",
    "    # --------------------------------------------------\n",
    "    for k in range(NUM_TIME_STEPS):\n",
    "        print(\"========= time step \" + str(k) + \" ======\")\n",
    "        print(\" price: 1,2,3,4,5,6,7,8,9,10,11,12\")\n",
    "\n",
    "        for i in range(NUM_BLOCKS):\n",
    "            row_str = ______________________________\n",
    "            for j in range(NUM_S):\n",
    "                row_str += ______________________________\n",
    "            print(row_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " ‚ùì **Question 1.6**: Fill the function **optimal_policy** to compute the optimal policy from the learned Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59f26e",
   "metadata": {},
   "source": [
    "\n",
    " ‚ùì **Question 1.7**: Print the optimal policy obtained from the Q-learning and Sarsa algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde6224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a01fb3b",
   "metadata": {},
   "source": [
    "<a id=Mathematical-Questions></a>\n",
    "<h1> <center> 3. Mathematical Questions </center> </h1>\n",
    "\n",
    "We recall that given a policy $\\pi = (\\pi_s)_{t \\leq s \\leq T}$, i.e., a $\\mathcal{P}(A)$-valued map,  the value function $V^{\\pi}$ of the stochastic control problem is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(t,x) &= \\mathbb{E}_{\\pi} \\Big[ \\int_t^T f(s,X_s,\\pi_s) ds + \\lambda \\mathcal{E}(\\pi(s,X_s))  g(X_T) \\big | X_t = x \\Big], \\\\\n",
    "&= \\mathbb{E}_{\\pi} \\Big[ \\int_t^T f(s,X_s,\\pi_s) ds - \\lambda \\text{log} (\\pi(s,X_s,\\alpha_s))  g(X_T) \\big | X_t = x \\Big]\n",
    "\\end{align}\n",
    "$$\n",
    "where the controlled state process is given for $\\alpha=(\\alpha_s)_{t \\leq s \\leq T} \\sim \\pi$ by \n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "dX_s &= b(X_s, \\alpha_s) ds + \\sigma(X_s, \\alpha_s) dW_s, \\quad s \\in [t,T], \\notag \\\\\n",
    "X_t &= x, \\notag\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "and the optimal value function is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(t,x) &= \\sup_{\\pi} V^{\\pi}(t,x). \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "  ‚ùì **Question 3.1**:  Given a policy $\\pi$, recall the Bellman relation for $V^{\\pi}$ and the Bellman optimality principle for $V$.\n",
    "\n",
    "  ‚ùì **Question 3.2**:  Apply It√¥'s formula to the process $V^{\\pi}(s,X_s)$ between $t$ and $t+h$ for $h > 0$ and show that $V^{\\pi}$ satisfies the   following linear PDE:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial V^{\\pi}}{\\partial t} (t,x) + \\int_{A} \\big[ H(x,a, \\nabla_x V^{\\pi}(t,x), D_x^2 V^{\\pi}(t,x)) - \\lambda \\text{log}(\\pi(t,x,a)) \\big] \\pi(t,x,a)\\nu(da) &= 0,\\notag \\\\\n",
    "V^{\\pi}(T,x) &= g(x),\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "where the map $H$ is defined as\n",
    "$$\n",
    "\\begin{align}\n",
    "H(x,a,p,M) = b(x,a) \\cdot p + \\frac{1}{2} \\text{tr}(\\sigma \\sigma^{\\top}(x,a) M) + f(x,a), \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "for $x \\in \\mathbb{R}^d$, $a \\in A$, $p \\in \\mathbb{R}^d$, and $M \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "\n",
    "‚ùì **Question 3.3**:  Deduce the Bellman equation satisfied by the optimal value function $V$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial V}{\\partial t} (t,x) + \\underset{\\pi \\in \\mathcal{P}(A)}{\\text{ sup }} \\int_{A} \\big[ H(x,a, \\nabla_x V^{\\pi}(t,x), D_x^2 V^{\\pi}(t,x)) - \\lambda \\text{log}(\\pi(t,x,a)) \\big] \\pi(t,x,a)\\nu(da) &= 0,\\notag \\\\\n",
    "V(T,x) &= g(x),\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "‚ùì **Question 3.4**:  Recall from the course the form of the optimal randomized policy $\\pi^{\\star}$ in terms of the value function $V$ and show that it leads to the following form of the Bellman equation for $V$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial V}{\\partial t} (t,x) + \\lambda \\text{ log } \\bigg[ \\int_{A} \\text{ exp } (   \\frac{1}{\\lambda} H(x,a, D_x V(t,x)),D^2_x V(t,x)) \\nu(d a ) \\bigg] &= 0,\\notag \\\\\n",
    "V(T,x) &= g(x),\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<a id=Mathematical-Questions-LQ-case></a>\n",
    "\n",
    "<h2> Linear quadratic case in continuous time : </h2>\n",
    "\n",
    "\n",
    "\n",
    "Suppose that the coefficients of the state dynamics and the reward functions are given by\n",
    "$$\\begin{align}\n",
    "\\begin{cases}\n",
    "b(x,a) &= Bx + Ca, \\notag \\\\\n",
    "\\sigma(x,a) &= Dx + Fa, \\notag \\\\\n",
    "f(x,a) &=  x^{\\top} Q x + a^{\\top} N a , \\notag \\\\\n",
    "g(x) &= x^{\\top} P x, \\notag\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "with $x \\in \\mathbb{R}^d$, $a \\in \\mathbb{R}^m$, and where $B$, $C$, $D$, $F$, $Q$, $N$, and $P$ are matrices of appropriate dimensions, with $Q$, $N$, and $P$ symmetric positive definite.\n",
    "\n",
    "\n",
    "‚ùì **Question 3.5**:  Make the ansatz that the optimal value function is quadratic in the state variable, ie., of the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V(t,x) = x^{\\top} K(t) x + R(t), \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "for some deterministic functions $K : [0,T] \\rightarrow \\mathbb{S}_{+}^d$ and $R : [0,T] \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "- Show that the map $H$ is given by the form in the course (Slide 59 of Lecture 1).\n",
    "- Show using Question 3.1.4 that the Bellman equation for $V$ is satisfies if $K$ and $R$ satisfy the system of ODEs given in the course (Slide 60 of Lecture 1).\n",
    "- Show that the optimal feedback control policy $\\pi^{\\star}$ is given by a Gaussian distribution with mean and covariance matrix given in the course (Slide 60 of Lecture 1). Discuss the impact of $\\lambda$ on the optimal policy and compare it to the case $\\lambda = 0$ (without entropy regularization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c166f7",
   "metadata": {},
   "source": [
    "<a id=Mathematical-Questions></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fef6e4",
   "metadata": {},
   "source": [
    "<a id=references></a>\n",
    "\n",
    "<h2> <center> 4. References   </center> </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- R. Sutton and A. Barto: Introduction to reinforcement learning, second edition 2016, available [here](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).\n",
    "\n",
    "- Y. Jia and X.Y. Zhou: Policy gradient and Actor-Critic learning in continuous time and space: theory and algorithms, 2022, Journal of Machine Learning and Research. available [here](https://arxiv.org/abs/2111.11232).\n",
    "\n",
    "-  Y. Jia and X.Y. Zhou: q-Learning in continuous time, 2023, Journal of Machine Learning and Research. available [here](https://arxiv.org/abs/2207.00713).\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
