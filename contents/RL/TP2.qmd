# Lab work n°2 

You can download **Lab Work n°2: RL for stochastic control problems** as a Jupyter Notebook [here](TP2-RL/Lab%20Session%20n%C2%B02%20-%20RL%20for%20stochastic%20control%20problems.ipynb ).^[If you end-up with a **.txt** file, download it and rename it as a **.ipynb** file.] 

All the necessary informations are already included in the **notebook**^[There will be **coding** and **math** questions.].  Below is a brief summary of the lab content and some expected results.


## Summary of the lab 


::: {#alg-description .algorithm}


<span class="para-title"> Content : </span>

The Lab work is divided into 2 parts :
 
- The first part is devoted to the implementation of some RL algorithms for solving a **market impact** problem. We give an **expected plot** for the evolution of the sum of the rewards for the **market impact** problem


![](https://raw.githubusercontent.com/SamyMekk/Labs-Stochastic-Control-and-Machine-Learning/main/images/RL/SarsaQlearningtables.png){fig-align="center" width="40%"}


- The second  part is devoted to retrieve the results of the course on  the optimal policy for the linear quadratic control problems in continuous time, i.e. to recover the associated **Ricatti** equations and the associated **optimal** policy.

:::

## Towards the open ended mini-project


::: {#alg-description .algorithm}

The list of potential projects for the ** Reinforcement Learning for stochastic control problems** can be found in the PDF file by clicking [here]().

- Please do not forget to indicate your name on the Excel sheet [here](https://docs.google.com/spreadsheets/d/1H2ZDF7BeY9pMEMaEnG-JyBBCpDcvnkJcoZzA4M7mCMo/edit?gid=0#gid=0).
- Recall that the **submission deadline** for the mini-project and the answers to the lab session^[**No PDF submission** is required for the lab session answers. The answers should be written directly in the **Jupyter notebook** and submitted along with the documents associated with the mini-project.] is set to **31 March 2026**.
:::




