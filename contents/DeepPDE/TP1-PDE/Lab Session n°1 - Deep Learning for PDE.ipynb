{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82797175",
   "metadata": {},
   "source": [
    "<h1> <center> Lab Session : Deep Learning for PDE </center> </h1>\n",
    "\n",
    "\n",
    "\n",
    "<h2> üìå Objectives: </h2>\n",
    "\n",
    "This lab session aims to provide a hands-on implementation of the methods presented during the course, focusing on how Deep Learning techniques can be applied to solve Partial Differential Equations (PDEs) that commonly arise in financial mathematics. You will explore and implement algorithms introduced in lectures using Python and its scientific libraries.\n",
    "\n",
    "\n",
    "<h2>üìö Goal of the Lab: </h2>\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand and implement the **Deep Galerkin Method (DGM)** and the **Deep BSDE method**.\n",
    "\n",
    "- Apply these methods to solve financial PDEs such as those arising in option pricing and risk management.\n",
    "\n",
    "- Analyze numerical results and compare them to analytical or benchmark solutions.\n",
    "    \n",
    "<h2> üóÇÔ∏è Lab Structure and assignments: </h2>\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "**1. [On the DGM](#galerkin-Applications)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.1 [Methodology and Implementation](#galerkin-reminder)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.2 [Numerical results on various PDE](#galerkin-results)  \n",
    "\n",
    "**2. [On the Deep BSDE Solver](#deepBSDE-Applications)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.1 [Methodology and Implementation](#deepBSDE-reminder)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.2 [Numerical results on various PDE](#deepBSDE-results)  \n",
    "\n",
    "**3. [References](#references)**  \n",
    "\n",
    " Each subsection of the lab will include **mathematics** and/or **coding** questions indicated by ‚ùì. **Your answers** indicated by ‚úèÔ∏è will count for your final grade of the course, with a weight to be determined later with respect to the project. Note that the project will have a significant higher weight in the final grade.\n",
    "\n",
    "**Mathematics Questions**\n",
    "\n",
    "- You can answer directly in the **Jupyter notebook** using LaTeX (compatible with Markdown).\n",
    "\n",
    "\n",
    "\n",
    "**Coding Questions**\n",
    "\n",
    "-  Complete the corresponding code sections **directly in the notebook**.\n",
    "-  **Code readability**, **quality**, and **clarity of comments** will be taken into account in the **grading**.\n",
    "\n",
    "\n",
    "If you choose this lab, you will have to send your work by e-mail at [samy.mekkaoui@polytechnique.edu](mailto:samy.mekkaoui@polytechnique.edu). The submission deadline will be announced later during the course.\n",
    "\n",
    "<h2>‚ÑπÔ∏è Other informations: </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Key References**: If you want to go deeper on the use of Deep Learning methods for analyzing PDE, you can look at the section [References](#references). <br> <br>\n",
    "\n",
    "\n",
    "\n",
    "- **Contact**: If you find any mistakes in this notebook, or have any other feedback or questions, please feel free to e-mail me at [samy.mekkaoui@polytechnique.edu](mailto:samy.mekkaoui@polytechnique.edu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe458721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import math\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b8df7",
   "metadata": {},
   "source": [
    "<a id=galerkin-Applications></a>\n",
    "\n",
    "<center> <h1> On  the DGM </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7602da",
   "metadata": {},
   "source": [
    "<a id=galerkin-reminder></a>\n",
    "\n",
    "<h2> 1.1 : Methodology and Implementation: </h2>\n",
    "\n",
    "In this subsection, you are going to implement the Deep Galerkin method. For this, you are going first to implement a Neural network which will map a function from $(t,s) \\in \\mathbb{R}^2 \\to \\mathbb{R}$ with 2 hidden layers with $100$ neurons and with activation function given by `tanh`. Finally, from the last layer to the ouput, you just go through a linear function.\n",
    "\n",
    "\n",
    " ‚ùì **Question 1.1.1**: Fill in the definition of the network and especially in the `nn.Sequential` part the neural network using `nn.Linear` and the `tanh` activation function. \n",
    " \n",
    "\n",
    "`Hint`: To implement the neural network, you are going to use the PyTorch module. The code structure is really convenient using the `nn.Sequential` method so we encourage you to use it. For the ones who don't know about PyTorch, you can look at this video [here](https://www.youtube.com/watch?time_continue=1220&v=IC0_FRiX-sw&embeds_referring_euri=https%3A%2F%2Fdocs.pytorch.org%2F&source_ve_path=MTM5MTE3LDEzOTExNywyODY2Ng).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08713dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Neural Network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Sequential(# Write your code here ...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "v_net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a4464",
   "metadata": {},
   "source": [
    "Now, from your previous neural work denoted by $v$ which will take as input $(t,x) \\in [0,T] \\times \\mathbb{R}^+$, you are going to use the automatic differentiation of Pytorch using the `torch.autograd` module.\n",
    "\n",
    "\n",
    " ‚ùì **Question 1.1.2**: Fill in the function compute_derivatives the code to return the derivatives of your neural network. You will use the `torch.autograd` module.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute first and second derivatives of the network output\n",
    "def compute_derivatives(\n",
    "    net: torch.nn.Module,         # Neural network taking (T, S) as input\n",
    "    S: torch.Tensor,              # Underlying asset(s), shape: (batch_size, input_dim)\n",
    "    T: torch.Tensor               # Time to maturity, shape: (batch_size, 1)\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        output:          net(T, S)\n",
    "        d_output_dT:     ‚àÇoutput / ‚àÇT\n",
    "        d_output_dS:     ‚àÇoutput / ‚àÇS\n",
    "        d2_output_dS2:   ‚àÇ¬≤output / ‚àÇS¬≤\n",
    "    \"\"\"\n",
    "      # Ensure T and S require gradients\n",
    "      # Concatenate inputs and forward pass through net\n",
    "      # Compute first-order gradients w.r.t T and S\n",
    "      # Compute second-order gradient w.r.t S\n",
    "\n",
    "    return   # output, d_output_dT, d_output_dS, d2_output_dS2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5275179",
   "metadata": {},
   "source": [
    "<a id=galerkin-results></a>\n",
    "\n",
    "\n",
    "<h2> 1.2 : Numerical results on various PDE: </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24910e9f",
   "metadata": {},
   "source": [
    "<h3> The Black-Scholes PDE: </h3>\n",
    "\n",
    "We give you below the classes `Call` and `Forward` which for a given strike, maturity and quantity can compute the price and the payoff. They will be useful in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ca71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.02\n",
    "sigma = 0.2\n",
    "\n",
    "class Call():\n",
    "    def __init__(self, K, T,q):\n",
    "        self.T = T # Maturity\n",
    "        self.K = K # Strike\n",
    "        self.q = q # Quantity\n",
    "\n",
    "    def d_plus(self, St, t):\n",
    "        return (torch.log(St/self.K) + (r + 0.5 * sigma**2) * torch.tensor(self.T-t)) / (sigma * torch.sqrt(torch.tensor(self.T-t)))\n",
    "\n",
    "    def d_minus(self, St, t):\n",
    "        return self.d_plus(St, t) - sigma * torch.sqrt(torch.tensor(self.T-t))\n",
    "\n",
    "    def delta(self, St, t):\n",
    "        delta = self.q * Normal(0, 1).cdf(self.d_plus(St, t))\n",
    "        return delta\n",
    "\n",
    "    def price(self, St, t):\n",
    "        return self.q * (St * Normal(0, 1).cdf(self.d_plus(St, t)) - self.K * Normal(0, 1).cdf(self.d_minus(St, t)) * torch.exp(torch.tensor(-r*(self.T-t))))\n",
    "    \n",
    "    def g(self,St,K):\n",
    "        return torch.max(St-self.K,torch.tensor(0.0))\n",
    "class Forward():\n",
    "    def __init__(self, K, T,q):\n",
    "        self.T = T # Maturit√©\n",
    "        self.K = K # Strike\n",
    "        self.q = q # Quantit√©\n",
    "    def price(self,St,t):\n",
    "        return self.q* (St - self.K*torch.exp(torch.tensor(-r*(self.T-t))))\n",
    "    def g(self,St,K):\n",
    "        return St - self.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc53fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "CallOption = Call(110, 1,1)\n",
    "ForwardOption=Forward(100,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9812fe",
   "metadata": {},
   "source": [
    " ‚ùì **Question 1.2.1**: Recall the Black-Scholes PDE for an European option given by his payoff $g(S_T)$ for a given function $g$.\n",
    "\n",
    "‚úèÔ∏è **Your answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144e627",
   "metadata": {},
   "source": [
    " ‚ùì **Question 1.2.2**: You are going to implement the loss function in the case of the Black-Scholes PDE. For this, you are going to get the loss due to the residual PDE by using the `compute_derivatives` function and then you are going to get the terminal_loss using the terminal function $g$. Fill the missing code in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fc78bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import torch'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss function for training the value network to solve the Black-Scholes PDE\n",
    "def loss_fn(\n",
    "    S: torch.Tensor,                  # Underlying asset price(s), shape: (batch_size, 1)\n",
    "    T: torch.Tensor,                  # Time(s), shape: (batch_size, 1)\n",
    "    value_net: torch.nn.Module,       # Neural network approximating V(T, S)\n",
    "    Option,               # Portfolio object ( can be either a Call or a Forward)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        total_loss: scalar loss combining PDE residual and terminal condition error\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Compute value and derivatives ---\n",
    "    # Compute V(T, S), ‚àÇV/‚àÇT, ‚àÇV/‚àÇS, ‚àÇ¬≤V/‚àÇS¬≤ using autograd\n",
    "    v, dv_dt, dv_dS, d2v_dS2 = compute_derivatives(value_net, S, T)\n",
    "\n",
    "    # --- Step 2: Compute Black-Scholes operator applied to V ---\n",
    "    # A_v = r * S * ‚àÇV/‚àÇS + 0.5 * œÉ¬≤ * S¬≤ * ‚àÇ¬≤V/‚àÇS¬≤\n",
    "    # Full residual: ‚àÇV/‚àÇT + A_v - r * V\n",
    "\n",
    "    loss_pde = ...  # torch.mean((dv_dt + A_v - r * v) ** 2)\n",
    "\n",
    "    # --- Step 3: Compute terminal condition loss ---\n",
    "    # Extract the strike price from the Portfolio contract\n",
    "    strike = Option.K # 0\n",
    "\n",
    "    # Compute the true terminal value using the payoff function\n",
    "    terminal_target = ...\n",
    "\n",
    "    # Build input with time set to maturity T_max\n",
    "    T_terminal = ...\n",
    "\n",
    "    # Evaluate network prediction at T = T_max\n",
    "    v_terminal_pred = ...\n",
    "\n",
    "    # Compare prediction to target at maturity\n",
    "    loss_terminal = ...  # torch.mean((v_terminal_pred - terminal_target) ** 2)\n",
    "\n",
    "    # --- Step 4: Combine both loss terms ---\n",
    "    total_loss = loss_pde + loss_terminal\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d777051",
   "metadata": {},
   "source": [
    " ‚ùì **Question 1.2.3**: You are going to implement now the training procedure. First, recall how does the training sampling works for the DGM method.\n",
    " \n",
    " ‚úèÔ∏è **Your answer**: \n",
    " \n",
    " \n",
    " Then, in our case you are going to sample using a  grid of time points $(T,S)$ as we are working in a 1D setting. For this, you are going to discretize with the following parameters settings :\n",
    " \n",
    "- $T_{max}$ = 1.0\n",
    "- $S_{max}$ = 200.0\n",
    "- $S_{min}$ = 20.0\n",
    "- $t_{points}$ = 100\n",
    "- $s_{points}$ = 100\n",
    "- $batch_{size}$ = 1000 \n",
    "- epochs = 2000\n",
    "- $learning_{rate}$ = 1e-2.\n",
    "\n",
    "Of course, you can change this parameters after checking that your code runs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Create the input grid of (T, S) points for training and validation ---\n",
    "\n",
    "# Create a grid of time points from 0.01 to T_max using torch.linspace\n",
    "T = ...  \n",
    "\n",
    "# Create a grid of asset prices from S_min to S_max\n",
    "S = ... \n",
    "\n",
    "# Create a full meshgrid of (T, S) combinations using torch.meshgrid()\n",
    "T_grid, S_grid = ...  \n",
    "\n",
    "# Flatten the grid to get a list of (T, S) pairs\n",
    "T_flat = ...  \n",
    "S_flat = ...  \n",
    "\n",
    "# --- Step 2: Split data into training and validation sets ---\n",
    "\n",
    "# Randomly shuffle all indices using torch.randperm\n",
    "indices = ...  \n",
    "\n",
    "# Use 80% for training and 20% for validation\n",
    "train_indices = ...\n",
    "val_indices = ...\n",
    "\n",
    "# Select the training and validation inputs accordingly\n",
    "T_train, S_train = ...\n",
    "T_val, S_val = ...\n",
    "\n",
    "# --- Step 3: Initialize the optimizer ---\n",
    "\n",
    "# Use Adam optimizer on the parameters of the value network\n",
    "optimizer = ...  \n",
    "\n",
    "# --- Step 4: Initialize lists to store loss values ---\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# --- Step 5: Training loop ---\n",
    "\n",
    "# Loop over the number of epochs with a progress bar\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "\n",
    "    # Reset gradients from previous step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute training loss using current network\n",
    "    loss_train = ...  \n",
    "\n",
    "    # Backpropagation to compute gradients\n",
    "    loss_train.backward()\n",
    "\n",
    "    # Update network parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute validation loss (no backward pass here)\n",
    "    loss_val = ...  \n",
    "\n",
    "    # Store loss values for later plotting\n",
    "    train_losses.append(loss_train.item())\n",
    "    val_losses.append(loss_val.item())\n",
    "\n",
    "    # Print progress every 100 epochs\n",
    "    if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "        tqdm.write(f\"Epoch {epoch}, Train Loss: {loss_train.item():.4f}, Val Loss: {loss_val.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40b4cd",
   "metadata": {},
   "source": [
    "Now, you can plot your **training** and **validation** losses with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Plot the loss curves ---\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Plot training and validation loss across epochs\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Evolution of the loss during the Learning Process\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d40a74",
   "metadata": {},
   "source": [
    "You are now going to evaluate your neural network by defining points on the grid $(t,s)$ and by plotting in a 3D map $(t,s,v(t,s))$.\n",
    "\n",
    " ‚ùì **Question 1.2.4**: Compute the 3D maps for a call option and for a forward option.  What is the expected shape for the function $s \\mapsto C(t,s)$ for a fixed $t$ and for the function $s \\mapsto F(t,s)$ where $C$ denotes the price of a call option and $F$ the price of a forward option. Do you indeed observe theses behaviours ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Set the trained network to evaluation mode ---\n",
    "v_net.eval()  # Disable dropout, etc.\n",
    "\n",
    "# --- Step 2: Create a grid of time and asset values for plotting ---\n",
    "\n",
    "# Generate time values from 0 to T_max\n",
    "T_values = ...  \n",
    "\n",
    "# Generate asset values from 0 to S_max\n",
    "S_values = ...  \n",
    "\n",
    "# Create a meshgrid of all (T, S) pairs\n",
    "T_grid, S_grid = ... \n",
    "\n",
    "# Stack and convert the grid into a PyTorch tensor of shape (N, 2)\n",
    "points = ...  \n",
    "\n",
    "# --- Step 3: Evaluate the network over the grid of inputs ---\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Predict V(t, S) at all grid points\n",
    "    v_pred = ...  \n",
    "\n",
    "# --- Step 4: Plot the 3D surface of the value function ---\n",
    "\n",
    "# Create a matplotlib figure\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "# Add a 3D subplot\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "\n",
    "# Plot the surface V(t, S)\n",
    "surf = ...  # ax.plot_surface(..., cmap='viridis')\n",
    "\n",
    "# Label the axes\n",
    "ax.set_xlabel('Time t')\n",
    "ax.set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c4961",
   "metadata": {},
   "source": [
    "<h3> A PDE for the Credit Valuation Adjustment (CVA): </h3>\n",
    "\n",
    "The Credit Valuation Adjustment (CVA) is a financial quantity computed by the banking industry which aims to represent the losses for them in the case of the default of one of their counterparties.\n",
    "\n",
    "The CVA is usually parametrized by the following quantiites :\n",
    "\n",
    "\n",
    "- $\\tau$ the time default of the counterparty \n",
    "- $R^C$ the recovery rate in case of default of the counterparty\n",
    "- $V_t$ the time at $t$ of the portfolio value such that $(V_t)^+$ correspond the exposure of the portfolio for the bank.\n",
    "- $T$ the time maturity of the portfolio.\n",
    "\n",
    "\n",
    "\n",
    "In a default intensity model (ie where we assume that $\\tau^c \\sim \\mathcal{E}(\\lambda)$ where $\\mathcal{E}(\\lambda)$ means that $\\mathbb{Q}(\\tau^c \\geq t) = e^{-\\lambda t}$ , the process $(CVA)_{0 \\leq t \\leq T}$ can be represented as:\n",
    "\n",
    "\\begin{align}\\tag{1}\n",
    "CVA_t = \\mathbb{1}_{\\tau_C > t}  (1-R^C) \\mathbb{E}^{\\mathbb{Q}} [ \\int_{t}^{T} e^{- (r+\\lambda)(s-t)}(V_s)^+ \\lambda d s | \\mathcal{F}_t] \n",
    "\\end{align}\n",
    "where $\\mathcal{F}_t$ is the filtration given by the asset price process on the market $(S_t)_{ 0 \\leq \\leq T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5bb2d",
   "metadata": {},
   "source": [
    " ‚ùì **Question 1.2.5**: Show that in the case of the Black-Scholes and assuming that $\\tau^c \\sim \\mathcal{E}(\\lambda)$ where $\\mathcal{E}(\\lambda)$ means that $\\mathbb{Q}(\\tau^c \\geq t) = e^{-\\lambda t}$ is independant from $(V_t)_{0 \\leq t \\leq T}$ , the process $CVA_t$ can be represented as a suitable function $\\phi$ such that \n",
    " \n",
    " $$CVA_t = \\mathbb{1}_{\\tau_C \\geq t} \\phi(t,S_t) \\hspace{0.1 cm} dt \\otimes  d \\mathbb{P} a.e ,$$ \n",
    " \n",
    " where the function $[0,T] \\times \\mathbb{R}^d \\ni (t,x) \\mapsto \\phi(t,x)$ is solution to the following PDE:\n",
    " \n",
    " \n",
    "\\begin{align}\\label{eq : PDE CVA}\n",
    "    \\partial_t \\phi(t,x) + \\mathcal{L}\\phi(t,x) - (r+ \\lambda^C)\\phi(t,x) + (1-R^c)(V(t,x))^+\\lambda^C &= 0, \\quad (t,x) \\in [0,T( \\times \\mathbb{R}_*^+ \\\\\n",
    "    \\phi(T,x) &= 0, \\quad x \\in \\mathbb{R}_{*}^+ \\notag \n",
    "\\end{align}\n",
    "\n",
    "where we used that $V_t$ is a function of $V(t,S_t)$.\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f908e",
   "metadata": {},
   "source": [
    "In our numerical experiments, we will take the portfolio to be a standard call option with the same characteristics as the `CallOption` object defined before.\n",
    "\n",
    "\n",
    "\n",
    "Moreover, we will assume $R^C=0$ and show the results for 2 different values of $\\lambda^C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612d2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaC = 0.1 # Default Intensity for the counterparty\n",
    "R = 0 # Recovery Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112eda6a",
   "metadata": {},
   "source": [
    "‚ùì **Question 1.2.6**: Make the fewer updates of the code above to update to our current setting. You should only update one function. Give the name of the function and how to update it.\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4c92a",
   "metadata": {},
   "source": [
    "‚ùì **Question 1.2.7**: Give another formula for $CVA_t$ which doesn't involve any conditional expectation for the case of an European call.\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986633a",
   "metadata": {},
   "source": [
    "‚ùì **Question 1.2.8**: With the help of the 2 previous questions, plot the functions $(t,s) \\mapsto CVA(t,s)$ for an European call option with $\\lambda^C = 0.1$ and $\\lambda^C = 0.4$.  Explain how does the value of $\\lambda^C$ impact the value of $CVA$ and give an explanation. Moreover, explain also why the behavior of the mapping $s \\mapsto CVA(t,s)$ for a fixed $t$ is expected.\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249f1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d611c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f1957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "522ce3d5",
   "metadata": {},
   "source": [
    "<h3> A system of coupled PDE arising from risk management metrics: </h3>\n",
    "\n",
    "\n",
    "The Funding Valuation Adjustment (FVA) and (Capital Valuation Adjustment) (KVA) are financial quantities computed by the banking industry which aims respectively the cost of funding and the cost of remuneration for the shareholders.\n",
    "\n",
    "\n",
    "In a toy model, we can show that $KVA$ and $FVA$ are  solution to the following coupled systems of PDE associated respectively with $w$ and $v$ :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{v}}{\\partial{t}}+\\mathcal{L}v +  \\lambda(max(\\alpha f  \\sigma  S  |\\frac{\\partial{v}}{\\partial{S}}- \\Delta_{bs} |,w) + v - u_{bs})^{-}-rv=0 \\quad (t,x) \\in ]0,T[\\times  \\mathbb{R}_{*}^+ \\tag{10} \\\\\n",
    "\\frac{\\partial{w}}{\\partial{t}}+\\mathcal{L}w+ h max(\\alpha f \\sigma S | \\frac{\\partial{v}}{\\partial{S}}- \\Delta_{bs}|,w)-(r+h)w=0, \\quad  (t,x) \\in ]0,T[ \\times  \\mathbb{R}_{*}^+ \\tag{11} \\\\\n",
    "v(T,x)=w(T,x)=0  \\quad x \\in \\mathbb{R}_{*}^+ \\notag \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where :\n",
    "\n",
    "- $h$ represents a dividend rate\n",
    "- $\\alpha$ represents a mishedge parameter\n",
    "- $\\lambda$ is a funding rate \n",
    "- $f$ is a quantile level\n",
    "- $u_{bs}$ and $\\Delta_{bs}$ represent the call and delta price of a single call option of same characteristics as before.\n",
    "\n",
    " \n",
    " For the numerical experiments, we will take $\\alpha = 0.3$,  $\\lambda=0.02$, $f=1.2$ and $h=0.1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dff6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3\n",
    "hparam = 0.1\n",
    "f = 1.2\n",
    "lambd = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f3146",
   "metadata": {},
   "source": [
    "‚ùì **Question 1.2.9**: Make the fewer updates of the code above to update to our current setting. Again, you should only update one function. Give the name of the function and what explain how you can handle the fact that we are dealing with a coupled PDE systems.\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3bf4d",
   "metadata": {},
   "source": [
    "‚ùì **Question 1.2.10**: With the help of the 2 previous questions, plot the functions $(t,s) \\mapsto CVA(t,s)$ for an European call option. Explain how does the value of $\\lambda$, the funding rate impacts the value of the $FVA$ and the $KVA$ and how does the value of $h$, the dividend rate impact the value of the $KVA$ and the $FVA$ and give an explanation of this behaviour.\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2e4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a907c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d4d4c3e",
   "metadata": {},
   "source": [
    "<a id=deepBSDE-Applications></a>\n",
    "\n",
    "<center> <h1> On  the Deep BSDE Solver </h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0c22e",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.1**: What is the main difference between the DGM and the Deep BSDE Solver from a mathematical perspective ?\n",
    "\n",
    "‚úèÔ∏è **Your answer**: \n",
    "\n",
    " ‚ùì **Question 2.1.2**: Discuss the name $\\textit{Deep BSDE Solver}$ given by the algorithm from his implementation procedure.  Do you know other algorithms similar to the $\\textit{Deep BSDE Solver}$ ? If yes, give the main difference between theses algorithms and the  $\\textit{Deep BSDE Solver}$.\n",
    " \n",
    "`Hint`: Think about the word `backward'.\n",
    " \n",
    "‚úèÔ∏è **Your answer**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e53e6",
   "metadata": {},
   "source": [
    "<a id=deepBSDE-reminder></a>\n",
    "\n",
    "<h2> 2.1 : Methodology and Implementation: </h2>\n",
    "\n",
    "In this subsection, you are going to implement the Deep BSDE Solver. For this, we are going to do it through a class implementation.\n",
    "\n",
    "You are going to implement the class Model by using the class `fbsde` which defines all the objects defining a FBSDE equation. First you will need to define a neural network which will approximate $(Z_{t_i})_{i =0 , \\ldots, n }$. For this, you will start from a layer of size $dim_x +1$ and goes to size $dim_y \\times dim_d$ and the mapping from the process $X$ to $Z$ will be down with the function  `phi`.\n",
    "Moreover, you will add $y_0$ as a trainable parameter of the neural network which will be learnt during the learning process.\n",
    " \n",
    " \n",
    " `Hint` For setting $y_0$ as a trainable parameter, use the function `nn.Parameter`from PyTorch.\n",
    " \n",
    " \n",
    "Once this is done, you will implement the function  `forward` which will gives forward and backward process terminal values $X_T$ and $Y_T$ by using the Euler-Maruyama scheme which needs  to sample paths of Brownian motion.\n",
    "\n",
    "Then, you will implement the class `BSDEsolver` where will you complete the function `train` by using the loss function in the Deep BSDE Solver using the function $g$ and the associated paths $(X_T,Y_T)$ from your `forward` function in the class `Model` . You should return 2 lists : one for the evolution of the loss during the training processa and the other one for the evolution of the trainable parameter $y_0$.\n",
    "\n",
    " `Hint` Use the  `nn.MSELoss ` function to use the quadratic loss in the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# FBSDE problem definition\n",
    "class fbsde():\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_0: torch.Tensor,               # (dim_x,) initial state\n",
    "        b: callable,                     # b(t: float, x: Tensor) ‚Üí Tensor(batch_size, dim_x)\n",
    "        sigma: callable,                 # sigma(t: float, x: Tensor) ‚Üí Tensor(batch_size, dim_x, dim_d)\n",
    "        f: callable,                     # f(t: float, x: Tensor, y: Tensor, z: Tensor) ‚Üí Tensor(batch_size, dim_y)\n",
    "        g: callable,                     # g(x: Tensor) ‚Üí Tensor(batch_size, dim_y)\n",
    "        T: float,                        # final time\n",
    "        dim_x: int, dim_y: int, dim_d: int\n",
    "    ):\n",
    "        self.x_0 = x_0.to(device)\n",
    "        self.b = b\n",
    "        self.sigma = sigma\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "        self.T = T\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.dim_d = dim_d\n",
    "\n",
    "\n",
    "# Neural network model for approximating Y and Z\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        equation: fbsde,                # FBSDE problem\n",
    "        dim_h: int                      # Hidden layer size\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "         #Write your code here for the definition of the neural network and add y_0 as a trainable parameter\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_size: int,               # Number of samples\n",
    "        N: int                         # Time discretization steps\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            x: Tensor(batch_size, dim_x) ‚Äî terminal state\n",
    "            y: Tensor(batch_size, dim_y) ‚Äî terminal Y value\n",
    "        \"\"\"\n",
    "        def phi(x): # This function will approximate Z through the neural network.\n",
    "           \n",
    "        # Simulate forward and backward paths using Euler-Maruyama and phi network\n",
    "        return x,y\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Solver that trains the model to estimate the BSDE solution\n",
    "class BSDEsolver():\n",
    "    def __init__(\n",
    "        self,\n",
    "        equation: fbsde,               # FBSDE problem\n",
    "        dim_h: int                     # Hidden layer size\n",
    "    ):\n",
    "        self.model = Model(equation,dim_h).to(device)\n",
    "        self.equation = equation\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        batch_size: int,              # Batch size for training\n",
    "        N: int,                       # Time discretization steps\n",
    "        itr: int                      # Number of training iterations\n",
    "    ) -> tuple[list[float], list[float]]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            loss_data: List of training losses per iteration\n",
    "            y0_data: List of Y‚ÇÄ estimates during training\n",
    "        \"\"\"\n",
    "        criterion = # Define the MSELoss here \n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.model.parameters(),lr=1e-2)\n",
    "\n",
    "        loss_data, y0_data = [], []\n",
    "\n",
    "        # Training loop: simulate, compute loss, backprop, optimize\n",
    "        \n",
    "        \n",
    "        return loss_data,y0_data  # loss_data, y0_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f010e",
   "metadata": {},
   "source": [
    "<a id=deepBSDE-results></a>\n",
    "\n",
    "\n",
    "<h2> 2.2 : Numerical results on various PDE: </h2>\n",
    "\n",
    "In this subsection, you are going to test your previous implementation to different types of PDEs.\n",
    "\n",
    "- **Black-Scholes (B-S) PDE** in low and high dimensions.\n",
    "- **Allen-Cahn PDE**.\n",
    "- A PDE from optimal stochastic control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ada1e",
   "metadata": {},
   "source": [
    "<h3> The Black-Scholes PDE: </h3>\n",
    "\n",
    "We now assume a $B-S$ model dynamics with the underlying dynamics $S=(S^1,\\ldots,S^d)$ and $W=(W^1,\\ldots,W^d)$ multidimensional brownian motion given by :\n",
    "$$\n",
    "\\begin{align}\n",
    "dS_t^i = S_t^i ( r dt + \\sigma^i dW_t^i), \\quad S_0^i \\in  (\\mathbb{R}^{+}_{*}), \\quad i =1,\\ldots,d\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Under the option pricing theory in the $B-S$, for an european option with price at time $t$ denoted by $C(t,S_t)$ we know that we have the following PDE for the option price $v$ defined on $[0,T] \\times (\\mathbb{R}^{+}_{*})^d$ as :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\partial_t v + \\mathcal{L}v- rv &= 0 \\quad (t,x) \\in [0,T) \\times (\\mathbb{R}^{+}_{*})^d \\\\\n",
    "v(T,x) &= g(x) \\quad x \\in (\\mathbb{R}^{+}_{*})^d\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the infinitemisal generator is given by :\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}v(t,x) = r x^{\\top} D_x v (t,x) + \\frac{1}{2} \\sigma^2 Tr(xx^{\\top} D^2_x v(t,x))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca274cbd",
   "metadata": {},
   "source": [
    " \n",
    " For the numerical experiments, we will set the following quantities :\n",
    "  \n",
    "- $ x_0 = (1,\\ldots,1) \\in \\mathbb{R}^{dim_x}$\n",
    "- $ K= 1$\n",
    "- $ r=0.05$\n",
    "- $ \\sigma = 0.2 * I_{dim_x}$ where $I_{dim_x}$ is the identity matrix of size $dim_x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4b824",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.3**: In the context of the B-S PDE, what is the associated $f$ function ?\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a987acb",
   "metadata": {},
   "source": [
    "We are now going to test your previous Deep BSDE Solver with various payoff functions $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05d4e9",
   "metadata": {},
   "source": [
    "<h4> A numerical result on a  Basket Call Option under $B-S$ model  in low and high dimension $d$ :  </h4>\n",
    "\n",
    "- $g(x) = (\\sum_{i=1}^{d} x_i- d K)^+$\n",
    "\n",
    "\n",
    "\n",
    " ‚ùì **Question 2.1.4**: Fill in the drift function `b`, the volatility function `sigma`, the running cost function `f` and the terminal function `g` in the case of the Basket Call option?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f99d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters lists\n",
    "\n",
    "r, sigma_value = 0.05, 0.2 \n",
    "\n",
    "dim_x, dim_y, dim_d, dim_h, N, itr, batch_size, K =100, 1, 100, dim_x+10, 20, 2000, 1000, 1   # \n",
    "\n",
    "x_0, T = torch.ones(dim_x), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fe67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift function\n",
    "def b(t, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: (float) Current time\n",
    "        x: (Tensor) Shape: (batch_size, dim_x)\n",
    "    Returns:\n",
    "        Tensor: Shape (batch_size, dim_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    return   \n",
    "\n",
    "# Diffusion (volatility) function\n",
    "def sigma(t, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: (float) Current time\n",
    "        x: (Tensor) Shape: (batch_size, dim_x)\n",
    "    Returns:\n",
    "        Tensor: Shape (batch_size, dim_x, dim_d)\n",
    "    \"\"\"\n",
    "    return  \n",
    "\n",
    "# BSDE driver function (running cost)\n",
    "def f(t, x, y, z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        t: (float) Time\n",
    "        x: (Tensor) Shape: (batch_size, dim_x)\n",
    "        y: (Tensor) Shape: (batch_size, dim_y)\n",
    "        z: (Tensor) Shape: (batch_size, dim_y, dim_d)\n",
    "    Returns:\n",
    "        Tensor: Shape: (batch_size, dim_y)\n",
    "    \"\"\"\n",
    "    return   \n",
    "\n",
    "# Terminal condition (payoff)\n",
    "def g(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (Tensor) Shape: (batch_size, dim_x)\n",
    "    Returns:\n",
    "        Tensor: Shape: (batch_size, dim_y)\n",
    "    \"\"\"\n",
    "    return  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6dce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FBSDE equation\n",
    "equation = fbsde(\n",
    "    x_0=x_0,                # (Tensor) Initial value, shape: (dim_x,)\n",
    "    b=b,                   # (function) Drift\n",
    "    sigma=sigma,           # (function) Diffusion\n",
    "    f=f,                   # (function) Driver\n",
    "    g=g,                   # (function) Terminal condition\n",
    "    T=T,                   # (float) Terminal time\n",
    "    dim_x=dim_x,\n",
    "    dim_y=dim_y,\n",
    "    dim_d=dim_d\n",
    ")\n",
    "\n",
    "# Instantiate BSDE solver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e2e93",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.5**: Plot the evolution of the error during the learning process and the evolution of the initial price of the option $y_0$. \n",
    " \n",
    " \n",
    " `Hint`: Use  the function  `train` in the  `BSDEsolver` class.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61344c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a448d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6af2510",
   "metadata": {},
   "source": [
    "‚ùì **Question 2.1.6**: Compare the true value of a call option with the same parameters given from the BS price function with the price given by the Deep BSDE Solver. \n",
    "\n",
    "You can implement below the true value of a call option in dimension $d=1$ for instance given by the famous B-S formula.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0d41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5b4c21",
   "metadata": {},
   "source": [
    "‚úèÔ∏è **Your answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fec6d",
   "metadata": {},
   "source": [
    "<h4> A numerical result on a Price Put under $B-S$ model in low and high dimension $d$:  </h4>\n",
    "\n",
    "\n",
    "- $g(x) = (d K- \\sum_{i=1}^{d} x_i)^+$\n",
    "\n",
    " ‚ùì **Question 2.1.7**: Fill in the drift function `b`, the volatility function `sigma`, the running cost function `f` and the terminal function `g` in the case of the Basket Put option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1dabd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ded7dd0",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.8**: Plot the evolution of the error during the learning process and the evolution of the initial price of the option $y_0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60276fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08d563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c517d1",
   "metadata": {},
   "source": [
    "‚ùì **Question 2.1.9**: Compare the true value of a put option with the same parameters given from the BS price function with the price given by the Deep BSDE Solver. \n",
    "\n",
    "You can implement below the true value of a put option in dimension $d=1$ for instance given by the famous B-S formula.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ad76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec469a01",
   "metadata": {},
   "source": [
    "‚úèÔ∏è **Your answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65447ec7",
   "metadata": {},
   "source": [
    "<h4> A numerical result for a Binary Call under $B-S$ model in dimension 1 : </h4> \n",
    "\n",
    "\n",
    "- $g(x) = \\mathbb{1}_{x \\geq K}$\n",
    "\n",
    " ‚ùì **Question 2.1.10**: Fill in the drift function `b`, the volatility function `sigma`, the running cost function `f` and the terminal function `g` in the case of the Binary Call option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee816df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339a4a66",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.11**: Plot the evolution of the error during the learning process and the evolution of the initial price of the option $y_0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3bb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c50a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02724c5",
   "metadata": {},
   "source": [
    "‚ùì **Question 2.1.12** : What do you observe during the training process and the value of the price option compared to previously ? How can you explain this behaviour ?\n",
    "\n",
    "‚úèÔ∏è **Your answer**: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166221f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7342cfb7",
   "metadata": {},
   "source": [
    "‚ùì **Question 2.1.13**: Compare the true value of a binary option with the same parameters given from the BS price function with the price given by the Deep BSDE Solver. \n",
    "\n",
    "You can implement below the true value of a binary option in dimension $d=1$ for instance given by the famous B-S formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50941bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9039bcf5",
   "metadata": {},
   "source": [
    "‚úèÔ∏è **Your answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c4693",
   "metadata": {},
   "source": [
    "<h3> The Allen-Cahn PDE: </h3>\n",
    "\n",
    "The Allen-Cahn PDE is given by the following :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\partial_t v + \\Delta_{x} v + v - v^3 &= 0 \\quad (t,x) \\in [0,T(  \\times \\mathbb{R}^d \\notag \\\\\n",
    "v(T,x) &= \\frac{1}{ 2+ \\frac{2}{5} \\lVert x \\rVert^2} \\quad x \\in  \\mathbb{R}^d\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\Delta_x v = \\sum_{i=1}^{d} \\partial^2_{x_i} v$ where we noted $x:=(x_1,\\ldots,x_d)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec222ee7",
   "metadata": {},
   "source": [
    "‚ùì **Question 2.1.14**: In the context of the Allen-Cahn PDE, what are the associated functions $b$,$\\sigma$, $f$ and $g$ ?\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b37149",
   "metadata": {},
   "source": [
    "\n",
    "In the numerical experiments, we will set $T=\\frac{3}{10}$ with $x_0=(0,\\ldots,0) \\in \\mathbb{R}^d$ and $d=100$ and try to recover the true estimate value of the PDE which can be shown to be $\\approx$ 0.052802."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7617409",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x, dim_y, dim_d, dim_h, N, itr, batch_size = 100, 1, 100, dim_x + 10, 20, 3000, 1000\n",
    "\n",
    "x_0, T = torch.zeros(dim_x), 3/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786a2d5",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.15**: Fill in the drift function `b`, the volatility function `sigma`, the running cost function `f` and the terminal function `g` in the case of the Allen Cahn PDE\n",
    " \n",
    "  `Hint`: Use  the functions from the `torch` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf87640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t,x):\n",
    "    return ...\n",
    "\n",
    "def sigma(t, x):\n",
    "    return ...\n",
    "    #return torch.sqrt(torch.abs(x)).reshape(batch_size, dim_x, dim_d)\n",
    "\n",
    "\n",
    "def f(t, x, y, z):\n",
    "    return ...\n",
    "\n",
    "\n",
    "def g(x):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FBSDE equation\n",
    "equation = fbsde(\n",
    "    x_0=x_0,                # (Tensor) Initial value, shape: (dim_x,)\n",
    "    b=b,                   # (function) Drift\n",
    "    sigma=sigma,           # (function) Diffusion\n",
    "    f=f,                   # (function) Driver\n",
    "    g=g,                   # (function) Terminal condition\n",
    "    T=T,                   # (float) Terminal time\n",
    "    dim_x=dim_x,\n",
    "    dim_y=dim_y,\n",
    "    dim_d=dim_d\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b7e6d",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.16**: Plot the evolution of the error during the learning process and the evolution of the initial value $v(0,x_0)$ in the case of the Allen-Cahn PDE. Compare it with the true value given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f275b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4c817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db07d62",
   "metadata": {},
   "source": [
    "<h3> A PDE from an optimal control problem: </h3>\n",
    "\n",
    "We consider the following PDE which can be shown to be the PDE arising from an HJB equation in optimal control :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\partial_t v +  \\Delta_x v - \\frac{1}{2} | \\nabla_x v|^2 &= 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^d \\notag \\\\\n",
    "    v(T,x) &= g(x) \\notag \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60bc3b3",
   "metadata": {},
   "source": [
    "‚ùì **Question 2.1.17**: In the context of this optimal control PDE, what are the associated functions $b$,$\\sigma$ and $f$ ?\n",
    "\n",
    "\n",
    "‚úèÔ∏è **Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04656d",
   "metadata": {},
   "source": [
    "For the numerical experiments, we choose $x_0=0$, $d=100$, and $g(x) = \\text{ln}(\\frac{1}{2}( 1+ \\lVert x \\rVert^2)$ with $\\textit{semi-explicit form}$ given by Hopf-Cole transformation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    v(0,x_0) = - \\text{ln}\\bigg(\\mathbb{E}\\Big[\\text{exp}\\big(-g(x_0 + \\sigma W_T) \\big)\\Big]\\bigg) \\notag \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6516a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_value = np.sqrt(2) \n",
    "\n",
    "\n",
    "dim_x, dim_y, dim_d, dim_h, N, itr, batch_size = 100, 1, 100, dim_x+10, 20, 5000, 1000\n",
    "\n",
    "x_0, T = torch.zeros(dim_x), 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad04f1a",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.18**: Fill in the drift function `b`, the volatility function `sigma` and the running cost function `f`  in the case of this PDE from an optimal control problem\n",
    " \n",
    "  `Hint`: Use  the functions from the `torch` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t,x):\n",
    "    return ...\n",
    "\n",
    "def sigma(t, x):\n",
    "    return ...\n",
    "    #return torch.sqrt(torch.abs(x)).reshape(batch_size, dim_x, dim_d)\n",
    "\n",
    "\n",
    "def f(t, x, y, z):\n",
    "    return ...\n",
    "\n",
    "\n",
    "def g(x):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FBSDE equation\n",
    "equation = fbsde(\n",
    "    x_0=x_0,                # (Tensor) Initial value, shape: (dim_x,)\n",
    "    b=b,                   # (function) Drift\n",
    "    sigma=sigma,           # (function) Diffusion\n",
    "    f=f,                   # (function) Driver\n",
    "    g=g,                   # (function) Terminal condition\n",
    "    T=T,                   # (float) Terminal time\n",
    "    dim_x=dim_x,\n",
    "    dim_y=dim_y,\n",
    "    dim_d=dim_d\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d138021",
   "metadata": {},
   "source": [
    " ‚ùì **Question 2.1.19**: Plot the evolution of the error during the learning process and the evolution of the initial value $v(0,x_0)$ in the case of this PDE. Compare it with the true value given by the Hopf-Cole transformation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd584f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc54ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487579fc",
   "metadata": {},
   "source": [
    "<a id=references></a>\n",
    "<h2>  References: </h2>\n",
    "\n",
    "$\\bullet$ $\\textit{Germain, Pham, Warin: \"Neural networks-based algorithms for stochastic control and PDEs\"}$, 2023 available [here](https://arxiv.org/pdf/2101.08068).\n",
    "\n",
    "$\\bullet$ $\\textit{Bachouch, Hur√©, Langren√©, Pham : \"Deep neural networks algorithms for stochastic control problems on finite horizon: numerical applications\"}$,  2019 available [here](https://arxiv.org/pdf/1812.05916).\n",
    "\n",
    "$\\bullet$ $\\textit{Hur√©, Pham, Bachouch, Langren√© : \"Deep neural networks algorithms for stochastic control problems on finite horizon: convergence analysis\"}$,  2019 available [here](https://arxiv.org/pdf/1812.04300).\n",
    "\n",
    "$\\bullet$ $\\textit{E, Han, Jentzen : \"Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations\"}$, 2017 available [here](https://arxiv.org/pdf/1706.04702).\n",
    "\n",
    "$\\bullet$ $\\textit{Sirignano, Spiliopoulos : DGM: A deep learning algorithm for solving partial differential equations\"}$, 2017 available [here](https://arxiv.org/pdf/1708.07469)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppStatApp_TP2",
   "language": "python",
   "name": "appstatapp_tp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
