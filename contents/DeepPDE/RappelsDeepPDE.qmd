# Course reminders

The following is a reminder of the main theoretical results about **Deep Learning algorithms for PDE** seen during the course. 

## Some reminders on PDE and stochastic control

### Stochastic control in a nutshell


::: {#formula-description .formulation}

<span class="para-title">Mathematical setup</span>

- <span class="para-subtitle">Dynamics of the controlled state process</span>

Let $(\Omega,\mathcal{F}, \mathbb{F}=(\mathcal{F}_t)_{t \geq 0}, \mathbb{P})$ be a probability space satisfying the usual assumptions and big enough to support a Brownian motion $W=(W_t)_{t \geq 0}$. We consider a control model where the state of the system is governed by a stochastic differential equation (SDE) valued in $\mathbb{R}^n$ given by 

\begin{align}
    d X_s = b(X_s,\alpha_s) d s + \sigma(X_s, \alpha_s ) d W_s, \hspace{0.5 cm}
\end{align}
starting from $X_0= x \in \mathbb{R}^n$ and where we are given 2 measurable maps $(b,\sigma) : \ \mathbb{R}^n \times A \to \mathbb{R}^n ,\mathbb{R}^{d \times n}$ and the control $\alpha=(\alpha_t)_{0 \leq t \leq T}$ is a progressively measurable  (with respect to $\mathbb{F})$ process valued in $A \subset \mathbb{R}^m$. We suppose that the measurable maps $(b,\sigma)$ satisfy a uniform Lipschitz condition on $A$: $\exists K \geq 0$, $\forall x,y \in \mathbb{R}^n$, $\forall a \in A$,
\begin{align}
    | b(x,a) - b(y,a) |+ |\sigma(x,a) - \sigma(y,a) | \leq  K |x-y|.
\end{align}
In the sequel, for $0 \leq t \leq T$, we denote by $\mathcal{T}_{t,T}$ the set of stopping times valued in $[t,T]$. Fix $T > 0$ and we denote by $\mathcal{A}$ the set of progressively mesurable with respect to $\mathbb{F}$ control processes $\alpha = (\alpha_t)_{0 \leq t \leq T}$ such that
\begin{align}
\mathbb{E} \Big[ \int_{0}^{T} | b(0,\alpha_t)|^2 + |\sigma(0,\alpha_t)|^2 dt \Big] < + \infty.
\end{align}
It is known that under the previous conditions, existence and unicity of a strong solution to the SDE (1) for any initial condtion $(t,x) \in [0,T] \in \mathbb{R}^n$. Starting from $x$ at $s=t$, we then denote by $\big \lbrace X_s^{t,x}, t \leq s \leq T \big \rbrace$ the solution to $(1)$ which admits a modification with continuous paths up to indistinguability. We also recall that under the standard conditions on $(b,\sigma)$ and on the integrability condition on $\alpha$, we have
\begin{align}
  \mathbb{E} \Big[ \underset{s \leq t \leq T}{\text{ sup }} |X_s^{t,x}|^2 \Big] < \infty.
\end{align}


- <span class="para-subtitle">Functional objective</span>

Let $f :[0,T] \times \mathbb{R}^n \times A \to \mathbb{R}$ and $g : \mathbb{R}^n \to \mathbb{R}$ two measurable functions. We suppose that
\begin{enumerate}
\item [(1)] g is bounded from below.
or \item [(2)] g satisfies a quadratic growth condition : $|g(x)| \leq C( 1+ |x|^2)$, $\quad$ $\forall x \in \mathbb{R}^n$. for some constant $C$ independant of $x$.
\end{enumerate}
:::
### The dynamic programming approach : HJB equation




### The BSDE approach : Pontryagin's formulation


We introduce the pair of processes $(Y,Z) = (Y_t,Z_t)_{0 \leq t \leq T}$ 


$$
\begin{align}
\begin{cases}
   dY_t &= - f(t,Y_t,Z_t) d t + Z_t d W_t ,\\
   Y_T &= \xi
\end{cases}
\end{align}
$$ {#eq-BSDE-differential}

:::{#def-solBSDE}

A solution to the BSDE @eq-BSDE-differential is a pair of processes $(Y,Z) \in \mathbb{S}^2([0,T];\mathbb{R}) \times \mathbb{H}^2([0,T]; \mathbb{R}^d)$ satisfying
$$
\begin{align}
Y_t = \xi + \int_{t}^{T} f(s,Y_s,Z_s) ds - \int_{t}^{T} Z_s d W_s, \quad 0 \leq t \leq T, \quad \mathbb{P - }\text{ a.s}. \notag 
\end{align}
$$
:::
We now state an import result ensuring existence and unicity to the BSDE $(1)$


:::{#thm-ExistenceUnicityBSDE}

Given a pair of $(\xi,f)$ satisfying Assumptions $(A)$ and $(B)$, there exists a unique solution $(Y,Z) \in \mathbb{S}^2([0,T];\mathbb{R}) \times \mathbb{H}^2([0,T];\mathbb{R}^d)$ to $(1)$.

:::

::: proof

The proof of @thm-ExistenceUnicityBSDE is based on a fixed point argument on the Banach space $\mathbb{S}^2([0,T];\mathbb{R}) \times \mathbb{H}^2([0,T];\mathbb{R}^d)$ endowed with the norm 
$$
\begin{align}
\lVert (Y,Z) \rVert_{\beta} = \bigg(\mathbb{E} \Big[ \int_{0}^{T}  e^{\beta s} \big( |Y_s|^2 + |Z_s|^2 \big) \Big] \bigg)^{\frac{1}{2}}
\end{align}
$$. 
We now show that for a suitable choice of $\beta$, the  mapping $\Phi$ is  well defined on $\mathbb{S}^2([0,T];\mathbb{R}) \times \mathbb{H}^2([0,T];\mathbb{R}^d)$ into $\mathbb{S}^2([0,T];\mathbb{R}) \times \mathbb{H}^2([0,T];\mathbb{R}^d)$ as $(Y,Z) = \Phi(U,V)$. Formally, defining the martingale process $M=(M_t)_{0 \leq t \leq T}$ as $M_t = \mathbb{E} \Big[ \xi + \int_{0}^{T} f(s,U_s,V_s)| \mathcal{F}_t \Big]$, from the Itô's martingale decomposition theorem, we got the existence of a process $Z=(Z_t)_{0 \leq t \leq T} \in \mathbb{H}^2([0,T];\mathbb{R}^d)$ such that
$$
\begin{align}
M_t = M_0 + \int_{0}^{t} Z_s d W_s.
\end{align}
$$
We then define the process $Y=(Y_t)_{0 \leq t \leq T}$ as $$
\begin{align}
Y_t = M_t - \int_{0}^{t} f(s,U_s, V_s) d s .
\end{align}
$$
It is easy to see that $Y_T = \xi$ by construction and that our $Y$ candidate satisfies $(1)$. Now, you can check under the assumptions on $f$ and $\xi$ that
$$
\begin{align}
\mathbb{E} \Big[ \underset{0 \leq t \leq T}{\text{ sup }} |\int_{t}^{T} Z_s d W_s |^2 \Big] \leq 4 \mathbb{E}\Big[ \int_{0}^{T} |Z_s|^2 d s  \Big] < + \infty.
\end{align}
$$
where the inequality follows from Doob's inequality. Therefore, $\Phi$ is a well defined map. Now, you can check that for $\beta$ small enough, $\Phi$ is a contraction and therefore the Banach fixed point ensures that there exists a unique solution to $(1)$. 
:::




::: {#alg-description .algorithm}

<span class="para-title">BSDE, PDE and nonlinear Feynman-Kac formula</span>

In this chapter, we will study an extension of the Feynman-Kac formula for **semi-linear PDE** in the form
$$
\begin{align}
\begin{cases}
\partial_t v(t,x) + \mathcal{L}v(t,x) + f(t,x,v(t,x), \sigma(t,x)^{\top} D_x v(t,x)) = 0, \quad (t,x) \in [0,T) \times \mathbb{R}^n, \\
v(T,x) = g(x).
\end{cases}
\end{align}
$${#eq-semilinearPDE}
In fact, we will represent the PDE solution of @eq-semilinearPDE through a suitable BSDE representation as below

$$
\begin{align}
\begin{cases}
    d Y_s &= - f(s,X_s,Y_s,Z_s) d s + Z_s d W_s , \\
    Y_T &= g(X_T),
\end{cases}
\end{align}
$${#eq-BSDE-differential-2}
and  we recall that the forward SDE $\mathbb{R}^n$-valued process $X=(X_t)_{0 \leq t \leq T}$ is given by
$$
\begin{align}
 d X_s = b(s,X_s) d s + \sigma(s,X_s) d W_s
\end{align}
$$
<span class="para-subtitle">Proposition : Link between $v$ and $(Y,Z)$ :</span>

Let $v \in \mathcal{C}^{1,2}([0,T) \times \mathbb{R}^n) \cap C^0([0,T] \times \mathbb{R}^n)$ be a solution to the PDE $(4)$, satisfying a linear growth condition and such that there exists positive constants $C,q$ such that $| D_x v(t,x)| \leq C( 1+ |x|^q)$ for any $x \in \mathbb{R}^n$. Then the pair of processes $(Y,Z)$ defined by

<div class="eq-frame">
$$
\begin{align}
\begin{cases}
Y_t &= v(t,X_t), \\
Z_t &= \sigma^{\top}(X_t) D_x v(t,X_t), \quad 0 \leq t \leq T
\end{cases}
\end{align}
$$
</div>
is the solution to the BSDE @eq-BSDE-differential-2.

**Proof.** This result is an immediate application of the Itô's formula applied to the process $\big(v(t,X_t)\big)_{0 \leq t \leq T}$ and the fact that $v$ is a solution to the PDE **add ref**. Indeed, applying Itô's formula between $t$ and $T$, we have
$$
\begin{align}
   v(t,X_t)= v(T,X_T) - \int_{t}^{T} \Big( \partial_t v(s,X_s) - \mathcal{L}v(s,X_s)  \Big) d s - \int_{t}^{T} \sigma^{\top}(s,X_s) D_x v(s,X_s) d W_s, \quad 0 \leq t \leq T.
\end{align}
$$
Now, using that $v$ is a solution to the PDE, we get that
$$
\begin{align}
Y_t = g(X_T) + \int_{t}^{T} f(s,X_s,Y_s,Z_s) d s - \int_{t}^{T} Z_s d W_s,  \quad 0 \leq t \leq T,
\end{align}
$$
where we set $(Y_t,Z_t) = \Big(v(t,X_t), \sigma^{\top}(t,X_t)D_x v(t,X_t)\Big)$ for any $t \in [0,T]$. Moreover, from the growth assumptions on $v$ and $D_x v$, we get that $(Y,Z)$ lies in $\mathbb{S}^2([0,T]; \mathbb{R}) \times \mathbb{H}^2([0,T]; \mathbb{R}^d)$ and is unique by @thm-ExistenceUnicityBSDE .





:::





## Neural networks based algorithms for solving PDEs

Now, that we have shown how stochastic control problems naturally lead to PDEs, we will rely on some recent advances which appear to numerically solve these PDE, i.e to characterize the function and/or its derivative solution the PDE. Formally, we are going to tackle the following kind of problems.


::: {#alg-description .algorithm}


<span class="para-title">PDE formulation</span>

Let $v$ a function defined on $[0,T] \times \mathbb{R}^n$ supposed to satisfy the following PDE
\begin{cases}
\partial_t v(t,x) + \mathcal{H}[v](t,x) = 0, \quad (t,x) \in [0,T) \times \mathbb{R}^n, \quad (1) \\
v(T,x) = g(x).
\end{cases}
where the operator $\mathcal{H}$ is defined over the space of functions over $[0,T] \times \mathbb{R}^n$ potentially with some regularity
$[0,T] \times \mathbb{R}^n$ and can be rewritten in the case we are going to cover as
\begin{align}
\mathcal{H}[v](t,x) = H \big(t,x,v(t,x), D_x v(t,x), D^2_x v(t,x) \big). \quad (2)
\end{align}
The complexity of such systems comes notably from 

- The non linearity of $H$ with respect to $v$ and its derivatives.
- The potentially high dimension of the underlying space ($n \approx 100$).

<span class="para-subtitle">Example of PDEs.</span>



- <span class="para-subsubtitle">Linear PDE : </span>

\begin{align}
\begin{cases}
  \partial_t v(t,x) - r(x) v(t,x) + b(x) \cdot D_x v + \frac{1}{2} (\sigma \sigma^{\top})(t,x) : D^2_x v(t,x) + f(x)= 0, \quad (t,x) \in [0,T) \times \mathbb{R}^n \\
  v(T,x) = g(x), \quad x \in \mathbb{R}^n
\end{cases}
\end{align}
In this case, the operator $\mathcal{H}$ is linear with respect to its arguments  with the map $H$ (recall $(1)$) given by
\begin{align}
H(t,x,y,z,\gamma):= -r(x) y(t,x) + b(x) \cdot z(t,x) + \frac{1}{2} (\sigma \sigma^{\top})(t,x) : \gamma(t,x)
\end{align}
This is typically the type of $PDE$ which arises in option pricing in B-S model by taking $b=r$, $\sigma$ the volatility, $f=0$ and $g$ the option payoff.

- <span class="para-subsubtitle">Quasilinear PDE : </span>
\begin{align}
\begin{cases}
  \partial_t v + \mathcal{H}[v] + f(x,v)= 0, \quad (t,x) \in [0,T) \times \mathbb{R}^n, \quad (2)\\
  v(T,x) = g(x), \quad x \in \mathbb{R}^n
\end{cases}
\end{align}
For instance, when $f(x,y) = r \text{max}(y,0) - ry$, this is the type of PDE which arises from pricing of CVA (Credit Valuation adjustment) where $r$ denotes the intensity of default of a counterparty.


<span class="para-subtitle">Numerical challenges.</span>

However, solving these PDEs have always been highly challenging due to the curse of dimensionnality due to the exponentially scaling when discretizing the action mesh size $\mathbb{R}^n$ (grid based methods) but also from the point of view of Monte-Carlo methods which are limited to low dimensional setting $n \approx 6$ and where the solution is essentially computed at a fixed point $(t,x) \in [0,T] \times \mathbb{R}^n$. In order to solve $(1)$ efficiently, we will rely on neural-network based algorithms which can provide a functional representation of the map $(t,x) \in [0,T] \times \mathbb{R}^n$ at any $(t,x)$ and for $n$ beeing large.
:::




### Deep Galerkin Algorithm


:::{#method-description .methodology}
**Mathematical description of the method**

:::


::: {#alg-description .algorithm}
**Algorithm : Deep Galerkin Method**

Let \( \pi_\theta \) be a policy such that …
1. Initialize parameters
2. Iterate until convergence
3. Return the value function
:::

### Deep BSDE Solver

:::{#method-description .methodology}
**Mathematical description of the method**

:::


::: {#alg-description .algorithm}
**Algorithm: Deep BSDE Solver**

Let \( \pi_\theta \) be a policy such that …
1. Initialize parameters
2. Iterate until convergence
3. Return the value function
:::


### Deep BDP Solver

:::{#method-description .methodology}
**Mathematical description of the method**

:::


::: {#alg-description .algorithm}
**Algorithm: Deep BDP Solver**

**Input:** discount factor $\gamma$ , tolerance $\varepsilon$  

**Output:** value function $V^{\star}$

1. Initialize \( V_0(s) = 0 \) for all states \( s \)
   
2. Repeat:
   1. For each state \( s \):
      \[
      V_{k+1}(s) \leftarrow \max_a \left( r(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right)
      \]

3. Until \( \| V_{k+1} - V_k \| < \varepsilon \)
   
4. Return \( V^\* \)
:::

