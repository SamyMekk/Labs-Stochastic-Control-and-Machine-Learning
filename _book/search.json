[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "",
    "text": "Global informations\nWelcome to the webpage of the practical sessions of the course Machine learning and stochastic control taught by Professor Pham within the Master’s program in Probability and Finance (M2PF).",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "index.html#global-informations",
    "href": "index.html#global-informations",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "",
    "text": "Prerequisites :\n\nGood knowledge on Probability Theory and Stochastic Processes.\nBasics on Optimal control theory.\nBasics on Deep Learning.\nFamilarity with Python and potentially with PyTorch.\n\nSchedule :\n\nLab work n° \\(1\\) : Tuesday, January 27th, 9h-12h, room \\(102\\) in tower \\(15-25\\).\nLab work n° \\(2\\) : Tuesday, February 3rd, 9h-12h, room \\(102\\) in tower \\(15-25\\).\nLab work n° \\(3\\) : Tuesday, February 19th, 9h-12h, room \\(102\\) in tower \\(15-25\\).\n\nMaterials :\nThe practical sessions will be conducted in Python. You should bring your own laptop and have your own Python environment set up for each session.1\nPlanning :\n\n Lab work n°1 : Deep PDE Solver \n\nThe first lab session will be about Deep PDE Solver for solving partial differential equations using neural networks. You will implement some of the algorithms seen during the course.\n\n Lab work n°2 : RL for stochastic control problems \n\nThe second lab session will be on Reinforcement Learning for solving some stochastic control problems. We will implement some of the algorithms seen during the course.\n\n Lab work n°3 : GenAI for data generation \n\nThe third lab session will be about Generative IA for data generation based on the Schrödinger Bridge. You will implement the Schrödinger Bridge Time series (SBTS) algorithm and apply it to generate some new samples of generic time series.\nGrade :\nYou will have to choose one lab (TP) among the three proposed by modifying the column named “Lab Choice” associated to your name directly on the Excel sheet available here. Note that the project has to be done by groups of 2 or 3.\nYour objective will be to answer the questions of the chosen lab, and an open-ended mini-project will be proposed at the end of it.\nThe final grade will be based on both the lab questions and the mini-project. In any case, the final course grade will be calculated as follows:\n\n\\[\\begin{align}\n    \\text{Final Grade = 50 \\% multiple-choice exam (QCM) + 50 \\% (Lab +project)}.\n\\end{align}\\]\n\nQuestions ? :\nIf you have any questions, you can either e-mail Samy or Alexandre who prepared the lab sessions for this course.",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "index.html#use-this-website",
    "href": "index.html#use-this-website",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "Use this website",
    "text": "Use this website\n\nStructure of the website :\nThe site is structured into three sections that make up the course, each consisting of two chapters.2\n\n Course reminders: \n\nThe first chapter, named Course reminders, reviews the key theoretical concepts covered in the course. It revisits important results and the corresponding algorithms for the underlying problems.\n\n Practical session: \n\nThe second chapter, named Lab work, contains the practical session instructions along with a link to a Jupyter notebook where you can write your code. These notebooks are designed to be self-contained, allowing you to work independently during the sessions. At the end of each practical session, an open-ended project3 is provided. This project is not intended to be completed during the session, but it is closely related to the material covered and offers an opportunity to explore the concepts in more depth.\n\n\n\nFor your information, this site is generated using Quarto through GitHub Pages (see this GitHub page). If you spot any errors on the site, feel free to report them through pull requests.",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nWe would like to thank the following individuals and organizations without whose support this course would not be possible:\n\nProfessor Huyên Pham for the support and valuable feedbacks during the creation of this lab.\nParticipants in the course for your interest in Machine Learning for Stochastic Control and its applications.",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "",
    "text": "The required scientific packages used during the practical sessions will be specified during the lab sessions.↩︎\nThe sections will be uploaded shortly before the beginning of each lab session.↩︎\nThe projects related to each course session will be uploaded after the third lab session, i.e., after February 19.↩︎",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html",
    "href": "contents/DeepPDE/RappelsDeepPDE.html",
    "title": "1  Course reminders",
    "section": "",
    "text": "1.1 Some reminders on PDE and stochastic control\nThe following is a reminder of the main theoretical results about Deep Learning algorithms for PDE seen during the course.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-pde-and-stochastic-control",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-pde-and-stochastic-control",
    "title": "1  Course reminders",
    "section": "",
    "text": "1.1.1 Stochastic control in a nutshell\n\nMathematical setup\n\nDynamics of the controlled state process\n\nLet \\((\\Omega,\\mathcal{F}, \\mathbb{F}=(\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})\\) be a probability space satisfying the usual assumptions and big enough to support a Brownian motion \\(W=(W_t)_{t \\geq 0}\\). We consider a control model where the state of the system is governed by a stochastic differential equation (SDE) valued in \\(\\mathbb{R}^n\\) given by\n\\[\\begin{align}\n    d X_s = b(X_s,\\alpha_s) d s + \\sigma(X_s, \\alpha_s ) d W_s, \\hspace{0.5 cm}\n\\end{align}\\] starting from \\(X_0= x \\in \\mathbb{R}^n\\) and where we are given 2 measurable maps \\((b,\\sigma) : \\ \\mathbb{R}^n \\times A \\to \\mathbb{R}^n ,\\mathbb{R}^{d \\times n}\\) and the control \\(\\alpha=(\\alpha_t)_{0 \\leq t \\leq T}\\) is a progressively measurable (with respect to \\(\\mathbb{F})\\) process valued in \\(A \\subset \\mathbb{R}^m\\). We suppose that the measurable maps \\((b,\\sigma)\\) satisfy a uniform Lipschitz condition on \\(A\\): \\(\\exists K \\geq 0\\), \\(\\forall x,y \\in \\mathbb{R}^n\\), \\(\\forall a \\in A\\), \\[\\begin{align}\n    | b(x,a) - b(y,a) |+ |\\sigma(x,a) - \\sigma(y,a) | \\leq  K |x-y|.\n\\end{align}\\] In the sequel, for \\(0 \\leq t \\leq T\\), we denote by \\(\\mathcal{T}_{t,T}\\) the set of stopping times valued in \\([t,T]\\). Fix \\(T &gt; 0\\) and we denote by \\(\\mathcal{A}\\) the set of progressively mesurable with respect to \\(\\mathbb{F}\\) control processes \\(\\alpha = (\\alpha_t)_{0 \\leq t \\leq T}\\) such that \\[\\begin{align}\n\\mathbb{E} \\Big[ \\int_{0}^{T} | b(0,\\alpha_t)|^2 + |\\sigma(0,\\alpha_t)|^2 dt \\Big] &lt; + \\infty.\n\\end{align}\\] It is known that under the previous conditions, existence and unicity of a strong solution to the SDE (1) for any initial condtion \\((t,x) \\in [0,T] \\in \\mathbb{R}^n\\). Starting from \\(x\\) at \\(s=t\\), we then denote by \\(\\big \\lbrace X_s^{t,x}, t \\leq s \\leq T \\big \\rbrace\\) the solution to \\((1)\\) which admits a modification with continuous paths up to indistinguability. We also recall that under the standard conditions on \\((b,\\sigma)\\) and on the integrability condition on \\(\\alpha\\), we have \\[\\begin{align}\n  \\mathbb{E} \\Big[ \\underset{s \\leq t \\leq T}{\\text{ sup }} |X_s^{t,x}|^2 \\Big] &lt; \\infty.\n\\end{align}\\]\n\nFunctional objective\n\nLet \\(f :[0,T] \\times \\mathbb{R}^n \\times A \\to \\mathbb{R}\\) and \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) two measurable functions. We suppose that\n\n\n\n1.1.2 The dynamic programming approach : HJB equation\n\n\n1.1.3 The BSDE approach : Pontryagin’s formulation\nWe introduce the pair of processes \\((Y,Z) = (Y_t,Z_t)_{0 \\leq t \\leq T}\\)\n\\[\n\\begin{align}\n\\begin{cases}\n   dY_t &= - f(t,Y_t,Z_t) d t + Z_t d W_t ,\\\\\n   Y_T &= \\xi\n\\end{cases}\n\\end{align}\n\\tag{1.1}\\]\n\nDefinition 1.1 A solution to the BSDE Equation 1.1 is a pair of processes \\((Y,Z) \\in \\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T]; \\mathbb{R}^d)\\) satisfying \\[\n\\begin{align}\nY_t = \\xi + \\int_{t}^{T} f(s,Y_s,Z_s) ds - \\int_{t}^{T} Z_s d W_s, \\quad 0 \\leq t \\leq T, \\quad \\mathbb{P - }\\text{ a.s}. \\notag\n\\end{align}\n\\]\n\nWe now state an import result ensuring existence and unicity to the BSDE \\((1)\\)\n\nTheorem 1.1 Given a pair of \\((\\xi,f)\\) satisfying Assumptions \\((A)\\) and \\((B)\\), there exists a unique solution \\((Y,Z) \\in \\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) to \\((1)\\).\n\n\nProof. The proof of Theorem 1.1 is based on a fixed point argument on the Banach space \\(\\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) endowed with the norm \\[\n\\begin{align}\n\\lVert (Y,Z) \\rVert_{\\beta} = \\bigg(\\mathbb{E} \\Big[ \\int_{0}^{T}  e^{\\beta s} \\big( |Y_s|^2 + |Z_s|^2 \\big) \\Big] \\bigg)^{\\frac{1}{2}}\n\\end{align}\n\\]. We now show that for a suitable choice of \\(\\beta\\), the mapping \\(\\Phi\\) is well defined on \\(\\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) into \\(\\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) as \\((Y,Z) = \\Phi(U,V)\\). Formally, defining the martingale process \\(M=(M_t)_{0 \\leq t \\leq T}\\) as \\(M_t = \\mathbb{E} \\Big[ \\xi + \\int_{0}^{T} f(s,U_s,V_s)| \\mathcal{F}_t \\Big]\\), from the Itô’s martingale decomposition theorem, we got the existence of a process \\(Z=(Z_t)_{0 \\leq t \\leq T} \\in \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) such that \\[\n\\begin{align}\nM_t = M_0 + \\int_{0}^{t} Z_s d W_s.\n\\end{align}\n\\] We then define the process \\(Y=(Y_t)_{0 \\leq t \\leq T}\\) as \\[\n\\begin{align}\nY_t = M_t - \\int_{0}^{t} f(s,U_s, V_s) d s .\n\\end{align}\n\\] It is easy to see that \\(Y_T = \\xi\\) by construction and that our \\(Y\\) candidate satisfies \\((1)\\). Now, you can check under the assumptions on \\(f\\) and \\(\\xi\\) that \\[\n\\begin{align}\n\\mathbb{E} \\Big[ \\underset{0 \\leq t \\leq T}{\\text{ sup }} |\\int_{t}^{T} Z_s d W_s |^2 \\Big] \\leq 4 \\mathbb{E}\\Big[ \\int_{0}^{T} |Z_s|^2 d s  \\Big] &lt; + \\infty.\n\\end{align}\n\\] where the inequality follows from Doob’s inequality. Therefore, \\(\\Phi\\) is a well defined map. Now, you can check that for \\(\\beta\\) small enough, \\(\\Phi\\) is a contraction and therefore the Banach fixed point ensures that there exists a unique solution to \\((1)\\).\n\n\nBSDE, PDE and nonlinear Feynman-Kac formula\nIn this chapter, we will study an extension of the Feynman-Kac formula for semi-linear PDE in the form \\[\n\\begin{align}\n\\begin{cases}\n\\partial_t v(t,x) + \\mathcal{L}v(t,x) + f(t,x,v(t,x), \\sigma(t,x)^{\\top} D_x v(t,x)) = 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n, \\\\\nv(T,x) = g(x).\n\\end{cases}\n\\end{align}\n\\tag{1.2}\\] In fact, we will represent the PDE solution of Equation 1.2 through a suitable BSDE representation as below\n\\[\n\\begin{align}\n\\begin{cases}\n    d Y_s &= - f(s,X_s,Y_s,Z_s) d s + Z_s d W_s , \\\\\n    Y_T &= g(X_T),\n\\end{cases}\n\\end{align}\n\\tag{1.3}\\] and we recall that the forward SDE \\(\\mathbb{R}^n\\)-valued process \\(X=(X_t)_{0 \\leq t \\leq T}\\) is given by \\[\n\\begin{align}\nd X_s = b(s,X_s) d s + \\sigma(s,X_s) d W_s\n\\end{align}\n\\] Proposition : Link between \\(v\\) and \\((Y,Z)\\) :\nLet \\(v \\in \\mathcal{C}^{1,2}([0,T) \\times \\mathbb{R}^n) \\cap C^0([0,T] \\times \\mathbb{R}^n)\\) be a solution to the PDE \\((4)\\), satisfying a linear growth condition and such that there exists positive constants \\(C,q\\) such that \\(| D_x v(t,x)| \\leq C( 1+ |x|^q)\\) for any \\(x \\in \\mathbb{R}^n\\). Then the pair of processes \\((Y,Z)\\) defined by\n\n\\[\n\\begin{align}\n\\begin{cases}\nY_t &= v(t,X_t), \\\\\nZ_t &= \\sigma^{\\top}(X_t) D_x v(t,X_t), \\quad 0 \\leq t \\leq T\n\\end{cases}\n\\end{align}\n\\]\n\nis the solution to the BSDE Equation 1.3.\nProof. This result is an immediate application of the Itô’s formula applied to the process \\(\\big(v(t,X_t)\\big)_{0 \\leq t \\leq T}\\) and the fact that \\(v\\) is a solution to the PDE add ref. Indeed, applying Itô’s formula between \\(t\\) and \\(T\\), we have \\[\n\\begin{align}\n   v(t,X_t)= v(T,X_T) - \\int_{t}^{T} \\Big( \\partial_t v(s,X_s) - \\mathcal{L}v(s,X_s)  \\Big) d s - \\int_{t}^{T} \\sigma^{\\top}(s,X_s) D_x v(s,X_s) d W_s, \\quad 0 \\leq t \\leq T.\n\\end{align}\n\\] Now, using that \\(v\\) is a solution to the PDE, we get that \\[\n\\begin{align}\nY_t = g(X_T) + \\int_{t}^{T} f(s,X_s,Y_s,Z_s) d s - \\int_{t}^{T} Z_s d W_s,  \\quad 0 \\leq t \\leq T,\n\\end{align}\n\\] where we set \\((Y_t,Z_t) = \\Big(v(t,X_t), \\sigma^{\\top}(t,X_t)D_x v(t,X_t)\\Big)\\) for any \\(t \\in [0,T]\\). Moreover, from the growth assumptions on \\(v\\) and \\(D_x v\\), we get that \\((Y,Z)\\) lies in \\(\\mathbb{S}^2([0,T]; \\mathbb{R}) \\times \\mathbb{H}^2([0,T]; \\mathbb{R}^d)\\) and is unique by Theorem 1.1 .",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#neural-networks-based-algorithms-for-solving-pdes",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#neural-networks-based-algorithms-for-solving-pdes",
    "title": "1  Course reminders",
    "section": "1.2 Neural networks based algorithms for solving PDEs",
    "text": "1.2 Neural networks based algorithms for solving PDEs\nNow, that we have shown how stochastic control problems naturally lead to PDEs, we will rely on some recent advances which appear to numerically solve these PDE, i.e to characterize the function and/or its derivative solution the PDE. Formally, we are going to tackle the following kind of problems.\n\nPDE formulation\nLet \\(v\\) a function defined on \\([0,T] \\times \\mathbb{R}^n\\) supposed to satisfy the following PDE\n\\[\\begin{cases}\n\\partial_t v(t,x) + \\mathcal{H}[v](t,x) = 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n, \\quad (1) \\\\\nv(T,x) = g(x).\n\\end{cases}\\]\nwhere the operator \\(\\mathcal{H}\\) is defined over the space of functions over \\([0,T] \\times \\mathbb{R}^n\\) potentially with some regularity \\([0,T] \\times \\mathbb{R}^n\\) and can be rewritten in the case we are going to cover as \\[\\begin{align}\n\\mathcal{H}[v](t,x) = H \\big(t,x,v(t,x), D_x v(t,x), D^2_x v(t,x) \\big). \\quad (2)\n\\end{align}\\] The complexity of such systems comes notably from\n\nThe non linearity of \\(H\\) with respect to \\(v\\) and its derivatives.\nThe potentially high dimension of the underlying space (\\(n \\approx 100\\)).\n\nExample of PDEs.\n\nLinear PDE : \n\n\\[\\begin{align}\n\\begin{cases}\n  \\partial_t v(t,x) - r(x) v(t,x) + b(x) \\cdot D_x v + \\frac{1}{2} (\\sigma \\sigma^{\\top})(t,x) : D^2_x v(t,x) + f(x)= 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n \\\\\n  v(T,x) = g(x), \\quad x \\in \\mathbb{R}^n\n\\end{cases}\n\\end{align}\\] In this case, the operator \\(\\mathcal{H}\\) is linear with respect to its arguments with the map \\(H\\) (recall \\((1)\\)) given by \\[\\begin{align}\nH(t,x,y,z,\\gamma):= -r(x) y(t,x) + b(x) \\cdot z(t,x) + \\frac{1}{2} (\\sigma \\sigma^{\\top})(t,x) : \\gamma(t,x)\n\\end{align}\\] This is typically the type of \\(PDE\\) which arises in option pricing in B-S model by taking \\(b=r\\), \\(\\sigma\\) the volatility, \\(f=0\\) and \\(g\\) the option payoff.\n\nQuasilinear PDE :  \\[\\begin{align}\n\\begin{cases}\n\\partial_t v + \\mathcal{H}[v] + f(x,v)= 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n, \\quad (2)\\\\\nv(T,x) = g(x), \\quad x \\in \\mathbb{R}^n\n\\end{cases}\n\\end{align}\\] For instance, when \\(f(x,y) = r \\text{max}(y,0) - ry\\), this is the type of PDE which arises from pricing of CVA (Credit Valuation adjustment) where \\(r\\) denotes the intensity of default of a counterparty.\n\nNumerical challenges\nHowever, solving these PDEs have always been highly challenging due to the curse of dimensionnality due to the exponentially scaling when discretizing the action mesh size \\(\\mathbb{R}^n\\) (grid based methods) but also from the point of view of Monte-Carlo methods which are limited to low dimensional setting \\(n \\approx 6\\) and where the solution is essentially computed at a fixed point \\((t,x) \\in [0,T] \\times \\mathbb{R}^n\\). In order to solve \\((1)\\) efficiently, we will rely on neural-network based algorithms which can provide a functional representation of the map \\((t,x) \\in [0,T] \\times \\mathbb{R}^n\\) at any \\((t,x)\\) and for \\(n\\) beeing large.\n\n\n1.2.1 Deep Galerkin Algorithm\n\nMathematical description : \n\n\nDeep Galerkin Algorithm : \nLet ( _) be a policy such that … 1. Initialize parameters 2. Iterate until convergence 3. Return the value function\n\nLet ( _) be a policy such that … 1. Initialize parameters 2. Iterate until convergence 3. Return the value function :::\n\n\n1.2.2 Deep BSDE Solver\n\nMathematical description : \n\n\nDeep BSDE Algorithm : \nLet ( _) be a policy such that … 1. Initialize parameters 2. Iterate until convergence 3. Return the value function\n\n\n\n1.2.3 Deep BDP Solver\n\nMathematical description : \n\n\nDeep BDP Algorithm : \nInput: discount factor \\(\\gamma\\) , tolerance \\(\\varepsilon\\)\nOutput: value function \\(V^{\\star}\\)\n\nInitialize ( V_0(s) = 0 ) for all states ( s )\nRepeat:\n\nFor each state ( s ): [ V_{k+1}(s) a ( r(s,a) + {s’} P(s’|s,a) V_k(s’) ) ]\n\nUntil ( | V_{k+1} - V_k | &lt; )\nReturn ( V^* )",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html",
    "href": "contents/DeepPDE/TP1.html",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "2.1 Summary of the lab\nYou can download the Lab Work n°1: Deep PDE Solver as a Jupyter Notebook file here. 1\nAll the necessary informations are already included in the notebook2. Below is a brief summary of the lab content and some expected results.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#summary-of-the-lab",
    "href": "contents/DeepPDE/TP1.html#summary-of-the-lab",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "Content : \nThe Lab work is divided into 2 parts :\n\nThe first part is devoted to the implementation of the Deep Galerkin algorithm where you are going to test it on various PDEs arising in finance. We give below some expected plots that you should obtain after finishing this part.\n\n\n\n\n\n\nEvolution of the loss in the PDE learning of the call price\n\n\n\n\n\n\nPrice surface \\((t,s) \\mapsto C(t,s)\\) for a call option\n\n\n\n\n\nThe second part is devoted to the implementation of the Deep BSDE Solver where you are going to test it on known PDEs arising from finance and stochastic control problems. We give below some expected plots that you should obtain after finishing this part.\n\n\n\n\n\n\nEvolution of the loss for \\(y_0\\) in the LQ control problem\n\n\n\n\n\n\nEvolution of \\(y_0\\) in the LQ control problem",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#towards-the-open-ended-project",
    "href": "contents/DeepPDE/TP1.html#towards-the-open-ended-project",
    "title": "2  Lab work n°1",
    "section": "2.2 Towards the open ended project",
    "text": "2.2 Towards the open ended project\n\nThe project will be fairly open-ended, allowing you to explore on your own the use of Deep Learning algorithms to solve partial differential equations (PDEs). The expected workload corresponds to approximately one full weekend of work.\nYou may be asked to reproduce results from selected research papers and to test the proposed methods on other types of PDEs that you are interested in. For more ambitious students, we may also propose topics that are less directly related to PDE solving but that rely on the methods studied in lectures and tutorials to address more challenging control problems.\nIf you already have other project ideas in mind that make use of the methods covered during the course, please feel free to inform us in advance (between today and second week of February) so that the project can potentially be validated.\nIn all cases, the final mini-project will require the submission of a PDF report of at most 8 pages. The report should include the references to the papers used, a clear formulation of the problem addressed in these papers, a description of the numerical methods employed, as well as your numerical results. You will also need to submit the associated code (either in .py or .ipynb format).\nThe mini-project3 will be announced at the end of the third tutorial session on generative models, which will take place on February 19.4 The submission deadline for the mini-project and the answers to the lab session5 will be announced later.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#footnotes",
    "href": "contents/DeepPDE/TP1.html#footnotes",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "If you end-up with a .txt file, download it and rename it as a .ipynb file.↩︎\nThere will be coding and math questions.↩︎\ni.e., the list of proposed papers along with the expected work, as well as alternative, more challenging project options.↩︎\nA follow-up email will be sent to recall you to complete the Excel sheet available here to choose your lab.↩︎\nNo PDF submission is required for the lab session answers. The answers should be written directly in the Jupyter notebook and submitted along with the documents associated with the mini-project.↩︎",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  }
]