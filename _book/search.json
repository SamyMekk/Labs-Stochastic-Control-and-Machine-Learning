[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "",
    "text": "Global informations\nWelcome to the webpage of the practical sessions of the course Machine learning and stochastic control taught by Professor Pham within the Master’s program in Probability and Finance (M2).",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "index.html#global-informations",
    "href": "index.html#global-informations",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "",
    "text": "Prerequisites :\n\nGood knowledge on Probability Theory and Stochastic Processes.\nBasics on optimal control theory.\nBasics on Deep Learning.\nFamilarity with Python and potentially with PyTorch.\n\nSchedule :\n\nLab work n° \\(1\\) : Tuesday, January 27th, 9h-12h, room \\(102\\) in tower \\(15-25\\).\nLab work n° \\(2\\) : Tuesday, February 3rd, 9h-12h, room \\(102\\) in tower \\(15-25\\).\nLab work n° \\(3\\) : Tuesday, February 19th, 9h-12h, room \\(102\\) in tower \\(15-25\\).\n\nMaterials :\nThe practical sessions will be conducted in Python. You should bring your own laptop and have your own Python environment set up for each session.\nPlanning :\n\n Lab work n°1 : Deep PDE Solver \n\nThe first lab session will be about Deep PDE Solver for solving partial differential equations using neural networks. You will implement some of the algorithms which were seen during the course.\n\n Lab work n°2 : RL for stochastic control problems \n\nThe second lab session will be about Reinforcement Learning for solving some control problems. We will implement some of the algorithms seen during the course About Reinforcement Learning for stochastic control problems.\n\n Lab work n°3 : GenAI for data generation \n\nThe third lab session will be about Generative IA for data generation based on the Schrödinger bridge. You will implement the Schrödinger Bridge Time series (SBTS) algorithm and apply it to generate some new samples of generic time series.\nGrade :\nYou will have to choose one lab (TP) among the three proposed by modifying the column named “Lab Choice” associated to your name directly on the Excel sheet available here. Note that the project has be to done by groups of 2 or 3.\nYour objective will be to answer the questions of the chosen lab, and a fairly open-ended mini-project will be proposed at the end of it.\nThe final grade will be based on both the lab questions and the mini-project. In any case, the final course grade will be calculated as follows:\n\\[\\begin{align}\n    \\text{Final Grade = 50 \\% multiple-choice exam (QCM) + 50 \\% project}.\n\\end{align}\\]\nQuestions ? :\nIf you have any questions, you can either e-mail Samy or Alexandre who prepared the lab sessions for this course.",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "index.html#use-this-website",
    "href": "index.html#use-this-website",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "Use this website",
    "text": "Use this website\n\nStructure of the website :\nThe site is structured into three sections that make up the course, each consisting of two chapters.\n\n Course reminders: \n\nThe first chapter, titled Course Reminders, reviews the key theoretical concepts covered in the course. It revisits important results and the corresponding algorithms for the underlying problems.\n\n Practical session: \n\nThe second chapter, titled Lab Work, contains the practical session instructions along with a link to a Jupyter Notebook where you can write your code. These notebooks are designed to be self-contained, allowing you to work independently during the sessions.\nAt the end of each practical session, an open-ended project is provided. This project is not intended to be completed during the session, but it is closely related to the material covered and offers an opportunity to explore the concepts in more depth.\n\n References: \n\nThe site includes a References section, where we list the key papers forming the core content of this course. Much of this material is relatively recent and represents an active area of research. Feel free to contact us if you are interested in exploring these topics further!\n\n Appendix: \n\nThe site also includes an Appendix section, where we explore in greater depth some of the topics covered in the course. This section is still under development and will be updated regularly throughout the year.\nFor your information, this site is generated using Quarto through GitHub Pages (see this GitHub page). If you spot any errors on the site, feel free to report them through pull requests.",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Lab sessions : Stochastic Control and Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nWe would like to thank the following individuals and organizations without whose support this course would not be possible\n\nProfessor Huyên Pham for the support and valuable feedbacks during the creation of this lab.\nParticipants in the course for your interest in Machine Learning for stochastic control and its applications.",
    "crumbs": [
      "Global informations"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html",
    "href": "contents/DeepPDE/RappelsDeepPDE.html",
    "title": "1  Course reminders",
    "section": "",
    "text": "1.1 Some reminders on PDE and stochastic control\nThe following is a reminder of the main theoretical results about Deep Learning algorithms for PDE seen during the course.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-pde-and-stochastic-control",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-pde-and-stochastic-control",
    "title": "1  Course reminders",
    "section": "",
    "text": "1.1.1 Stochastic control in a nutshell\n\nMathematical setup\n\nDynamics of the controlled state process\n\nLet \\((\\Omega,\\mathcal{F}, \\mathbb{F}=(\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})\\) be a probability space satisfying the usual assumptions and big enough to support a Brownian motion \\(W=(W_t)_{t \\geq 0}\\). We consider a control model where the state of the system is governed by a stochastic differential equation (SDE) valued in \\(\\mathbb{R}^n\\) given by\n\\[\\begin{align}\n    d X_s = b(X_s,\\alpha_s) d s + \\sigma(X_s, \\alpha_s ) d W_s, \\hspace{0.5 cm} (1)\n\\end{align}\\] starting from \\(X_0= x \\in \\mathbb{R}^n\\) and where we are given 2 measurable maps \\((b,\\sigma) : \\ \\mathbb{R}^n \\times A \\to \\mathbb{R}^n ,\\mathbb{R}^{d \\times n}\\) and the control \\(\\alpha=(\\alpha_t)_{0 \\leq t \\leq T}\\) is a progressively measurable (with respect to \\(\\mathbb{F})\\) process valued in \\(A \\subset \\mathbb{R}^m\\). We suppose that the measurable maps \\((b,\\sigma)\\) satisfy a uniform Lipschitz condition on \\(A\\): \\(\\exists K \\geq 0\\), \\(\\forall x,y \\in \\mathbb{R}^n\\), \\(\\forall a \\in A\\), \\[\\begin{align}\n    | b(x,a) - b(y,a) |+ |\\sigma(x,a) - \\sigma(y,a) | \\leq  K |x-y|.\n\\end{align}\\] In the sequel, for \\(0 \\leq t \\leq T\\), we denote by \\(\\mathcal{T}_{t,T}\\) the set of stopping times valued in \\([t,T]\\). Fix \\(T &gt; 0\\) and we denote by \\(\\mathcal{A}\\) the set of progressively mesurable with respect to \\(\\mathbb{F}\\) control processes \\(\\alpha = (\\alpha_t)_{0 \\leq t \\leq T}\\) such that \\[\\begin{align}\n\\mathbb{E} \\Big[ \\int_{0}^{T} | b(0,\\alpha_t)|^2 + |\\sigma(0,\\alpha_t)|^2 dt \\Big] &lt; + \\infty.\n\\end{align}\\] It is known that under the previous conditions, existence and unicity of a strong solution to the SDE (1) for any initial condtion \\((t,x) \\in [0,T] \\in \\mathbb{R}^n\\). Starting from \\(x\\) at \\(s=t\\), we then denote by \\(\\big \\lbrace X_s^{t,x}, t \\leq s \\leq T \\big \\rbrace\\) the solution to \\((1)\\) which admits a modification with continuous paths up to indistinguability. We also recall that under the standard conditions on \\((b,\\sigma)\\) and on the integrability condition on \\(\\alpha\\), we have \\[\\begin{align}\n  \\mathbb{E} \\Big[ \\underset{s \\leq t \\leq T}{\\text{ sup }} |X_s^{t,x}|^2 \\Big] &lt; \\infty.\n\\end{align}\\]\n\nFunctional objective\n\nLet \\(f :[0,T] \\times \\mathbb{R}^n \\times A \\to \\mathbb{R}\\) and \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) two measurable functions. We suppose that\n\n\n\n1.1.2 The dynamic programming approach : HJB equation\n\n\n1.1.3 The BSDE approach : Pontryagin’s formulation\nWe introduce the pair of processes \\((Y,Z) = (Y_t,Z_t)_{0 \\leq t \\leq T}\\)\n\\[\n\\begin{align}\n\\begin{cases}\n   dY_t &= - f(t,Y_t,Z_t) d t + Z_t d W_t , \\quad (1)\\\\\n   Y_T &= \\xi\n\\end{cases}\n\\end{align}\n\\]\n\nDefinition 1.1 A solution to the BSDE is a pair of processes \\((Y,Z) \\in \\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T]; \\mathbb{R}^d)\\) satisfying \\[\n\\begin{align}\nY_t = \\xi + \\int_{t}^{T} f(s,Y_s,Z_s) ds - \\int_{t}^{T} Z_s d W_s, \\quad 0 \\leq t \\leq T, \\quad \\mathbb{P - }\\text{ a.s}. \\notag\n\\end{align}\n\\]\n\nWe now state an import result ensuring existence and unicity to the BSDE \\((1)\\)\n\nTheorem 1.1 Given a pair of \\((\\xi,f)\\) satisfying Assumptions \\((A)\\) and \\((B)\\), there exists a unique solution \\((Y,Z) \\in \\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) to \\((1)\\).\n\n\nProof. The proof is based on a fixed point argument on the Banach space \\(\\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) endowed with the norm \\[\n\\begin{align}\n\\lVert (Y,Z) \\rVert_{\\beta} = \\bigg(\\mathbb{E} \\Big[ \\int_{0}^{T}  e^{\\beta s} \\big( |Y_s|^2 + |Z_s|^2 \\big) \\Big] \\bigg)^{\\frac{1}{2}}\n\\end{align}\n\\]. We now show that for a suitable choice of \\(\\beta\\), the mapping \\(\\Phi\\) is well defined on \\(\\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) into \\(\\mathbb{S}^2([0,T];\\mathbb{R}) \\times \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) as \\((Y,Z) = \\Phi(U,V)\\). Formally, defining the martingale process \\(M=(M_t)_{0 \\leq t \\leq T}\\) as \\(M_t = \\mathbb{E} \\Big[ \\xi + \\int_{0}^{T} f(s,U_s,V_s)| \\mathcal{F}_t \\Big]\\), from the Itô’s martingale decomposition theorem, we got the existence of a process \\(Z=(Z_t)_{0 \\leq t \\leq T} \\in \\mathbb{H}^2([0,T];\\mathbb{R}^d)\\) such that \\[\n\\begin{align}\nM_t = M_0 + \\int_{0}^{t} Z_s d W_s.\n\\end{align}\n\\] We then define the process \\(Y=(Y_t)_{0 \\leq t \\leq T}\\) as \\[\n\\begin{align}\nY_t = M_t - \\int_{0}^{t} f(s,U_s, V_s) d s .\n\\end{align}\n\\] It is easy to see that \\(Y_T = \\xi\\) by construction and that our \\(Y\\) candidate satisfies \\((1)\\). Now, you can check under the assumptions on \\(f\\) and \\(\\xi\\) that \\[\n\\begin{align}\n\\mathbb{E} \\Big[ \\underset{0 \\leq t \\leq T}{\\text{ sup }} |\\int_{t}^{T} Z_s d W_s |^2 \\Big] \\leq 4 \\mathbb{E}\\Big[ \\int_{0}^{T} |Z_s|^2 d s  \\Big] &lt; + \\infty.\n\\end{align}\n\\] where the inequality follows from Doob’s inequality. Therefore, \\(\\Phi\\) is a well defined map. Now, you can check that for \\(\\beta\\) small enough, \\(\\Phi\\) is a contraction and therefore the Banach fixed point ensures that there exists a unique solution to \\((1)\\).\n\n\nBSDE, PDE and nonlinear Feynman-Kac formula\nIn this chapter, we will study an extension of the Feynman-Kac formula for semi-linear PDE in the form \\[\n\\begin{align}\n\\begin{cases}\n\\partial_t v(t,x) + \\mathcal{L}v(t,x) + f(t,x,v(t,x), \\sigma(t,x)^{\\top} D_x v(t,x)) = 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n, \\quad (4) \\\\\nv(T,x) = g(x).\n\\end{cases}\n\\end{align}\n\\] In fact, we will represent the solution of this PDE through a BSDE, namely from the BSDE\n\\[\n\\begin{align}\n\\begin{cases}\n    d Y_s &= - f(s,X_s,Y_s,Z_s) d s + Z_s d W_s , \\\\\n    Y_T &= g(X_T),\n\\end{cases}\n\\end{align}\n\\] and we recall that the forward SDE \\(\\mathbb{R}^n\\)-valued process \\(X=(X_t)_{0 \\leq t \\leq T}\\) is given by \\[\n\\begin{align}\nd X_s = b(s,X_s) d s + \\sigma(s,X_s) d W_s\n\\end{align}\n\\] Link between \\(v\\) and \\((Y,Z)\\) :\nLet \\(v \\in \\mathcal{C}^{1,2}([0,T) \\times \\mathbb{R}^n) \\cap C^0([0,T] \\times \\mathbb{R}^n)\\) be a solution to the PDE \\((4)\\), satisfying a linear growth condition and such that there exists positive constants \\(C,q\\) such that \\(| D_x v(t,x)| \\leq C( 1+ |x|^q)\\) for any \\(x \\in \\mathbb{R}^n\\). Then the pair \\((Y,Z)\\) defined by\n\n\\[\n\\begin{align}\n\\begin{cases}\nY_t &= v(t,X_t), \\\\\nZ_t &= \\sigma^{\\top}(X_t) D_x v(t,X_t), \\quad 0 \\leq t \\leq T\n\\end{cases}\n\\end{align}\n\\]\n\nis a solution to the BSDE \\((1)\\).",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#neural-networks-based-algorithms-for-solving-pdes",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#neural-networks-based-algorithms-for-solving-pdes",
    "title": "1  Course reminders",
    "section": "1.2 Neural networks based algorithms for solving PDEs",
    "text": "1.2 Neural networks based algorithms for solving PDEs\nNow, that we have shown how stochastic control problems naturally lead to PDEs, we will rely on some recent advances which appear to numerically solve these PDE, i.e to characterize the function and/or its derivative solution the PDE. Formally, we are going to tackle the following kind of problems.\n\nPDE formulation\nLet \\(v\\) a function defined on \\([0,T] \\times \\mathbb{R}^n\\) supposed to satisfy the following PDE\n\\[\\begin{cases}\n\\partial_t v(t,x) + \\mathcal{H}[v](t,x) = 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n, \\quad (1) \\\\\nv(T,x) = g(x).\n\\end{cases}\\]\nwhere the operator \\(\\mathcal{H}\\) is defined over the space of functions over \\([0,T] \\times \\mathbb{R}^n\\) potentially with some regularity \\([0,T] \\times \\mathbb{R}^n\\) and can be rewritten in the case we are going to cover as \\[\\begin{align}\n\\mathcal{H}[v](t,x) = H \\big(t,x,v(t,x), D_x v(t,x), D^2_x v(t,x) \\big). \\quad (2)\n\\end{align}\\] The complexity of such systems comes notably from\n\nThe non linearity of \\(H\\) with respect to \\(v\\) and its derivatives.\nThe potentially high dimension of the underlying space (\\(n \\approx 100\\)).\n\nExample of PDEs.\n\nLinear PDE : \n\n\\[\\begin{align}\n\\begin{cases}\n  \\partial_t v(t,x) - r(x) v(t,x) + b(x) \\cdot D_x v + \\frac{1}{2} (\\sigma \\sigma^{\\top})(t,x) : D^2_x v(t,x) + f(x)= 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n \\\\\n  v(T,x) = g(x), \\quad x \\in \\mathbb{R}^n\n\\end{cases}\n\\end{align}\\] In this case, the operator \\(\\mathcal{H}\\) is linear with respect to its arguments with the map \\(H\\) (recall \\((1)\\)) given by \\[\\begin{align}\nH(t,x,y,z,\\gamma):= -r(x) y(t,x) + b(x) \\cdot z(t,x) + \\frac{1}{2} (\\sigma \\sigma^{\\top})(t,x) : \\gamma(t,x)\n\\end{align}\\] This is typically the type of \\(PDE\\) which arises in option pricing in B-S model by taking \\(b=r\\), \\(\\sigma\\) the volatility, \\(f=0\\) and \\(g\\) the option payoff.\n\nQuasilinear PDE :  \\[\\begin{align}\n\\begin{cases}\n\\partial_t v + \\mathcal{H}[v] + f(x,v)= 0, \\quad (t,x) \\in [0,T) \\times \\mathbb{R}^n, \\quad (2)\\\\\nv(T,x) = g(x), \\quad x \\in \\mathbb{R}^n\n\\end{cases}\n\\end{align}\\] For instance, when \\(f(x,y) = r \\text{max}(y,0) - ry\\), this is the type of PDE which arises from pricing of CVA (Credit Valuation adjustment) where \\(r\\) denotes the intensity of default of a counterparty.\n\nNumerical challenges.\nHowever, solving these PDEs have always been highly challenging due to the curse of dimensionnality due to the exponentially scaling when discretizing the action mesh size \\(\\mathbb{R}^n\\) (grid based methods) but also from the point of view of Monte-Carlo methods which are limited to low dimensional setting \\(n \\approx 6\\) and where the solution is essentially computed at a fixed point \\((t,x) \\in [0,T] \\times \\mathbb{R}^n\\). In order to solve \\((1)\\) efficiently, we will rely on neural-network based algorithms which can provide a functional representation of the map \\((t,x) \\in [0,T] \\times \\mathbb{R}^n\\) at any \\((t,x)\\) and for \\(n\\) beeing large.\n\n\n1.2.1 Deep Galerkin Algorithm\n\nMathematical description of the method\n\n\nAlgorithm : Deep Galerkin Method\nLet ( _) be a policy such that … 1. Initialize parameters 2. Iterate until convergence 3. Return the value function\n\n\n\n1.2.2 Deep BSDE Solver\n\nMathematical description of the method\n\n\nAlgorithm: Deep BSDE Solver\nLet ( _) be a policy such that … 1. Initialize parameters 2. Iterate until convergence 3. Return the value function\n\n\n\n1.2.3 Deep BDP Solver\n\nMathematical description of the method\n\n\nAlgorithm: Deep BDP Solver\nInput: discount factor \\(\\gamma\\) , tolerance \\(\\varepsilon\\)\nOutput: value function \\(V^{\\star}\\)\n\nInitialize ( V_0(s) = 0 ) for all states ( s )\nRepeat:\n\nFor each state ( s ): [ V_{k+1}(s) a ( r(s,a) + {s’} P(s’|s,a) V_k(s’) ) ]\n\nUntil ( | V_{k+1} - V_k | &lt; )\nReturn ( V^* )",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html",
    "href": "contents/DeepPDE/TP1.html",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "2.1 Summary of the lab\nYou can download Lab Work n°1: Deep PDE Solver as a Jupyter Notebook here. All the necessary information is already included in the notebook, but a brief summary of the lab content is provided below for convenience.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#summary-of-the-lab",
    "href": "contents/DeepPDE/TP1.html#summary-of-the-lab",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "Content : \nThe Lab work is divided into 3 parts :\n\nThe first part is devoted to get more familiar with the PyTorch library for the ones who are not that familiar with the library.\nThe second part is devoted to the implementation of the Deep Galerkin algorithm with so\nThe third part is devoted to the implementation of the Deep BSDE Solver and to\nTBD.\nTBD\n\n Main goals : \nThe TP aims you to implement the various PDE algorithms seen during the class session :\n\nGet more familiar with the PyTorch library\nImplement the Deep Galerkin and Deep BSDE algorithm and benchmark it with some classic PDE for which we know explicit or semi explicit formulas.\n\nI will add some plots here.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#towards-the-open-ended-mini-project",
    "href": "contents/DeepPDE/TP1.html#towards-the-open-ended-mini-project",
    "title": "2  Lab work n°1",
    "section": "2.2 Towards the open ended mini-project",
    "text": "2.2 Towards the open ended mini-project\n\nAdd here what is expected for the mini-project.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html",
    "href": "contents/RL/RappelsRL.html",
    "title": "3  Course reminders",
    "section": "",
    "text": "3.1 Some Foundations of Reinforcement Learning\nThe following is a reminder of the main theoretical results about Reinforcement Learning for stochastic control problems seen during the course. A full PDF version is also available at this link.\nWe will introduce in the following the main concepts of Reinforcement Learning. If you want to look for more in depth theory, you can look at",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#some-foundations-of-reinforcement-learning",
    "href": "contents/RL/RappelsRL.html#some-foundations-of-reinforcement-learning",
    "title": "3  Course reminders",
    "section": "",
    "text": "3.1.1 Basics of Reinforcement Learning\n\nDefinition 3.1 A Markov Decision Process is a quadruplet given by \\((\\mathcal{X},\\mathcal{A},P,r=(f,g))\\) such that :\n\n\\(\\mathcal{X}\\) denotes the space of states on which the discrete time state process \\((X_t)_{t \\in \\mathbb{N}}\\)\n\\(\\mathcal{A}\\) denotes the space of actions in which the control \\((\\alpha_t)_{t \\in \\mathbb{N}}\\) is defined\nState dynamics given by : \\[X_{t+1} \\sim P_t(X_t,\\alpha_t)\\] with a probability transition given by an application \\((t,x,a) \\in \\mathbb{N} \\times \\mathcal{X} \\times \\mathcal{A} \\mapsto P_t(x,a,dx') \\in \\mathcal{P}(\\mathcal{X})\\).\nReward given by a couple \\((f,g)\\) such that :\n\n\\(f(x,a)\\) is a running reward obtained in state \\(x\\) when choosing the action \\(a\\)\nTerminal reward \\(g(x)\\)\nDiscount factor \\(\\beta \\in [0,1]\\)\n\n\n\nNow, that we have defined the main components of a reinforcement learning problem, we can define the notion of policy\n\nDefinition 3.2 A policy \\(\\pi=(\\pi_t)_{t \\in \\mathbb{N}^{*}}\\) is a sequence of actions choosen in a markovian setting with respect to the state variable. A policy \\(\\pi\\) can be either :\n\ndeterministic when \\(\\pi_t : \\mathcal{X} \\mapsto \\mathcal{A}\\)\nrandomized when \\(\\pi_t : \\mathcal{X} : \\mapsto \\mathcal{P}(\\mathcal{A})\\) meaning that \\(\\pi_t\\) is a probability distribution of choosing an action at time \\(t\\) in state \\(x\\).\n\nWe will say that a control \\(\\alpha = (\\alpha_t)_{t \\in \\mathbb{N}}\\) is drawn from a policy \\(\\pi\\) if for each \\(t \\in \\mathbb{N}\\), we have :\n\n\\(\\alpha_t =\\pi_t(X_t)\\) in the case of deterministic policies\n\\(\\alpha_t \\sim \\pi_t(X_t)\\) in the case of randomized policies.\n\n\nThe goal of Reinforcement Learning will be to learn the control \\(\\alpha\\) with maximises the sum of rewards which will be defined in the value function.\n\nDefinition 3.3 Given a policy \\(\\pi=(\\pi_t)_{t \\in \\mathbb{N}}\\) and an horizon \\(T \\in \\mathbb{N}\\), we define :\n\nThe state value function is defined as : \\[\\begin{align}\nV_t^{\\pi}(x) = \\mathbb{E}_{\\pi}[\\sum_{s=t}^{T-1} f(X_s,\\alpha_s) + g(X_T) | X_t = x], \\quad x \\in \\mathcal{X}\n\\end{align}\n\\] where \\(\\mathbb{E}_{\\pi}\\) denotes the expectation when \\(\\alpha \\sim \\pi\\).\nThe Q value function of \\(\\pi\\) which is defined as : \\[\\begin{align}\nQ_t^{\\pi}(x,a) = \\mathbb{E}_{\\pi}[\\sum_{s=t}^{T-1} f(X_s,\\alpha_s) + g(X_T) | X_t = x,\\alpha_t = a], \\quad x \\in \\mathcal{X}, \\alpha \\in \\mathcal{A}\n\\end{align}\n\\]\nNotons par ailleurs que : \\[\nV_t^{\\pi}(x) = \\mathbb{E}_{a \\sim \\pi_t(x)} [Q_t^{\\pi}(x,a)]\n\\]\n\n\nThe goal is therefore to find a policy \\(\\pi^{*}\\) such that we have \\(V_t^{\\pi^*}(x) = \\underset{\\pi}{\\text{ sup }} V_t^{\\pi}(x)\\)\n\n\n3.1.2 Value-based methods\nIn the case of valued based methods, the goal is to learn a representation of the value function \\(V^{\\pi^*}\\) and \\(Q^{\\pi^*}\\) and then derive the optimal policy from the value function.\n\n\n3.1.3 Policy based methods\nIn the case of policy based methods, we model directly the policies by parametric functions \\(\\pi_{\\theta}\\) with parameters \\(\\theta\\) which can be approximators. For instance, we assume the following :\n\nStochastic randomized policies \\(\\pi^{\\theta}\\) of parameter \\(\\theta\\) with density \\(a \\mapsto \\pi^{\\theta}(t,x,a)\\)\n\n\nDefinition 3.4 When we have a policy based method with a parameter \\(\\theta\\), we can define the performance of the policy \\(\\pi^{\\theta}\\) as the following :\n\\[\nJ(\\theta) = \\mathbb{E}_{\\pi^{\\theta}} [ \\sum_{t=0}^{T-1} f(X_t,\\alpha_t) + g(X_T)]\n\\] The goal is therefore to look for an optimal \\(\\theta\\) which maximises \\(J(\\theta)\\) using a gradient method.\n\n\nTheorem 3.1 Denoting by \\(S=(X_0,\\alpha_0,X_1,\\ldots,\\alpha_{T-1},X_T)\\) a trajectory of state/action and by \\(R(S)\\) the associated total reward by \\(R(S) = \\sum_{t=0}^{T-1} f(X_t,\\alpha_t) + g(X_T)\\) so we have \\(J(\\theta) = \\mathbb{E}_{\\pi^{\\theta}}[R(S)]\\). We have :\n\\[\n\\begin{align}\n\\nabla_{\\theta}J(\\theta)= \\mathbb{E}_{\\pi^{\\theta}}[R(S) \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\text{ ln }\\pi^{\\theta}(t,X_t,\\alpha_t)]\n\\end{align}\n\\]",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#mdp",
    "href": "contents/RL/RappelsRL.html#mdp",
    "title": "3  Course reminders",
    "section": "3.2 MDP",
    "text": "3.2 MDP\n\nDefinition 3.5 Un intervalle de confiance de niveau \\(1-\\alpha\\) est un intervalle \\(I = [A,B]\\) dont les bornes \\(A,B\\) sont des statistiques, et tel que pour tout \\(\\theta\\), \\[P_\\theta(\\theta \\in I) \\geqslant 1 - \\alpha.\\] Un intervalle de confiance de niveau asymptotique \\(1-\\alpha\\) est une suite d’intervalles \\(I_n = [A_n,B_n]\\) dont les bornes \\(A_n,B_n\\) sont des statistiques, et tels que pour tout \\(n\\), \\[ P_\\theta(\\theta \\in I_n) \\geqslant 1 - \\alpha.\\]\n\n\nTheorem 3.2 (Décomposition biais-variance) Le risque quadratique \\(\\mathbb{E}_{\\theta} [|\\hat{\\theta}-\\theta|^2]\\) est égal à \\[\n\\underbrace{\\operatorname{Var}_{\\theta} (\\hat{\\theta})}_{\\text{variance}} +\n\\underbrace{\\mathbb{E}_{\\theta}[\\hat{\\theta}-\\theta]^2}_{\\text{carré du biais}} \\, .\n\\]\n\n\nProof. En notant \\(x\\) l’espérance de \\(\\hat{\\theta}\\), on voit que le risque quadratique est égal à \\(\\mathbb{E}[|\\hat{\\theta} - x - (\\theta - x)|^2]\\). Le carré se développe en trois termes : le premier, \\(\\mathbb{E}[|\\hat{\\theta} - x|^2]\\), est la variance de \\(\\hat{\\theta}\\). Le second, \\(-2\\mathbb{E}[(\\hat{\\theta} - x)(\\theta - x)]\\), est égal à \\(-2(\\theta - x)\\mathbb{E}[\\hat{\\theta} - x]\\), c’est-à-dire 0. Le dernier, \\(\\mathbb{E}[(\\theta - x)^2]\\), est égal à \\((\\theta - x)^2\\), c’est-à-dire \\((\\theta - \\mathbb{E}[\\hat{\\theta}])^2\\) : c’est bien le carré du biais.\n\n\n\n\nÀ gauche, RQ élevé mais biais nul ; à droite, RQ faible mais biais non nul.\n\n\n\nExample 3.1  \n\n\n3.2.1 Value-based methods\n\n\n3.2.2 Policy-based methods\n\n\n3.2.3 TBD\n\n\n3.2.4",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#reinforcement-learning-in-continous-time",
    "href": "contents/RL/RappelsRL.html#reinforcement-learning-in-continous-time",
    "title": "3  Course reminders",
    "section": "3.3 Reinforcement Learning in Continous Time",
    "text": "3.3 Reinforcement Learning in Continous Time\nWe present in the following the main concepts of Reinforcement learning in continuous time.\n\n3.3.1 Problem Formulation\n\nDefinition 3.6  \n\n\\(\\pi : [0,T] \\times \\mathbb{R}^d \\to \\mathcal{P}(A)\\) with density \\(A \\ni a \\mapsto \\pi(t,x,a)\\) with respect to a reference measure \\(\\nu\\) on \\(P(A)\\) and we will by misuse of notation write\n\n\\[ \\pi(t,x,da) = \\pi(t,x,a) d a .\\]\n\nA control \\(\\alpha = (\\alpha_t)_{t \\in [0,T]}\\) is said to be sampled from \\(\\pi=(\\pi_t)_{t \\in [0,T]}\\) if at each time \\(t \\in [0,T]\\), we have \\(\\alpha_t \\sim \\pi_t\\). There is a technical issue here to ensure that we can in fact sample \\(\\alpha\\) from \\(\\pi\\) using an appropriate randomization of the controls through an uncountable number of i.i.d uniform random variables \\((U_t)_{t \\in [0,T]}\\). This kind of randomization can be done under a suitable enlargment of the probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) by using the Fubini Extension to ensure the progressive measurability of \\(\\alpha\\) with respect to the filtration given by \\[\\tilde{\\mathbb{F}} = (\\tilde{\\mathcal{F}_t})_{t \\in [0,T]}, \\quad \\tilde{\\mathcal{F}_t} =  \\sigma((W_s)_{s \\leq t}) \\vee  \\sigma(U_s, s \\leq t).\\]\nState dynamics given by the Itô’s dynamics with \\(\\alpha \\sim \\pi\\)\n\n\\[ dX_t = b(X_t,a_t) dt  + \\sigma(X_t,a_t) dW_t.\\]\n\nNow, we can define the performance evaluation / value function in the context of reinforcement learning in continuous time.\n\nDefinition 3.7 Given a policy \\(\\pi=(\\pi_t)_{t \\in [0,T]}\\), we consider the performance value function defined as\n\\[\\begin{align}\nV_t^{\\pi}(x) &= \\mathbb{E}_{\\pi} \\Big[ \\int_{t}^{T} f(X_s,a_s) + \\lambda \\mathcal{E}(\\pi(s,(X_s)) d s  + g(X_T) | X_t = x \\Big] \\notag \\\\\n&=  \\mathbb{E}_{\\pi} \\Big[ \\int_{t}^{T} f(X_s,a_s) + \\lambda \\text{ log } \\pi(s,X_s,a_s) d s  + g(X_T) | X_t = x \\Big] \\notag\n\\end{align}\n\\] where \\(\\mathbb{E}_{\\pi}\\) denotes the expectation when \\(\\alpha \\sim \\pi\\) and when we added the Shannon entropy $ : (A) $ with temperature \\(\\lambda &gt; 0\\)\n\\[\\mathcal{E}(\\pi) = - \\int_{A} \\text{ log } \\pi(a) \\pi(d a)= - \\mathbb{E}_{a \\sim \\pi} [ \\text{ log } \\pi(a) ],\\]\nfor \\(\\pi \\in \\mathcal{P}(A)\\) with density given by \\(a \\mapsto \\pi(a)\\) used for exploration in the context of RL.\nThe optimal value function is given for \\(\\pi \\in \\Pi\\) where \\(\\Pi\\) denotes an admissible class of stochastic policies as\n\\[V_t(x) = \\underset{\\pi \\in \\Pi}{\\sup } V_t^{\\pi}(x) .\\]\n\n\n\n3.3.2 Policy gradient methods in continous time\nIn the policy gradient method, we will consider parametric family of randomized policies admitting a density writh respect to \\(\\nu\\) on A given by :\n\\[\n\\pi_{\\theta}(t,x,da) = p(t,x,a) \\nu(da)\n\\] Therefore, the perforamnce value function which is also called critic with entropy regularizer is given by :\n\\[\nV^{\\pi_{\\theta}}(t,x) = \\mathbb{E}_{\\pi_{\\theta}} [ \\int_{t}^{T} f(X_s,\\alpha_s) - \\lambda \\text{ log } (\\pi_{\\theta})(s,X_s,\\alpha_s) + g(X_T) | X_t= x]\n\\] and \\[\nJ(\\theta) = \\mathbb{E}[V^{\\pi_{\\theta}}(0,X_0)]\n\\]\n\n3.3.2.1 Policy Gradient Representation\nLet’s dive into policy gradient representation now.\n\n\n3.3.2.2 Actor critic algorithms\nLet’s dive into Actor critic algorithms\n\n\n\n3.3.3 Q-learning and approximations in continous time\n\n3.3.3.1 TBD\n\n\n3.3.3.2 TBD",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#rl-algorithms-to-solve-stochastic-control-problems",
    "href": "contents/RL/RappelsRL.html#rl-algorithms-to-solve-stochastic-control-problems",
    "title": "3  Course reminders",
    "section": "3.4 RL algorithms to solve stochastic control problems",
    "text": "3.4 RL algorithms to solve stochastic control problems",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#to-go-further",
    "href": "contents/RL/RappelsRL.html#to-go-further",
    "title": "3  Course reminders",
    "section": "3.5 To Go further",
    "text": "3.5 To Go further\nIf you are interested in such topics, you can have a look at the following papers as this is a current research topic.\n\nY. Jia and X.Y. Zhou: Policy gradient and Actor-Critic learning in continuous time and space: theory and algorithms, 2022, Journal of Machine Learning and Research. available here.\nY. Jia and X.Y. Zhou: q-Learning in continuous time, 2023, Journal of Machine Learning and Research. available here.",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/TP2.html",
    "href": "contents/RL/TP2.html",
    "title": "4  Lab work n°2",
    "section": "",
    "text": "4.1 Summary of the lab\nYou can download Lab Work n°2: RL for stochastic control problems as a Jupyter Notebook here. All the necessary information is already included in the notebook, but a brief summary of the lab content is provided below for convenience.",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab work n°2</span>"
    ]
  },
  {
    "objectID": "contents/RL/TP2.html#summary-of-the-lab",
    "href": "contents/RL/TP2.html#summary-of-the-lab",
    "title": "4  Lab work n°2",
    "section": "",
    "text": "Content : \nThe lab work is divided into 2 parts :\n\nThe first part is devoted to the implementation of Policy gradient algorithms in the context of RL in continuous time with applications to some stochastic models used in finance.\nThe second part is devoted to the use of Q-learning algorithms for RL in continuous time.\n\n Main goals : \nI will add some plots here.",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab work n°2</span>"
    ]
  },
  {
    "objectID": "contents/RL/TP2.html#towards-the-open-ended-mini-project",
    "href": "contents/RL/TP2.html#towards-the-open-ended-mini-project",
    "title": "4  Lab work n°2",
    "section": "4.2 Towards the open ended mini-project",
    "text": "4.2 Towards the open ended mini-project\n\nAdd here what is expected for the mini-project.",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab work n°2</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/RappelsGenAI.html",
    "href": "contents/GenerativeIA/RappelsGenAI.html",
    "title": "5  Course reminders",
    "section": "",
    "text": "5.1 Some fundamentals of Schrödinger Bridge and connection with optimal transport\nThe following is a reminder of the main theoretical results about Generative AI and Schrödinger Bridge seen during the course. A full PDF version is also available at this link.",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/RappelsGenAI.html#schrödinger-bridge-for-data-generation",
    "href": "contents/GenerativeIA/RappelsGenAI.html#schrödinger-bridge-for-data-generation",
    "title": "5  Course reminders",
    "section": "5.2 Schrödinger Bridge for data generation",
    "text": "5.2 Schrödinger Bridge for data generation",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/TP3.html",
    "href": "contents/GenerativeIA/TP3.html",
    "title": "6  Lab work n°3",
    "section": "",
    "text": "6.1 Summary of the lab\nYou can download the Lab work n°3 as a PDF file by clicking here. All the necessary information is already included in the PDF file, but a brief summary of the lab content is provided below for convenience.",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab work n°3</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/TP3.html#summary-of-the-lab",
    "href": "contents/GenerativeIA/TP3.html#summary-of-the-lab",
    "title": "6  Lab work n°3",
    "section": "",
    "text": "Content : \nThe lab work is divided into 2 parts :\n\nTBD\nTBD\n\n Main goals : \nI will add some plots here.",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab work n°3</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/TP3.html#towards-the-open-ended-mini-project",
    "href": "contents/GenerativeIA/TP3.html#towards-the-open-ended-mini-project",
    "title": "6  Lab work n°3",
    "section": "6.2 Towards the open ended mini-project",
    "text": "6.2 Towards the open ended mini-project\n\nAdd here what is expected for the mini-project.",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab work n°3</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/TP3.html#expected-results",
    "href": "contents/GenerativeIA/TP3.html#expected-results",
    "title": "6  Lab work n°3",
    "section": "6.3 Expected results",
    "text": "6.3 Expected results\n\n\n\nAnimation ici\n\n\n\n\n\n\nOptimal transport from \\(\\mathbf{\\delta_{x}}\\) to Gaussian\n\n\n\n\n\nOptimal transport from Dirac distribution to Gaussian",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab work n°3</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Deep Learning for PDE",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#deep-learning-for-pde",
    "href": "references.html#deep-learning-for-pde",
    "title": "References",
    "section": "",
    "text": "J.Han, A.Jentzen, W.E : Solving high-dimensional partial differential equations using deep learning available here.\nM. Germain, H. Pham, X. Warin: Neural networks-based algorithms for stochastic control and PDEs in finance, Machine Learning and Data Sciences for Financial Markets: a guide to contemporary practices, Cambridge University Press, 2023, Editors: A. Capponi and C. A. Lehalle available here.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#reinforcement-learning-for-stochastic-control",
    "href": "references.html#reinforcement-learning-for-stochastic-control",
    "title": "References",
    "section": "Reinforcement Learning for stochastic control",
    "text": "Reinforcement Learning for stochastic control\n\n\nY. Jia and X.Y. Zhou: Policy gradient and Actor-Critic learning in continuous time and space: theory and algorithms, 2022, Journal of Machine Learning and Research. available here.\nY. Jia and X.Y. Zhou: q-Learning in continuous time, 2023, Journal of Machine Learning and Research. available here.\nR. Sutton and A. Barto: Introduction to reinforcement learning, second edition 2016, available here.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "references.html#generative-ia-for-data-generation",
    "href": "references.html#generative-ia-for-data-generation",
    "title": "References",
    "section": "Generative IA for data generation",
    "text": "Generative IA for data generation\n\n\nM. Hamdouche, P. Henry-Labordère, H. Pham: Generative modeling for time series via Schrödinger bridge, 2023. available here.\nA.Alouadi, B.Barreau, L.Carlier, H.Pham : Robust time series generation via Schrödinger Bridge: a comprehensive evaluation available here.\nC. Remlinger, J. Mikael, R. Elie: Conditional loss and deep Euler scheme for time series generation, 2021, AAAI Conference on Artificial Intelligence. available here.\nM. Xia, X. Li, Q. Shen, T. Chou: Squared Wasserstein-2 distance for efficient reconstruction of stochastic differential equations, 2024, arXiv:2401.11354 available here.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "A primer on neural networks\nIn this appendix, you may find some useful stuffs . A full PDF version is also available at this link",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#a-primer-on-neural-networks",
    "href": "appendix.html#a-primer-on-neural-networks",
    "title": "Appendix",
    "section": "",
    "text": "Feedforward neural networks (FFNN)\n\n\nOther neural network architectures\n\nRecurrent neural networks (RNN)\n\n\nLong short term memory (LSTM)",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#an-overview-of-optimal-transport.",
    "href": "appendix.html#an-overview-of-optimal-transport.",
    "title": "Appendix",
    "section": "An overview of optimal transport.",
    "text": "An overview of optimal transport.\n\nCanonical problem of OT.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#summary-of-the-lab.",
    "href": "contents/DeepPDE/TP1.html#summary-of-the-lab.",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "Content : \nThe Lab work is divided into 3 parts :\n\nThe first part is devoted to get more familiar with the PyTorch library for the ones who are not that familiar with the library.\nThe second part is devoted to the implementation of the Deep Galerkin algorithm with so\nThe third part is devoted to the implementation of the Deep BSDE Solver and to\n\n Main goals : \nThe TP aims you to implement the various PDE algorithms seen during the class session :\n\nGet more familiar with the PyTorch library\nImplement the Deep Galerkin and Deep BSDE algorithm and benchmark it with some classic PDE for which we know explicit or semi explicit formulas.\n\nI will add some plots here.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#towards-the-open-ended-mini-project.",
    "href": "contents/DeepPDE/TP1.html#towards-the-open-ended-mini-project.",
    "title": "2  Lab work n°1",
    "section": "2.2 Towards the open ended mini-project.",
    "text": "2.2 Towards the open ended mini-project.\n\nAdd here what is expected for the mini-project.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  }
]