[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical sessions : Apprentissage automatique et contrôle stochastique",
    "section": "",
    "text": "Organisation\nWelcome to the webpage of the practical sessions of the course Apprentissage automatique et contrôle stochastique taught by Huyên Pham within the Master’s program in Probability and Finance (M2).\nThe practical sessions will be divided into three 3-hour sessions, each illustrating the concepts covered during the lecture.",
    "crumbs": [
      "Organisation"
    ]
  },
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "Practical sessions : Apprentissage automatique et contrôle stochastique",
    "section": "",
    "text": "Schedule: The practical sessions will take place on \\(\\ldots\\) each time in room 15/25 104 at Jussieu.\nMaterials: The practical sessions will be conducted in Python. You should bring your own laptop and have your own Python environment set up for each session.\nPlanning :\n\nLab work n°1 : About Deep PDE Solver for solving partial differential equations.\nLab work n°2 : About Reinforcement Learning for stochastic control problems.\nLab work n°3 : About Generative IA and Schrodinger Bridge for data generation.\n\nGrade : \\(\\ldots\\) To be confirmed",
    "crumbs": [
      "Organisation"
    ]
  },
  {
    "objectID": "index.html#use-this-website",
    "href": "index.html#use-this-website",
    "title": "Practical sessions : Apprentissage automatique et contrôle stochastique",
    "section": "Use this website",
    "text": "Use this website\nThe site is structured into three sections that make up the course, each consisting of two chapters. The first chapter, titled Course reminders, presents the key theoretical concepts, followed by the second chapter, which contains the practical session instructions and a link to a Jupyter notebook file where you can write your code.\nNote that the Jupyter notebooks files are designed to be self-sufficient, allowing you to work independently during the practical sessions.\nFor your information, this site is generated using Quarto, and the notes are available on this GitHub page. If you spot any errors on the site, feel free to report them through pull requests.",
    "crumbs": [
      "Organisation"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html",
    "href": "contents/DeepPDE/RappelsDeepPDE.html",
    "title": "1  Course reminders",
    "section": "",
    "text": "1.1 Some reminders on PDE and stochastic control\nThe following is a reminder of the main theoretical results about Deep Learning algorithms for PDE seen during the course. A full PDF version is also available at this link.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-pde-and-stochastic-control",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-pde-and-stochastic-control",
    "title": "1  Course reminders",
    "section": "",
    "text": "1.1.1 Stochastic control in a nutshell\n\n\n1.1.2 The dynamic programming approach : HJB equation",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-neural-networks",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#some-reminders-on-neural-networks",
    "title": "1  Course reminders",
    "section": "1.2 Some reminders on neural networks",
    "text": "1.2 Some reminders on neural networks\n\n1.2.1 Feedforward neural networks (FFNN)\n\n\n1.2.2 Other neural network architectures\n\n1.2.2.1 Recurrent neural networks (RNN)\n\n\n1.2.2.2 Long short term memory (LSTM)",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#neural-networks-algorithms-for-pde",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#neural-networks-algorithms-for-pde",
    "title": "1  Course reminders",
    "section": "1.3 Neural networks algorithms for PDE",
    "text": "1.3 Neural networks algorithms for PDE\n\n1.3.1 Deep Galerkin Method\n\n1.3.1.1 Algorithm Description\n\n\n1.3.1.2 \n\n\n\n1.3.2 Deep BSDE Solver\n\n1.3.2.1 A quick overview on BSDE\n\n\n1.3.2.2 Algorithm Description\n\n\n\n1.3.3 Deep BDP Solver\n\n1.3.3.1 Algorithm Description",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/RappelsDeepPDE.html#an-extension-deep-learning-for-mdp",
    "href": "contents/DeepPDE/RappelsDeepPDE.html#an-extension-deep-learning-for-mdp",
    "title": "1  Course reminders",
    "section": "1.4 An extension : Deep Learning for MDP",
    "text": "1.4 An extension : Deep Learning for MDP\n\n#  Import des Librairies\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Test\nprint(np.arange(5))\n\n\ndef f(x) :\n    return x**2\n\nL=[i for i in range(-5,6)]\n\nplt.scatter(L,[f(l) for l in L])\nplt.xlabel(\"Test\")\nplt.ylabel(\"Test2\")\nplt.title(\"Test Scatter Plot\")\nplt.grid()\n\n# Exemple simple de DataFrame\ndata = {\n    'Nom': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Âge': [24, 30, 18, 22],\n    'Ville': ['Paris', 'Lyon', 'Marseille', 'Toulouse'],\n    'Score': [85.5, 90.0, 78.0, 88.5]\n}\n\ndf = pd.DataFrame(data)\ndf\n\n[0 1 2 3 4]\n\n\n\n\n\n\n\n\n\nNom\nÂge\nVille\nScore\n\n\n\n\n0\nAlice\n24\nParis\n85.5\n\n\n1\nBob\n30\nLyon\n90.0\n\n\n2\nCharlie\n18\nMarseille\n78.0\n\n\n3\nDavid\n22\nToulouse\n88.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.1 A noter qu’il faut être à l’aise sur l’utilisation de Quarto",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html",
    "href": "contents/DeepPDE/TP1.html",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "2.1 Summary of the Lab work n°1\nYou can download the Lab work n°1 by clicking here. All the relevant informations are already provided on the notebook.\nAll the important information is already presented in the notebook. The following is a simple summary of what you will learn to do during the lab work and what is expected of you for the final submission if you choose this lab.\nThe Lab work is divided into 3 parts :",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#summary-of-the-lab-work-n1",
    "href": "contents/DeepPDE/TP1.html#summary-of-the-lab-work-n1",
    "title": "2  Lab work n°1",
    "section": "",
    "text": "The first part is devoted to get more familiar with the PyTorch library for the ones who are not that familiar with the library.\nThe second part is devoted to the implementation of the Deep Galerkin algorithm with so\nThe third part is devoted to the implementation of the Deep BSDE Solver and to\n\n\n2.1.1 Goals\nThe TP aims you to implement the various PDE algorithms seen during the class session :\n\nGet more familiar with the PyTorch library\nImplement the Deep Galerkin and Deep BSDE algorithm and benchmark it with some classic PDE for which we know explicit or semi explicit formulas.\n\n\n\n2.1.2 Your expected work\n\n\n2.1.3 Submission deadline\n\\(\\ldots\\) : TBD.",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/DeepPDE/TP1.html#expected-results",
    "href": "contents/DeepPDE/TP1.html#expected-results",
    "title": "2  Lab work n°1",
    "section": "2.2 Expected results",
    "text": "2.2 Expected results\n\n2.2.1 About the Deep Galerking Algorithm\n\n\n2.2.2 About the Deep BSDE Solver",
    "crumbs": [
      "Part n°1 : Deep Learning for PDE",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab work n°1</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html",
    "href": "contents/RL/RappelsRL.html",
    "title": "3  Course reminders",
    "section": "",
    "text": "3.1 Some Foundations of Reinforcement Learning\nThe following is a reminder of the main theoretical results about Reinforcement Learning for stochastic control problems seen during the course. A full PDF version is also available at this link.\nWe will introduce in the following the main concepts of Reinforcement Learning. If you want to look for more in depth theory, you can look at",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#some-foundations-of-reinforcement-learning",
    "href": "contents/RL/RappelsRL.html#some-foundations-of-reinforcement-learning",
    "title": "3  Course reminders",
    "section": "",
    "text": "3.1.1 Basics of Reinforcement Learning\n\nDefinition 3.1 A Markov Decision Process is a quadruplet given by \\((\\mathcal{X},\\mathcal{A},P,r=(f,g))\\) such that :\n\n\\(\\mathcal{X}\\) denotes the space of states on which the discrete time state process \\((X_t)_{t \\in \\mathbb{N}}\\)\n\\(\\mathcal{A}\\) denotes the space of actions in which the control \\((\\alpha_t)_{t \\in \\mathbb{N}}\\) is defined\nState dynamics given by : \\[X_{t+1} \\sim P_t(X_t,\\alpha_t)\\] with a probability transition given by an application \\((t,x,a) \\in \\mathbb{N} \\times \\mathcal{X} \\times \\mathcal{A} \\mapsto P_t(x,a,dx') \\in \\mathcal{P}(\\mathcal{X})\\).\nReward given by a couple \\((f,g)\\) such that :\n\n\\(f(x,a)\\) is a running reward obtained in state \\(x\\) when choosing the action \\(a\\)\nTerminal reward \\(g(x)\\)\nDiscount factor \\(\\beta \\in [0,1]\\)\n\n\n\nNow, that we have defined the main components of a reinforcement learning problem, we can define the notion of policy\n\nDefinition 3.2 A policy \\(\\pi=(\\pi_t)_{t \\in \\mathbb{N}^{*}}\\) is a sequence of actions choosen in a markovian setting with respect to the state variable. A policy \\(\\pi\\) can be either :\n\ndeterministic when \\(\\pi_t : \\mathcal{X} \\mapsto \\mathcal{A}\\)\nrandomized when \\(\\pi_t : \\mathcal{X} : \\mapsto \\mathcal{P}(\\mathcal{A})\\) meaning that \\(\\pi_t\\) is a probability distribution of choosing an action at time \\(t\\) in state \\(x\\).\n\nWe will say that a control \\(\\alpha = (\\alpha_t)_{t \\in \\mathbb{N}}\\) is drawn from a policy \\(\\pi\\) if for each \\(t \\in \\mathbb{N}\\), we have :\n\n\\(\\alpha_t =\\pi_t(X_t)\\) in the case of deterministic policies\n\\(\\alpha_t \\sim \\pi_t(X_t)\\) in the case of randomized policies.\n\n\nThe goal of Reinforcement Learning will be to learn the control \\(\\alpha\\) with maximises the sum of rewards which will be defined in the value function.\n\nDefinition 3.3 Given a policy \\(\\pi=(\\pi_t)_{t \\in \\mathbb{N}}\\) and an horizon \\(T \\in \\mathbb{N}\\), we define :\n\nThe state value function is defined as : \\[\\begin{align}\nV_t^{\\pi}(x) = \\mathbb{E}_{\\pi}[\\sum_{s=t}^{T-1} f(X_s,\\alpha_s) + g(X_T) | X_t = x], \\quad x \\in \\mathcal{X}\n\\end{align}\n\\] where \\(\\mathbb{E}_{\\pi}\\) denotes the expectation when \\(\\alpha \\sim \\pi\\).\nThe Q value function of \\(\\pi\\) which is defined as : \\[\\begin{align}\nQ_t^{\\pi}(x,a) = \\mathbb{E}_{\\pi}[\\sum_{s=t}^{T-1} f(X_s,\\alpha_s) + g(X_T) | X_t = x,\\alpha_t = a], \\quad x \\in \\mathcal{X}, \\alpha \\in \\mathcal{A}\n\\end{align}\n\\]\nNotons par ailleurs que : \\[\nV_t^{\\pi}(x) = \\mathbb{E}_{a \\sim \\pi_t(x)} [Q_t^{\\pi}(x,a)]\n\\]\n\n\nThe goal is therefore to find a policy \\(\\pi^{*}\\) such that we have \\(V_t^{\\pi^*}(x) = \\underset{\\pi}{\\text{ sup }} V_t^{\\pi}(x)\\)\n\n\n3.1.2 Value-based methods\nIn the case of valued based methods, the goal is to learn a representation of the value function \\(V^{\\pi^*}\\) and \\(Q^{\\pi^*}\\) and then derive the optimal policy from the value function.\n\n\n3.1.3 Policy based methods\nIn the case of policy based methods, we model directly the policies by parametric functions \\(\\pi_{\\theta}\\) with parameters \\(\\theta\\) which can be approximators. For instance, we assume the following :\n\nStochastic randomized policies \\(\\pi^{\\theta}\\) of parameter \\(\\theta\\) with density \\(a \\mapsto \\pi^{\\theta}(t,x,a)\\)\n\n\nDefinition 3.4 When we have a policy based method with a parameter \\(\\theta\\), we can define the performance of the policy \\(\\pi^{\\theta}\\) as the following :\n\\[\nJ(\\theta) = \\mathbb{E}_{\\pi^{\\theta}} [ \\sum_{t=0}^{T-1} f(X_t,\\alpha_t) + g(X_T)]\n\\] The goal is therefore to look for an optimal \\(\\theta\\) which maximises \\(J(\\theta)\\) using a gradient method.\n\n\nDenoting by $S=(X_0,0,X_1,,{T-1},X_T) a trajectory of state/action and by \\(R(S)\\) the associated total reward by \\(R(S) = \\sum_{t=0}^{T-1} f(X_t,\\alpha_t) + g(X_T)\\) so we have \\(J(\\theta) = \\mathbb{E}_{\\pi^{\\theta}}[R(S)]\\). We have :\n\\[\n\\begin{align}\n\\nabla_{\\theta}J(\\theta)= \\mathbb{E}_{\\pi^{\\theta}}[R(S) \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\text{ ln }\\pi^{\\theta}(t,X_t,\\alpha_t)]\n\\end{align}\n\\]\n\nPreuve : TBD",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#mdp",
    "href": "contents/RL/RappelsRL.html#mdp",
    "title": "3  Course reminders",
    "section": "3.2 MDP",
    "text": "3.2 MDP\n\nDefinition 3.5 Un intervalle de confiance de niveau \\(1-\\alpha\\) est un intervalle \\(I = [A,B]\\) dont les bornes \\(A,B\\) sont des statistiques, et tel que pour tout \\(\\theta\\), \\[P_\\theta(\\theta \\in I) \\geqslant 1 - \\alpha.\\] Un intervalle de confiance de niveau asymptotique \\(1-\\alpha\\) est une suite d’intervalles \\(I_n = [A_n,B_n]\\) dont les bornes \\(A_n,B_n\\) sont des statistiques, et tels que pour tout \\(n\\), \\[ P_\\theta(\\theta \\in I_n) \\geqslant 1 - \\alpha.\\]\n\n\nTheorem 3.1 (Décomposition biais-variance) Le risque quadratique \\(\\mathbb{E}_{\\theta} [|\\hat{\\theta}-\\theta|^2]\\) est égal à \\[\n\\underbrace{\\operatorname{Var}_{\\theta} (\\hat{\\theta})}_{\\text{variance}} +\n\\underbrace{\\mathbb{E}_{\\theta}[\\hat{\\theta}-\\theta]^2}_{\\text{carré du biais}} \\, .\n\\]\n\n\nProof. En notant \\(x\\) l’espérance de \\(\\hat{\\theta}\\), on voit que le risque quadratique est égal à \\(\\mathbb{E}[|\\hat{\\theta} - x - (\\theta - x)|^2]\\). Le carré se développe en trois termes : le premier, \\(\\mathbb{E}[|\\hat{\\theta} - x|^2]\\), est la variance de \\(\\hat{\\theta}\\). Le second, \\(-2\\mathbb{E}[(\\hat{\\theta} - x)(\\theta - x)]\\), est égal à \\(-2(\\theta - x)\\mathbb{E}[\\hat{\\theta} - x]\\), c’est-à-dire 0. Le dernier, \\(\\mathbb{E}[(\\theta - x)^2]\\), est égal à \\((\\theta - x)^2\\), c’est-à-dire \\((\\theta - \\mathbb{E}[\\hat{\\theta}])^2\\) : c’est bien le carré du biais.\n\n\n\n\nÀ gauche, RQ élevé mais biais nul ; à droite, RQ faible mais biais non nul.\n\n\n\nExample 3.1 On peut se demander si, dans la langue courante, les 21 lettres de l’alphabet ont à peu près la même probabilité d’apparaître comme première lettre d’un mot. Cela revient à tester si , hypothèse qui est évidemment fausse, il suffit de regarder l’épaisseur des 26 sections du dictionnaire pour s’en rendre compte.\nQu’en est-il des 9 chiffres ? On peut vouloir tester si, dans n’importe quel document (journal, site internet, article scientifique), ces 9 chiffres apparaissent à peu près uniformément en tant que premier chiffre d’un nombre. Cela reviendrait à tester.\nCe n’est pas le cas et cette hypothèse est très fréquemment réfutée : le premier chiffre significatif d’un nombre est bien plus souvent 1 (\\(\\approx 30\\%\\) des cas) que \\(9\\) (\\(\\approx 5\\%\\) cas). Ce phénomène s’appelle loi de Benford.\n\n\n3.2.1 Value-based methods\n\n\n3.2.2 Policy-based methods\n\n\n3.2.3 TBD\n\n\n3.2.4",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#reinforcement-learning-in-continous-time",
    "href": "contents/RL/RappelsRL.html#reinforcement-learning-in-continous-time",
    "title": "3  Course reminders",
    "section": "3.3 Reinforcement Learning in Continous Time",
    "text": "3.3 Reinforcement Learning in Continous Time\n\n3.3.1 Problem Formulation\n\n\n3.3.2 Policy gradient methods in continous time\nIn the policy gradient method, we will consider parametric family of randomized policies admitting a density writh respect to \\(\\nu\\) on A given by :\n\\[\n\\pi_{\\theta}(t,x,da) = p(t,x,a) \\nu(da)\n\\] Therefore, the perforamnce value function which is also called critic with entropy regularizer is given by :\n\\[\n\\begin{align}\nV^{\\pi_{\\theta}}(t,x) = \\mathbb{E}_{\\pi^{\\theta}} [ \\int_{t}^{T} f(X_s,\\alpha_s) - \\lambda log (\\pi_{\\theta})(s,X_s,\\alpha_s) + g(X_T) | X_t= x]\n\\] and \\[\nJ(\\theta) = \\mathbb{E}[V^{\\pi_{\\theta}}(0,X_0)]\n\\]\n\n3.3.2.1 Policy Gradient Representation\nLet’s dive into policy gradient representation now.\n\n\n3.3.2.2 Actor critic algorithms\nLet’s dive into Actor critic algorithms\n\n\n\n3.3.3 Q-learning and approximations in continous time\n\n3.3.3.1 TBD\n\n\n3.3.3.2 TBD",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/RappelsRL.html#to-go-further",
    "href": "contents/RL/RappelsRL.html#to-go-further",
    "title": "3  Course reminders",
    "section": "3.4 To Go Further",
    "text": "3.4 To Go Further\nIf you are interested in such topics, you can have a look at the following papers as this is a current research topic.",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/RL/TP2.html",
    "href": "contents/RL/TP2.html",
    "title": "4  Lab work n°2",
    "section": "",
    "text": "4.1 Summary of the Lab work n°2\nYou can download the Lab work n°2 by clicking here. All the relevant informations are already provided on the notebook.\nAll the important information is already presented in the notebook. The following is a simple summary of what you will learn to do during the lab work and what is expected of you for the final submission if you choose this lab.\n\\(\\bx\\)",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab work n°2</span>"
    ]
  },
  {
    "objectID": "contents/RL/TP2.html#summary-of-the-lab-work-n2",
    "href": "contents/RL/TP2.html#summary-of-the-lab-work-n2",
    "title": "4  Lab work n°2",
    "section": "",
    "text": "4.1.1 Goals\n\n\n4.1.2 Your expected work\n\n\n4.1.3 Submission deadline",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab work n°2</span>"
    ]
  },
  {
    "objectID": "contents/RL/TP2.html#expected-results",
    "href": "contents/RL/TP2.html#expected-results",
    "title": "4  Lab work n°2",
    "section": "4.2 Expected results",
    "text": "4.2 Expected results\n\n4.2.1 \n\n\n4.2.2",
    "crumbs": [
      "Part n°2  : Reinforcement Learning for stochastic control problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab work n°2</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/RappelsGenAI.html",
    "href": "contents/GenerativeIA/RappelsGenAI.html",
    "title": "5  Course reminders",
    "section": "",
    "text": "5.1 Some fundamentals of Schrödinger Bridge and connection with optimal transport\nThe following is a reminder of the main theoretical results about Generative AI and Schrödinger Bridge seen during the course. A full PDF version is also available at this link.",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/RappelsGenAI.html#schrödinger-bridge-for-data-generation",
    "href": "contents/GenerativeIA/RappelsGenAI.html#schrödinger-bridge-for-data-generation",
    "title": "5  Course reminders",
    "section": "5.2 Schrödinger Bridge for data generation",
    "text": "5.2 Schrödinger Bridge for data generation",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Course reminders</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/TP3.html",
    "href": "contents/GenerativeIA/TP3.html",
    "title": "6  Lab work n°3",
    "section": "",
    "text": "6.1 Summary of the Lab work n°3\nYou can download the Lab work n°3 by clicking here. All the relevant informations are already provided on the notebook.\nAll the important information is already presented in the notebook. The following is a simple summary of what you will learn to do during the lab work and what is expected of you for the final submission if you choose this lab.",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab work n°3</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/TP3.html#summary-of-the-lab-work-n3",
    "href": "contents/GenerativeIA/TP3.html#summary-of-the-lab-work-n3",
    "title": "6  Lab work n°3",
    "section": "",
    "text": "6.1.1 Goals\n\n\n6.1.2 Your expected work\n\n\n6.1.3 Submission deadline",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab work n°3</span>"
    ]
  },
  {
    "objectID": "contents/GenerativeIA/TP3.html#expected-results",
    "href": "contents/GenerativeIA/TP3.html#expected-results",
    "title": "6  Lab work n°3",
    "section": "6.2 Expected results",
    "text": "6.2 Expected results\n\n6.2.1 \n\n\n6.2.2",
    "crumbs": [
      "Part n°3 : Generative AI for data generation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab work n°3</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "$$\n\n\\newcommand{\\bx}{\\boldsymbol{x}}\n\n\\newcommand{\\bt}{\\boldsymbol{\\theta}}\n\n\\newcommand{\\dkl}{\\mathrm{d}_{\\mathrm{KL}}}\n\n\\newcommand{\\dtv}{\\mathrm{d}_{\\mathrm{TV}}}\n\n\\newcommand{\\emv}{\\hat{\\theta}_{\\mathrm{emv}}}\n\n\\newcommand{\\ent}{\\mathrm{Ent}}\n\n$$\n\n\n\n\n\n\n\n\n\nDeep Learning for PDE :\n\nTBD\nTBD\n[1] M. Germain, H. Pham, X. Warin: Neural networks-based algorithms for stochastic control and PDEs in finance, Machine Learning and Data Sciences for Financial Markets: a guide to contemporary practices, Cambridge University Press, 2023, Editors: A. Capponi and C. A. Lehalle available here\n\n\n\nReinforcement Learning for stochastic control :\n\n[3] Y. Jia and X.Y. Zhou: Policy gradient and Actor-Critic learning in continuous time and space: theory and algorithms, 2022, Journal of Machine Learning and Research. available here\n[4] Y. Jia and X.Y. Zhou: q-Learning in continuous time, 2023, Journal of Machine Learning and Research. available here\n[6] R. Sutton and A. Barto: Introduction to reinforcement learning, second edition 2016, available here\n\n\n\nGenerative IA for data generation :\n\n[2] M. Hamdouche, P. Henry-Labordère, H. Pham: Generative modeling for time series via Schrödinger bridge, 2023. available here.\n[4] A.Alouadi, B.Barreau, L.Carlier, H.Pham : Robust time series generation via Schrödinger Bridge: a comprehensive evaluation available here.\n[5] C. Remlinger, J. Mikael, R. Elie: Conditional loss and deep Euler scheme for time series generation, 2021, AAAI Conference on Artificial Intelligence. available here.\n[7] M. Xia, X. Li, Q. Shen, T. Chou: Squared Wasserstein-2 distance for efficient reconstruction of stochastic differential equations, 2024, arXiv:2401.11354 available here.\n\n\n\nSome personal recommendations :",
    "crumbs": [
      "References"
    ]
  }
]