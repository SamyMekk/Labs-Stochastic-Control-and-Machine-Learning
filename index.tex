% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  10,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreport}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[lmargin=2cm,rmargin=2cm,tmargin=2cm,bmargin=3cm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bt}{\boldsymbol{\theta}}
\newcommand{\dkl}{\mathrm{d}_{\mathrm{KL}}}
\newcommand{\dtv}{\mathrm{d}_{\mathrm{TV}}}
\newcommand{\emv}{\hat{\theta}_{\mathrm{emv}}}
\newcommand{\ent}{\mathrm{Ent}}



\usepackage{mathrsfs}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage[english]{selnolig} % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Stochastic Control and Machine Learning : Lab sessions},
  pdfauthor={Samy Mekkaoui},
  pdflang={en},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Stochastic Control and Machine Learning : Lab sessions}
\author{Samy Mekkaoui}
\date{2025-11-09}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Organisation}\label{organisation}
\addcontentsline{toc}{chapter}{Organisation}

\markboth{Organisation}{Organisation}

Welcome to the webpage of the practical sessions of the course
\textbf{Apprentissage automatique et contrôle stochastique} taught by
Professor \href{https://sites.google.com/site/phamxuanhuyen/}{H.Pham}
within the Master's program in Probability and Finance
(\href{https://finance.math.upmc.fr/}{M2-PF}).

Logo of the M2-PF's Master

The practical sessions will be divided into three 3-hour sessions, each
illustrating the concepts covered during the lecture.

\begin{itemize}
\item
  \textbf{Schedule}: The practical sessions will take place on
  \(\ldots\) each time in \textbf{room 15/25 104} at Jussieu.
\item
  \textbf{Materials}: The practical sessions will be conducted in
  \textbf{Python}. You should bring your own laptop and have your own
  Python environment set up for each session.
\item
  \textbf{Planning} :

  \begin{itemize}
  \tightlist
  \item
    \textbf{Lab work n°1} : About \textbf{Deep PDE Solver} for solving
    partial differential equations.
  \item
    \textbf{Lab work n°2} : About \textbf{Reinforcement Learning} for
    stochastic control problems.
  \item
    \textbf{Lab work n°3} : About \textbf{Generative IA} and
    \textbf{Schrodinger Bridge} for data generation.
  \end{itemize}
\item
  \textbf{Grade} : \(\ldots\) To be confirmed
\end{itemize}

\section*{Use this website}\label{use-this-website}
\addcontentsline{toc}{section}{Use this website}

\markright{Use this website}

The site is structured into three sections that make up the course, each
consisting of two chapters. The first chapter, titled \textbf{Course
reminders}, presents the key theoretical concepts, followed by the
second chapter, which contains the practical session instructions and a
link to a \textbf{Jupyter notebook file} where you can write your code.

Note that the Jupyter notebooks files are designed to be
\textbf{self-sufficient}, allowing you to work independently during the
practical sessions.

For your information, this site is generated using
\href{https://quarto.org}{Quarto}, and the notes are available on this
\href{https://github.com/SamyMekk/TP-Controle-Stochastique}{GitHub}
page. If you spot any errors on the site, feel free to report them
through pull requests.

\section*{Instructors}\label{instructors}
\addcontentsline{toc}{section}{Instructors}

\markright{Instructors}

The Lab sessions for this course have been prepared by
\href{https://samymekk.github.io/}{Samy} and
\href{https://www.linkedin.com/in/alexandre-alouadi/}{Alexandre}. We
want to thank \href{https://sites.google.com/site/phamxuanhuyen/}{Huyên
Pham} for beeing our advisor and for the confidence in letting us
preparing the lab works of his course.

The \textbf{prerequisites} for this course are the following:

\begin{itemize}
\tightlist
\item
  Good knowledge on \textbf{Probability Theory} and \textbf{Stochastic
  Processes}.
\item
  Basics on \textbf{optimal control theory}.
\item
  Basics on \textbf{Deep Learning}.
\item
  Familarity with \textbf{Python} and potentially with \textbf{PyTorch}.
\end{itemize}

If you have any \textbf{questions}, you can either e-mail
\href{mailto:samy.mekkaoui@polytechnique.edu}{Samy} or
\href{mailto:alexandre.alouadi@polytechnique.edu}{Alexandre}

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

We would like to thank the following individuals and organizations
without whose support this course would not be possible

\begin{itemize}
\tightlist
\item
  Professor \href{https://sites.google.com/site/phamxuanhuyen/}{Huyên
  Pham} for the support and valuable feedbacks during the creation of
  this lab.
\item
  Yadh Hafsi and Rémi Surat for their multiple feedbacks on the lectures
  notes and for their invaluable contributions on the labs.
\item
  \textbf{Participants} in the course of your interest in Machine
  Learning for stochastic control and its applications.
\end{itemize}

\part{Part n°1 : Deep Learning for PDE}

\chapter{Course reminders}\label{course-reminders}

The following is a reminder of the main theoretical results about
\textbf{Deep Learning algorithms for PDE} seen during the course. full
and self-content \textbf{PDF version} is also available at this
\href{PDF-File/Deep_Learning_for_PDE.pdf}{link}.

\section{Some reminders on PDE and stochastic
control}\label{some-reminders-on-pde-and-stochastic-control}

\subsection{Stochastic control in a
nutshell}\label{stochastic-control-in-a-nutshell}

\subsection{The dynamic programming approach : HJB
equation}\label{the-dynamic-programming-approach-hjb-equation}

\section{Some reminders on neural
networks}\label{some-reminders-on-neural-networks}

\subsection{Feedforward neural networks
(FFNN)}\label{feedforward-neural-networks-ffnn}

\subsection{Other neural network
architectures}\label{other-neural-network-architectures}

\subsubsection{Recurrent neural networks
(RNN)}\label{recurrent-neural-networks-rnn}

\subsubsection{Long short term memory
(LSTM)}\label{long-short-term-memory-lstm}

\section{Neural networks algorithms for
PDE}\label{neural-networks-algorithms-for-pde}

\subsection{Deep Galerkin Method}\label{deep-galerkin-method}

\subsubsection{Algorithm Description}\label{algorithm-description}

\subsubsection{}\label{section}

\subsection{Deep BSDE Solver}\label{deep-bsde-solver}

\subsubsection{A quick overview on BSDE}\label{a-quick-overview-on-bsde}

\subsubsection{Algorithm Description}\label{algorithm-description-1}

\subsection{Deep BDP Solver}\label{deep-bdp-solver}

\subsubsection{Algorithm Description}\label{algorithm-description-2}

\section{An extension : Deep Learning for
MDP}\label{an-extension-deep-learning-for-mdp}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  Import des Librairies}

\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\CommentTok{\# Test}
\BuiltInTok{print}\NormalTok{(np.arange(}\DecValTok{5}\NormalTok{))}


\KeywordTok{def}\NormalTok{ f(x) :}
    \ControlFlowTok{return}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}

\NormalTok{L}\OperatorTok{=}\NormalTok{[i }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{)]}

\NormalTok{plt.scatter(L,[f(l) }\ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in}\NormalTok{ L])}
\NormalTok{plt.xlabel(}\StringTok{"Test"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Test2"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Test Scatter Plot"}\NormalTok{)}
\NormalTok{plt.grid()}

\CommentTok{\# Exemple simple de DataFrame}
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{\textquotesingle{}Nom\textquotesingle{}}\NormalTok{: [}\StringTok{\textquotesingle{}Alice\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Charlie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}David\textquotesingle{}}\NormalTok{],}
    \StringTok{\textquotesingle{}Âge\textquotesingle{}}\NormalTok{: [}\DecValTok{24}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{22}\NormalTok{],}
    \StringTok{\textquotesingle{}Ville\textquotesingle{}}\NormalTok{: [}\StringTok{\textquotesingle{}Paris\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Lyon\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Marseille\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Toulouse\textquotesingle{}}\NormalTok{],}
    \StringTok{\textquotesingle{}Score\textquotesingle{}}\NormalTok{: [}\FloatTok{85.5}\NormalTok{, }\FloatTok{90.0}\NormalTok{, }\FloatTok{78.0}\NormalTok{, }\FloatTok{88.5}\NormalTok{]}
\NormalTok{\}}

\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(data)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0 1 2 3 4]
\end{verbatim}

\begin{verbatim}
C:\Users\samym\anaconda3\lib\site-packages\IPython\core\formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llrlr}
\toprule
{} &      Nom &  Âge &      Ville &  Score \\
\midrule
0 &    Alice &   24 &      Paris &   85.5 \\
1 &      Bob &   30 &       Lyon &   90.0 \\
2 &  Charlie &   18 &  Marseille &   78.0 \\
3 &    David &   22 &   Toulouse &   88.5 \\
\bottomrule
\end{tabular}

\pandocbounded{\includegraphics[keepaspectratio]{contents/DeepPDE/RappelsDeepPDE_files/figure-pdf/cell-2-output-4.pdf}}

\begin{definition}[]\protect\hypertarget{def-quarto}{}\label{def-quarto}

A noter qu'il faut être à l'aise sur l'utilisation de Quarto

\end{definition}

\chapter{Lab work n°1}\label{lab-work-n1}

You can download the \textbf{Lab work n°1} by clicking
\href{TP1-PDE/Lab\%20Session\%20n\%C2\%B01\%20-\%20Deep\%20Learning\%20for\%20PDE.ipynb}{here}.
All the relevant informations are already provided on the notebook.

All the important information is already presented in the notebook. The
following is a simple summary of what you will learn to do during the
lab work and what is expected of you for the final submission if you
choose this lab.

\section{Summary of the Lab work n°1}\label{summary-of-the-lab-work-n1}

The Lab work is divided into 3 parts :

\begin{itemize}
\tightlist
\item
  The first part is devoted to get more familiar with the PyTorch
  library for the ones who are not that familiar with the library.
\item
  The second part is devoted to the implementation of the Deep Galerkin
  algorithm with so
\item
  The third part is devoted to the implementation of the Deep BSDE
  Solver and to
\end{itemize}

\subsection{Goals}\label{goals}

The TP aims you to implement the various PDE algorithms seen during the
class session :

\begin{itemize}
\tightlist
\item
  Get more familiar with the PyTorch library
\item
  Implement the Deep Galerkin and Deep BSDE algorithm and benchmark it
  with some classic PDE for which we know explicit or semi explicit
  formulas.
\end{itemize}

\subsection{Your expected work}\label{your-expected-work}

\subsection{Submission deadline}\label{submission-deadline}

\(\ldots\) : TBD.

\section{Expected results}\label{expected-results}

\subsection{About the Deep Galerking
Algorithm}\label{about-the-deep-galerking-algorithm}

\subsection{About the Deep BSDE
Solver}\label{about-the-deep-bsde-solver}

\part{Part n°2 : Reinforcement Learning for stochastic control problems}

\chapter{Course reminders}\label{course-reminders-1}

The following is a reminder of the main theoretical results about
\textbf{Reinforcement Learning for stochastic control problems} seen
during the course. A full \textbf{PDF version} is also available at this
\href{PDF-File/RL_in_Finance.pdf}{link}.

\section{Some Foundations of Reinforcement
Learning}\label{some-foundations-of-reinforcement-learning}

We will introduce in the following the main concepts of Reinforcement
Learning. If you want to look for more in depth theory, you can look at

\subsection{Basics of Reinforcement
Learning}\label{basics-of-reinforcement-learning}

\begin{definition}[]\protect\hypertarget{def-MDP}{}\label{def-MDP}

A Markov Decision Process is a quadruplet given by
\((\mathcal{X},\mathcal{A},P,r=(f,g))\) such that :

\begin{itemize}
\tightlist
\item
  \(\mathcal{X}\) denotes the space of states on which the discrete time
  state process \((X_t)_{t \in \mathbb{N}}\)
\item
  \(\mathcal{A}\) denotes the space of actions in which the control
  \((\alpha_t)_{t \in \mathbb{N}}\) is defined
\item
  State dynamics given by : \[X_{t+1} \sim P_t(X_t,\alpha_t)\] with a
  probability transition given by an application
  \((t,x,a) \in \mathbb{N} \times \mathcal{X} \times \mathcal{A} \mapsto P_t(x,a,dx') \in \mathcal{P}(\mathcal{X})\).
\item
  Reward given by a couple \((f,g)\) such that :

  \begin{itemize}
  \tightlist
  \item
    \(f(x,a)\) is a running reward obtained in state \(x\) when choosing
    the action \(a\)
  \item
    Terminal reward \(g(x)\)
  \item
    Discount factor \(\beta \in [0,1]\)
  \end{itemize}
\end{itemize}

\end{definition}

Now, that we have defined the main components of a reinforcement
learning problem, we can define the notion of policy

\begin{definition}[]\protect\hypertarget{def-Policy}{}\label{def-Policy}

A policy \(\pi=(\pi_t)_{t \in \mathbb{N}^{*}}\) is a sequence of actions
choosen in a markovian setting with respect to the state variable. A
policy \(\pi\) can be either :

\begin{itemize}
\tightlist
\item
  deterministic when \(\pi_t : \mathcal{X} \mapsto \mathcal{A}\)
\item
  randomized when
  \(\pi_t : \mathcal{X} : \mapsto \mathcal{P}(\mathcal{A})\) meaning
  that \(\pi_t\) is a probability distribution of choosing an action at
  time \(t\) in state \(x\).
\end{itemize}

We will say that a control \(\alpha = (\alpha_t)_{t \in \mathbb{N}}\) is
drawn from a policy \(\pi\) if for each \(t \in \mathbb{N}\), we have :

\begin{itemize}
\tightlist
\item
  \(\alpha_t =\pi_t(X_t)\) in the case of deterministic policies
\item
  \(\alpha_t \sim \pi_t(X_t)\) in the case of randomized policies.
\end{itemize}

\end{definition}

The goal of Reinforcement Learning will be to learn the control
\(\alpha\) with maximises the sum of rewards which will be defined in
the value function.

\begin{definition}[]\protect\hypertarget{def-valuefunction}{}\label{def-valuefunction}

Given a policy \(\pi=(\pi_t)_{t \in \mathbb{N}}\) and an horizon
\(T \in \mathbb{N}\), we define :

\begin{itemize}
\item
  The state value function is defined as : \[\begin{align}
  V_t^{\pi}(x) = \mathbb{E}_{\pi}[\sum_{s=t}^{T-1} f(X_s,\alpha_s) + g(X_T) | X_t = x], \quad x \in \mathcal{X} 
  \end{align}
  \] where \(\mathbb{E}_{\pi}\) denotes the expectation when
  \(\alpha \sim \pi\).
\item
  The Q value function of \(\pi\) which is defined as : \[\begin{align}
  Q_t^{\pi}(x,a) = \mathbb{E}_{\pi}[\sum_{s=t}^{T-1} f(X_s,\alpha_s) + g(X_T) | X_t = x,\alpha_t = a], \quad x \in \mathcal{X}, \alpha \in \mathcal{A}
  \end{align}
  \]
\item
  Notons par ailleurs que : \[
  V_t^{\pi}(x) = \mathbb{E}_{a \sim \pi_t(x)} [Q_t^{\pi}(x,a)]
  \]
\end{itemize}

\end{definition}

The goal is therefore to find a policy \(\pi^{*}\) such that we have
\(V_t^{\pi^*}(x) = \underset{\pi}{\text{ sup }} V_t^{\pi}(x)\)

\subsection{Value-based methods}\label{value-based-methods}

In the case of valued based methods, the goal is to learn a
representation of the value function \(V^{\pi^*}\) and \(Q^{\pi^*}\) and
then derive the optimal policy from the value function.

\subsection{Policy based methods}\label{policy-based-methods}

In the case of policy based methods, we model directly the policies by
parametric functions \(\pi_{\theta}\) with parameters \(\theta\) which
can be approximators. For instance, we assume the following :

\begin{itemize}
\tightlist
\item
  Stochastic randomized policies \(\pi^{\theta}\) of parameter
  \(\theta\) with density \(a \mapsto \pi^{\theta}(t,x,a)\)
\end{itemize}

\begin{definition}[]\protect\hypertarget{def-PolicyFunction}{}\label{def-PolicyFunction}

When we have a policy based method with a parameter \(\theta\), we can
define the performance of the policy \(\pi^{\theta}\) as the following :

\[
J(\theta) = \mathbb{E}_{\pi^{\theta}} [ \sum_{t=0}^{T-1} f(X_t,\alpha_t) + g(X_T)]
\] The goal is therefore to look for an optimal \(\theta\) which
maximises \(J(\theta)\) using a gradient method.

\end{definition}

\begin{theorem}[]\protect\hypertarget{thm-GradientRepresentation}{}\label{thm-GradientRepresentation}

Denoting by \(S=(X_0,\alpha_0,X_1,\ldots,\alpha_{T-1},X_T)\) a
trajectory of state/action and by \(R(S)\) the associated total reward
by \(R(S) = \sum_{t=0}^{T-1} f(X_t,\alpha_t) + g(X_T)\) so we have
\(J(\theta) = \mathbb{E}_{\pi^{\theta}}[R(S)]\). We have :

\[
\begin{align}
\nabla_{\theta}J(\theta)= \mathbb{E}_{\pi^{\theta}}[R(S) \sum_{t=0}^{T-1} \nabla_{\theta} \text{ ln }\pi^{\theta}(t,X_t,\alpha_t)]
\end{align}
\]

\end{theorem}

\phantomsection\label{proof-PolicyGradient}

\section{MDP}\label{mdp}

\begin{definition}[]\protect\hypertarget{def-Test}{}\label{def-Test}

Un intervalle de confiance de niveau \(1-\alpha\) est un intervalle
\(I = [A,B]\) dont les bornes \(A,B\) sont des statistiques, et tel que
pour tout \(\theta\), \[P_\theta(\theta \in I) \geqslant 1 - \alpha.\]
Un intervalle de confiance de niveau asymptotique \(1-\alpha\) est une
\emph{suite} d'intervalles \(I_n = [A_n,B_n]\) dont les bornes
\(A_n,B_n\) sont des statistiques, et tels que pour tout \(n\),
\[ P_\theta(\theta \in I_n) \geqslant 1 - \alpha.\]

\end{definition}

\begin{theorem}[Décomposition
biais-variance]\protect\hypertarget{thm-biaisvar}{}\label{thm-biaisvar}

Le risque quadratique \(\mathbb{E}_{\theta} [|\hat{\theta}-\theta|^2]\)
est égal à \[
\underbrace{\operatorname{Var}_{\theta} (\hat{\theta})}_{\text{variance}} +
\underbrace{\mathbb{E}_{\theta}[\hat{\theta}-\theta]^2}_{\text{carré du biais}} \, .
\]

\end{theorem}

\begin{proof}
En notant \(x\) l'espérance de \(\hat{\theta}\), on voit que le risque
quadratique est égal à
\(\mathbb{E}[|\hat{\theta} - x - (\theta - x)|^2]\). Le carré se
développe en trois termes :~le premier,
\(\mathbb{E}[|\hat{\theta} - x|^2]\), est la variance de
\(\hat{\theta}\). Le second,
\(-2\mathbb{E}[(\hat{\theta} - x)(\theta - x)]\), est égal à
\(-2(\theta - x)\mathbb{E}[\hat{\theta} - x]\), c'est-à-dire 0. Le
dernier, \(\mathbb{E}[(\theta - x)^2]\), est égal à \((\theta - x)^2\),
c'est-à-dire \((\theta - \mathbb{E}[\hat{\theta}])^2\) :~c'est bien le
carré du biais.
\end{proof}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{images/cover.png}

}

\caption{À~gauche, RQ élevé mais biais nul ;~à droite, RQ faible mais
biais non nul.}

\end{figure}%

\begin{example}[]\protect\hypertarget{exm-testadeq}{}\label{exm-testadeq}

~

\end{example}

\subsection{Value-based methods}\label{value-based-methods-1}

\subsection{Policy-based methods}\label{policy-based-methods-1}

\subsection{TBD}\label{tbd}

\subsection{}\label{section-1}

\section{Reinforcement Learning in Continous
Time}\label{reinforcement-learning-in-continous-time}

We present in the following the main concepts of Reinforcement learning
in continuous time.

\subsection{Problem Formulation}\label{problem-formulation}

\begin{definition}[]\protect\hypertarget{def-RLContinuousTime}{}\label{def-RLContinuousTime}

~

\begin{itemize}
\tightlist
\item
  \(\pi : [0,T] \times \mathbb{R}^d \to \mathcal{P}(A)\) with density
  \(A \ni a \mapsto \pi(t,x,a)\) with respect to a reference measure
  \(\nu\) on \(P(A)\) and we will by misuse of notation write
\end{itemize}

\[ \pi(t,x,da) = \pi(t,x,a) d a .\]

\begin{itemize}
\tightlist
\item
  A control \(\alpha = (\alpha_t)_{t \in [0,T]}\) is said to be sampled
  from \(\pi=(\pi_t)_{t \in [0,T]}\) if at each time \(t \in [0,T]\), we
  have \(\alpha_t \sim \pi_t\). There is a technical issue here to
  ensure that we can in fact sample \(\alpha\) from \(\pi\) using an
  appropriate randomization of the controls through an uncountable
  number of i.i.d uniform random variables \((U_t)_{t \in [0,T]}\). This
  kind of randomization can be done under a suitable enlargment of the
  probability space \((\Omega,\mathcal{F},\mathbb{P})\) by using the
  Fubini Extension to ensure the progressive measurability of \(\alpha\)
  with respect to the filtration given by
  \[\tilde{\mathbb{F}} = (\tilde{\mathcal{F}_t})_{t \in [0,T]}, \quad \tilde{\mathcal{F}_t} =  \sigma((W_s)_{s \leq t}) \vee  \sigma(U_s, s \leq t).\]
\item
  State dynamics given by the Itô's dynamics with \(\alpha \sim \pi\)
\end{itemize}

\[ dX_t = b(X_t,a_t) dt  + \sigma(X_t,a_t) dW_t.\]

\end{definition}

Now, we can define the performance evaluation / value function in the
context of reinforcement learning in continuous time.

\begin{definition}[]\protect\hypertarget{def-valuefunctionRLTempsContinu}{}\label{def-valuefunctionRLTempsContinu}

Given a policy \(\pi=(\pi_t)_{t \in [0,T]}\), we consider the
performance value function defined as

\[\begin{align}
V_t^{\pi}(x) &= \mathbb{E}_{\pi} \Big[ \int_{t}^{T} f(X_s,a_s) + \lambda \mathcal{E}(\pi(s,(X_s)) d s  + g(X_T) | X_t = x \Big] \notag \\
&=  \mathbb{E}_{\pi} \Big[ \int_{t}^{T} f(X_s,a_s) + \lambda \text{ log } \pi(s,X_s,a_s) d s  + g(X_T) | X_t = x \Big] \notag 
\end{align}
\] where \(\mathbb{E}_{\pi}\) denotes the expectation when
\(\alpha \sim \pi\) and when we added the Shannon entropy \$\mathcal{E}
: \mathcal{P}(A) \to \R \$ with temperature \(\lambda > 0\)

\[\mathcal{E}(\pi) = - \int_{A} \text{ log } \pi(a) \pi(d a)= - \mathbb{E}_{a \sim \pi} [ \text{ log } \pi(a) ],\]

for \(\pi \in \mathcal{P}(A)\) with density given by
\(a \mapsto \pi(a)\) used for exploration in the context of RL.

The optimal value function is given for \(\pi \in \Pi\) where \(\Pi\)
denotes an admissible class of stochastic policies as

\[V_t(x) = \underset{\pi \in \Pi}{\sup } V_t^{\pi}(x) .\]

\end{definition}

\subsection{Policy gradient methods in continous
time}\label{policy-gradient-methods-in-continous-time}

In the policy gradient method, we will consider parametric family of
randomized policies admitting a density writh respect to \(\nu\) on A
given by :

\[
\pi_{\theta}(t,x,da) = p(t,x,a) \nu(da)
\] Therefore, the perforamnce value function which is also called critic
with entropy regularizer is given by :

\[
V^{\pi_{\theta}}(t,x) = \mathbb{E}_{\pi_{\theta}} [ \int_{t}^{T} f(X_s,\alpha_s) - \lambda \text{ log } (\pi_{\theta})(s,X_s,\alpha_s) + g(X_T) | X_t= x]
\] and \[
J(\theta) = \mathbb{E}[V^{\pi_{\theta}}(0,X_0)]
\]

\subsubsection{Policy Gradient
Representation}\label{policy-gradient-representation}

Let's dive into policy gradient representation now.

\subsubsection{Actor critic algorithms}\label{actor-critic-algorithms}

Let's dive into Actor critic algorithms

\subsection{Q-learning and approximations in continous
time}\label{q-learning-and-approximations-in-continous-time}

\subsubsection{TBD}\label{tbd-1}

\subsubsection{TBD}\label{tbd-2}

\section{To Go further}\label{to-go-further}

If you are interested in such topics, you can have a look at the
following papers as this is a current research topic.

\begin{itemize}
\item
  Y. Jia and X.Y. Zhou: Policy gradient and Actor-Critic learning in
  continuous time and space: theory and algorithms, 2022, Journal of
  Machine Learning and Research. available
  \href{https://arxiv.org/abs/2111.11232}{here}.
\item
  Y. Jia and X.Y. Zhou: q-Learning in continuous time, 2023, Journal of
  Machine Learning and Research. available
  \href{https://arxiv.org/abs/2207.00713}{here}.
\end{itemize}

\chapter{Lab work n°2}\label{lab-work-n2}

You can download the \textbf{Lab work n°2} by clicking
\href{TP2-RL/TP\%20n\%C2\%B02-\%20RL\%20in\%20Continuous\%20Time.ipynb}{here}.
All the relevant informations are already provided on the notebook.

All the important information is already presented in the notebook. The
following is a simple summary of what you will learn to do during the
lab work and what is expected of you for the final submission if you
choose this lab.

\section{Summary of the Lab work n°2}\label{summary-of-the-lab-work-n2}

The lab work is divided into 2 parts :

\begin{itemize}
\tightlist
\item
  The first part is devoted to the implementation of Policy gradient
  algorithms in the context of RL in continuous time with applications
  to some stochastic models used in finance.
\item
  The second part is devoted to the use of Q-learning algorithms for RL
  in continuous time.
\end{itemize}

\subsection{Goals}\label{goals-1}

\subsection{Your expected work}\label{your-expected-work-1}

\subsection{Submission deadline}\label{submission-deadline-1}

\section{Expected results}\label{expected-results-1}

\subsection{}\label{section-2}

\subsection{}\label{section-3}

\part{Part n°3 : Generative AI for data generation}

\chapter{Course reminders}\label{course-reminders-2}

The following is a reminder of the main theoretical results about
\textbf{Generative AI and Schrödinger Bridge} seen during the course. A
full \textbf{PDF version} is also available at this
\href{PDF-File/Schr\%C3\%B6dinger_Bridge_Data.pdf}{link}.

\section{Some fundamentals of Schrödinger Bridge and connection with
optimal
transport}\label{some-fundamentals-of-schruxf6dinger-bridge-and-connection-with-optimal-transport}

\section{Schrödinger Bridge for data
generation}\label{schruxf6dinger-bridge-for-data-generation}

\chapter{Lab work n°3}\label{lab-work-n3}

You can download the \textbf{Lab work n°3} by clicking
\href{https://raw.githubusercontent.com/SamyMekk/TP-Controle-Stochastique/main/contents/GenerativeIA/PDF-File/LabWork3_SchrodingerBridge.pdf}{here}.

All the important informations are provided in the PDF file. In the
following, we just summarize the main goals of this Lab Work.

\section{Summary of the Lab work n°3}\label{summary-of-the-lab-work-n3}

\subsection{Goals}\label{goals-2}

\subsection{Your expected work}\label{your-expected-work-2}

\subsection{Submission deadline}\label{submission-deadline-2}

\section{Expected results}\label{expected-results-2}

Animation ici

\begin{figure}[H]

{\centering \includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{index_files/mediabag/Gaussian_to_Gaussian.jpg}

}

\caption{Optimal transport from \(\mathbf{\delta_{x}}\) to Gaussian}

\end{figure}%

Optimal transport from Dirac distribution to Gaussian

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\end{CSLReferences}

\subsection*{Deep Learning for PDE :}\label{deep-learning-for-pde}
\addcontentsline{toc}{subsection}{Deep Learning for PDE :}

\begin{itemize}
\item
  {[}1{]} J.Han, A.Jentzen, W.E : Solving high-dimensional partial
  differential equations using deep learning available
  \href{https://arxiv.org/abs/1707.02568}{here}.
\item
  {[}2{]} M. Germain, H. Pham, X. Warin: Neural networks-based
  algorithms for stochastic control and PDEs in finance, Machine
  Learning and Data Sciences for Financial Markets: a guide to
  contemporary practices, Cambridge University Press, 2023, Editors: A.
  Capponi and C. A. Lehalle available
  \href{https://arxiv.org/abs/2101.08068}{here}.
\end{itemize}

\subsection*{Reinforcement Learning for stochastic control
:}\label{reinforcement-learning-for-stochastic-control}
\addcontentsline{toc}{subsection}{Reinforcement Learning for stochastic
control :}

\begin{itemize}
\item
  {[}3{]} Y. Jia and X.Y. Zhou: Policy gradient and Actor-Critic
  learning in continuous time and space: theory and algorithms, 2022,
  Journal of Machine Learning and Research. available
  \href{https://arxiv.org/abs/2111.11232}{here}.
\item
  {[}4{]} Y. Jia and X.Y. Zhou: q-Learning in continuous time, 2023,
  Journal of Machine Learning and Research. available
  \href{https://arxiv.org/abs/2207.00713}{here}.
\item
  {[}5{]} R. Sutton and A. Barto: Introduction to reinforcement
  learning, second edition 2016, available
  \href{https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf}{here}.
\end{itemize}

\subsection*{Generative IA for data generation
:}\label{generative-ia-for-data-generation}
\addcontentsline{toc}{subsection}{Generative IA for data generation :}

\begin{itemize}
\item
  {[}6{]} M. Hamdouche, P. Henry-Labordère, H. Pham: Generative modeling
  for time series via Schrödinger bridge, 2023. available
  \href{https://arxiv.org/pdf/2304.05093}{here}.
\item
  {[}7{]} A.Alouadi, B.Barreau, L.Carlier, H.Pham : Robust time series
  generation via Schrödinger Bridge: a comprehensive evaluation
  available \href{https://arxiv.org/pdf/2503.02943}{here}.
\item
  {[}8{]} C. Remlinger, J. Mikael, R. Elie: Conditional loss and deep
  Euler scheme for time series generation, 2021, AAAI Conference on
  Artificial Intelligence. available
  \href{https://arxiv.org/abs/2102.05313}{here}.
\item
  {[}9{]} M. Xia, X. Li, Q. Shen, T. Chou: Squared Wasserstein-2
  distance for efficient reconstruction of stochastic differential
  equations, 2024, arXiv:2401.11354 available
  \href{https://arxiv.org/abs/2401.11354}{here}.
\end{itemize}




\end{document}
