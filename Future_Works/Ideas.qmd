
<figure style="text-align: center; margin: 0;">
  <img src="https://raw.githubusercontent.com/SamyMekk/TP-Controle-Stochastique/main/images/MasterM2PF.png" 
       alt="Ma belle image"
       style="display: block; margin: 0 auto; max-width: 30%; height: auto;">
  <figcaption style="font-size: 0.9em; color: #333;">
  <p><em>Logo of the M2- Probability and Finance </em></p>
</figure>

 - part: "Part n°2  : Reinforcement Learning for stochastic control problems"
       chapters:
         - contents/RL/RappelsRL.qmd 
         - contents/RL/TP2.qmd
     - part: "Part n°3 : Generative AI for data generation"
       chapters:
         - contents/GenerativeIA/RappelsGenAI.qmd
         - contents/GenerativeIA/TP3.qmd
 
     - References.qmd
  

  - <span class="para-subtitle"> References: </span>

The site includes a **References** section, where we list the key papers forming the core content of this course. Much of this material is relatively recent and represents an active area of research. Feel free to contact us if you are interested in exploring these topics further!

  - <span class="para-subtitle"> Appendix: </span>

The site also includes an **Appendix** section, where we explore in greater depth some of the topics covered in the course. This section is still under development and will be updated regularly throughout the year.
```{python}

# -----------------------
# 5️⃣ Plot
# -----------------------
**


#| output-location: column-fragment

import os
import sys
from pathlib import Path

print("=== KERNEL DEBUG ===")
print("sys.executable:", sys.executable)
print("cwd:", os.getcwd())
print("Path.cwd():", Path.cwd())
print("====================")
#  Import des Librairies

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Test
print(np.arange(5))


def f(x) :
    return x**2

L=[i for i in range(-5,6)]

plt.scatter(L,[f(l) for l in L])
plt.xlabel("Test")
plt.ylabel("Test2")
plt.title("Test Scatter Plot")
plt.grid()

# Exemple simple de DataFrame
data = {
    'Nom': ['Alice', 'Bob', 'Charlie', 'David'],
    'Âge': [24, 30, 18, 22],
    'Ville': ['Paris', 'Lyon', 'Marseille', 'Toulouse'],
    'Score': [85.5, 90.0, 78.0, 88.5]
}



# test_pytorch_mlp.py

import torch
import torch.nn as nn
print("Hello")
# -----------------------------
# Définition du modèle
# -----------------------------
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super().__init__()

        layers = []
        in_dim = input_dim

        for h in hidden_dims:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h

        layers.append(nn.Linear(in_dim, output_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


# -----------------------------
# Instanciation du modèle
# -----------------------------
input_dim = 3
hidden_dims = [64, 64, 64]
output_dim = 1

model = MLP(input_dim, hidden_dims, output_dim)


# -----------------------------
# Affichage de l'architecture
# -----------------------------
print("\n=== Architecture du modèle ===\n")
print(model)


# -----------------------------
# Nombre de paramètres
# -----------------------------
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("\nNombre de paramètres entraînables :", num_params)


# -----------------------------
# Test forward (sanity check)
# -----------------------------
x = torch.randn(5, input_dim)
y = model(x)

print("\nEntrée x.shape :", x.shape)
print("Sortie y.shape :", y.shape)
print("\nSortie y :\n", y)

df = pd.DataFrame(data)
df
```    
### Policy based methods

In the case of policy based methods, we model directly the policies by parametric functions $\pi_{\theta}$ with parameters $\theta$ which can be approximators. For instance, we assume the following :

- Stochastic randomized policies $\pi^{\theta}$ of parameter $\theta$ with density $a \mapsto \pi^{\theta}(t,x,a)$


:::{#def-PolicyFunction}

When we have a policy based method with a parameter $\theta$, we can define the performance of the policy $\pi^{\theta}$ as the following :

$$
J(\theta) = \mathbb{E}_{\pi^{\theta}} [ \sum_{t=0}^{T-1} f(X_t,\alpha_t) + g(X_T)]
$$
The goal is therefore to look for an optimal $\theta$ which maximises $J(\theta)$ using a gradient method.
:::

:::{#thm-GradientRepresentation}

Denoting by $S=(X_0,\alpha_0,X_1,\ldots,\alpha_{T-1},X_T)$ a trajectory of state/action and by $R(S)$ the associated total reward by $R(S) = \sum_{t=0}^{T-1} f(X_t,\alpha_t) + g(X_T)$ so we have $J(\theta) = \mathbb{E}_{\pi^{\theta}}[R(S)]$. We have :

$$
\begin{align}
\nabla_{\theta}J(\theta)= \mathbb{E}_{\pi^{\theta}}[R(S) \sum_{t=0}^{T-1} \nabla_{\theta} \text{ ln }\pi^{\theta}(t,X_t,\alpha_t)]
\end{align}
$$
:::

### Policy gradient methods in continous time

In the policy gradient method, we will consider parametric family of randomized policies admitting a density writh respect to $\nu$ on A given  by :

$$
\pi_{\theta}(t,x,da) = p(t,x,a) \nu(da)
$$
Therefore, the performance value function which is also called critic with entropy regularizer is given by :


$$
V^{\pi_{\theta}}(t,x) = \mathbb{E}_{\pi_{\theta}} [ \int_{t}^{T} f(X_s,\alpha_s) - \lambda \text{ log } (\pi_{\theta})(s,X_s,\alpha_s) + g(X_T) | X_t= x]
$$
and 
$$
J(\theta) = \mathbb{E}[V^{\pi_{\theta}}(0,X_0)]
$$

#### Policy Gradient Representation

Let's dive into policy gradient representation now.


#### Actor critic algorithms

Let's dive into Actor critic algorithms

### Q-learning and approximations in continous time

#### TBD


#### TBD 

## RL algorithms to solve stochastic control problems




## To Go further

If you are interested in such topics, you can have a look at the following papers as this is a current research topic.

 -  Y. Jia and X.Y. Zhou: Policy gradient and Actor-Critic learning in continuous time and space: theory and algorithms, 2022, Journal of Machine Learning and Research. available [here](https://arxiv.org/abs/2111.11232).

 - Y. Jia and X.Y. Zhou: q-Learning in continuous time, 2023, Journal of Machine Learning and Research. available [here](https://arxiv.org/abs/2207.00713).


We will introduce in the following the main concepts of Reinforcement Learning. If you want to look for more in depth theory, you can look at the **References**.
