
<figure style="text-align: center; margin: 0;">
  <img src="https://raw.githubusercontent.com/SamyMekk/TP-Controle-Stochastique/main/images/MasterM2PF.png" 
       alt="Ma belle image"
       style="display: block; margin: 0 auto; max-width: 30%; height: auto;">
  <figcaption style="font-size: 0.9em; color: #333;">
  <p><em>Logo of the M2- Probability and Finance </em></p>
</figure>

 - part: "Part n°2  : Reinforcement Learning for stochastic control problems"
       chapters:
         - contents/RL/RappelsRL.qmd 
         - contents/RL/TP2.qmd
     - part: "Part n°3 : Generative AI for data generation"
       chapters:
         - contents/GenerativeIA/RappelsGenAI.qmd
         - contents/GenerativeIA/TP3.qmd
 
     - References.qmd
  

  - <span class="para-subtitle"> References: </span>

The site includes a **References** section, where we list the key papers forming the core content of this course. Much of this material is relatively recent and represents an active area of research. Feel free to contact us if you are interested in exploring these topics further!

  - <span class="para-subtitle"> Appendix: </span>

The site also includes an **Appendix** section, where we explore in greater depth some of the topics covered in the course. This section is still under development and will be updated regularly throughout the year.
```{python}

# -----------------------
# 5️⃣ Plot
# -----------------------
**


#| output-location: column-fragment

import os
import sys
from pathlib import Path

print("=== KERNEL DEBUG ===")
print("sys.executable:", sys.executable)
print("cwd:", os.getcwd())
print("Path.cwd():", Path.cwd())
print("====================")
#  Import des Librairies

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Test
print(np.arange(5))


def f(x) :
    return x**2

L=[i for i in range(-5,6)]

plt.scatter(L,[f(l) for l in L])
plt.xlabel("Test")
plt.ylabel("Test2")
plt.title("Test Scatter Plot")
plt.grid()

# Exemple simple de DataFrame
data = {
    'Nom': ['Alice', 'Bob', 'Charlie', 'David'],
    'Âge': [24, 30, 18, 22],
    'Ville': ['Paris', 'Lyon', 'Marseille', 'Toulouse'],
    'Score': [85.5, 90.0, 78.0, 88.5]
}



# test_pytorch_mlp.py

import torch
import torch.nn as nn
print("Hello")
# -----------------------------
# Définition du modèle
# -----------------------------
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super().__init__()

        layers = []
        in_dim = input_dim

        for h in hidden_dims:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h

        layers.append(nn.Linear(in_dim, output_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


# -----------------------------
# Instanciation du modèle
# -----------------------------
input_dim = 3
hidden_dims = [64, 64, 64]
output_dim = 1

model = MLP(input_dim, hidden_dims, output_dim)


# -----------------------------
# Affichage de l'architecture
# -----------------------------
print("\n=== Architecture du modèle ===\n")
print(model)


# -----------------------------
# Nombre de paramètres
# -----------------------------
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("\nNombre de paramètres entraînables :", num_params)


# -----------------------------
# Test forward (sanity check)
# -----------------------------
x = torch.randn(5, input_dim)
y = model(x)

print("\nEntrée x.shape :", x.shape)
print("Sortie y.shape :", y.shape)
print("\nSortie y :\n", y)

df = pd.DataFrame(data)
df
```    
